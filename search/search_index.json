{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"K8s Engineers \u00b6","title":"Home"},{"location":"#k8s-engineers","text":"","title":"K8s Engineers"},{"location":"1/","text":"Updating .... \u00b6","title":"Emart Project"},{"location":"1/#updating","text":"","title":"Updating ...."},{"location":"Context_MultiCluster/","text":"Context_MultiCluster \u00b6 Create a service account and bind it to the Cluster-admin role on the clusters. cat ks-clusteradmin.yml apiVersion : v1 kind : ServiceAccount metadata : name : ks-clusteradmin namespace : kube-system cat ks-clusteradmin-rbac.yml apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ks-clusteradmin roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin subjects : - kind : ServiceAccount name : ks-clusteradmin Generate the service account token: SA_NAME = \"ks-clusteradmin\" kubectl -n kube-system describe secret $( kubectl -n kube-system get secret | grep ${ SA_NAME } | awk '{print $1}' ) Updated the kube config file in the pattern below. vi .kube/config apiVersion : v1 kind : Config current-context : <User_Name>@<Cluster_Name> preferences : {} clusters : - cluster : Certificate-authority-data : <keys> server : https://<Server IP>:6443 name : <Cluster_Name> - cluster : certificate-authority-data : <keys> server : https://<Server IP>:8383 name : <Cluster_Name> ########################################## contexts : - context : cluster : <Cluster_Name> user : <User_Name> name : <User_Name>@<Cluster_Name> - context : cluster : <Cluster_Name> user : <User_Name> name : <User_Name>@<Cluster_Name> ######################### users : - name : <User_Name> user : token : <Token_key> - name : <User_Name> user : token : <Token_key> To see the current context kubectl config current-context Use the specific context while executing kubectl --context <context_name> get nodes Set the context as the default. kubectl config set-context NAME References https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/ https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-context/ https://sarasagunawardhana.medium.com/kubernetes-configure-context-and-switching-contexts-in-multiple-clusters-part-4-e93677737328 https://computingforgeeks.com/create-admin-user-to-access-kubernetes-dashboard/","title":"Context MultiCluster"},{"location":"Context_MultiCluster/#context_multicluster","text":"Create a service account and bind it to the Cluster-admin role on the clusters. cat ks-clusteradmin.yml apiVersion : v1 kind : ServiceAccount metadata : name : ks-clusteradmin namespace : kube-system cat ks-clusteradmin-rbac.yml apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ks-clusteradmin roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin subjects : - kind : ServiceAccount name : ks-clusteradmin Generate the service account token: SA_NAME = \"ks-clusteradmin\" kubectl -n kube-system describe secret $( kubectl -n kube-system get secret | grep ${ SA_NAME } | awk '{print $1}' ) Updated the kube config file in the pattern below. vi .kube/config apiVersion : v1 kind : Config current-context : <User_Name>@<Cluster_Name> preferences : {} clusters : - cluster : Certificate-authority-data : <keys> server : https://<Server IP>:6443 name : <Cluster_Name> - cluster : certificate-authority-data : <keys> server : https://<Server IP>:8383 name : <Cluster_Name> ########################################## contexts : - context : cluster : <Cluster_Name> user : <User_Name> name : <User_Name>@<Cluster_Name> - context : cluster : <Cluster_Name> user : <User_Name> name : <User_Name>@<Cluster_Name> ######################### users : - name : <User_Name> user : token : <Token_key> - name : <User_Name> user : token : <Token_key> To see the current context kubectl config current-context Use the specific context while executing kubectl --context <context_name> get nodes Set the context as the default. kubectl config set-context NAME References https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/ https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-context/ https://sarasagunawardhana.medium.com/kubernetes-configure-context-and-switching-contexts-in-multiple-clusters-part-4-e93677737328 https://computingforgeeks.com/create-admin-user-to-access-kubernetes-dashboard/","title":"Context_MultiCluster"},{"location":"about/","text":"updating....","title":"About"},{"location":"coredns/","text":"What is CoreDNS? \u00b6 CoreDNS is a DNS server. It is written in Go. CoreDNS is different from other DNS servers, such as (all excellent) BIND, Knot, PowerDNS and Unbound (technically a resolver, but still worth a mention), because it is very flexible, and almost all functionality is outsourced into plugins. Plugins can be stand-alone or work together to perform a \u201cDNS function\u201d. So what\u2019s a \u201cDNS function\u201d? For the purpose of CoreDNS, we define it as a piece of software that implements the CoreDNS Plugin API. The functionality implemented can wildly deviate. There are plugins that don\u2019t themselves create a response, such as metrics or cache, but that add functionality. Then there are plugins that do generate a response. These can also do anything: There are plugins that communicate with Kubernetes to provide service discovery, plugins that read data from a file or a database. There are currently about 30 plugins included in the default CoreDNS install, but there are also a whole bunch of external plugins that you can compile into CoreDNS to extend its functionality. CoreDNS is powered by plugins. Writing new plugins should be easy enough, but requires knowing Go and having some insight into how DNS works. CoreDNS abstracts away a lot of DNS details so that you can just focus on writing the plugin functionality you need. Our Requirements \u00b6 Requirements Configure CoreDNS with the domain name \u201cgroophy.org\". Each server can be resolve by any server in the cluster for that solution we are using CoreDNS with the host file. Setup \u00b6 Configure the host file with the hostname and IPs. cat /etc/hosts 192.168.0.100 dnsserver . cruise . com 192.168.135.21 test01 . cruise . com 192.168.0.100 dnsserver 127.0.0.1 localhost Download CoreDNS from the website, unpack the binary to /usr/local/bin and make it executable. wget https://github.com/coredns/coredns/releases/download/v1.8.6/coredns_1.8.6_linux_amd64.tgz tar -xvzf coredns_1.8.6_linux_amd64.tgz cp coredns /usr/local/bin/ sudo chmod +x /usr/local/bin/coredns ll /usr/local/bin/coredns Install resolvconf as a tool to manually manage /etc/resolv.conf . apt install resolvconf Set dns as default in /etc/NetworkManager/NetworkManager.conf . vi /etc/NetworkManager/NetworkManager.conf dns=default Add nameserver 127.0.0.1 to /etc/resolvconf/resolv.conf.d/head . vi /etc/resolvconf/resolv.conf.d/head nameserver 127.0.0.1 Create /etc/coredns/Corefile and paste the configuration shown below. vi /etc/coredns/Corefile groophy.org:53 { forward . tls://2606:4700:4700::1111 tls://1.1.1.1 hosts /etc/hosts log errors Cache } .:53 { forward . tls://2606:4700:4700::1111 tls://1.1.1.1 log errors Cache } We are using Cloudflare as a DNS provider(1.1.1.1). Create a new user for CoreDNS and set some permissions on the /opt/coredns directory. sudo useradd -d /var/lib/coredns -m coredns sudo chown coredns:coredns /opt/coredns Download the SystemD service unit file from coredns to /etc/systemd/system/coredns.service . wget https://github.com/coredns/deployment/blob/master/systemd/coredns.service mv coredns.service /etc/systemd/system/ Disable SystemD's default DNS server. sudo systemctl stop systemd-resolved && sudo systemctl disable systemd-resolved Note From that moment on, you will not be able to resolve any web pages anymore, unless you enable DNS again Enable and start CoreDNS sudo systemctl enable coredns && sudo systemctl start coredns Now we can able to resolve domain names, again. E.g. try to dig +short kit.edu . If an IP address is printed, everything works fine. dig +short kit.edu Output 141.3.128.6 Try the below command nslookup dnsserver.groophy.org Output Server : 192.168.0.100 Address : 192.168.0.100 # 53 Name : dnsserver . cruise . com Address : 192.168.0.100 Try the below command nslookup google.com Output Server : 192.168.0.100 Address : 192.168.0.100 # 53 Non - authoritative answer : Name : google . com Address : 142.250.71.14 Name : google . com Address : 2404 : 6800 : 4007 : 824 :: 200 e Conclusion Able to resolve both internal servers and outside the world.","title":"CoreDNS"},{"location":"coredns/#what-is-coredns","text":"CoreDNS is a DNS server. It is written in Go. CoreDNS is different from other DNS servers, such as (all excellent) BIND, Knot, PowerDNS and Unbound (technically a resolver, but still worth a mention), because it is very flexible, and almost all functionality is outsourced into plugins. Plugins can be stand-alone or work together to perform a \u201cDNS function\u201d. So what\u2019s a \u201cDNS function\u201d? For the purpose of CoreDNS, we define it as a piece of software that implements the CoreDNS Plugin API. The functionality implemented can wildly deviate. There are plugins that don\u2019t themselves create a response, such as metrics or cache, but that add functionality. Then there are plugins that do generate a response. These can also do anything: There are plugins that communicate with Kubernetes to provide service discovery, plugins that read data from a file or a database. There are currently about 30 plugins included in the default CoreDNS install, but there are also a whole bunch of external plugins that you can compile into CoreDNS to extend its functionality. CoreDNS is powered by plugins. Writing new plugins should be easy enough, but requires knowing Go and having some insight into how DNS works. CoreDNS abstracts away a lot of DNS details so that you can just focus on writing the plugin functionality you need.","title":"What is CoreDNS?"},{"location":"coredns/#our-requirements","text":"Requirements Configure CoreDNS with the domain name \u201cgroophy.org\". Each server can be resolve by any server in the cluster for that solution we are using CoreDNS with the host file.","title":"Our Requirements"},{"location":"coredns/#setup","text":"Configure the host file with the hostname and IPs. cat /etc/hosts 192.168.0.100 dnsserver . cruise . com 192.168.135.21 test01 . cruise . com 192.168.0.100 dnsserver 127.0.0.1 localhost Download CoreDNS from the website, unpack the binary to /usr/local/bin and make it executable. wget https://github.com/coredns/coredns/releases/download/v1.8.6/coredns_1.8.6_linux_amd64.tgz tar -xvzf coredns_1.8.6_linux_amd64.tgz cp coredns /usr/local/bin/ sudo chmod +x /usr/local/bin/coredns ll /usr/local/bin/coredns Install resolvconf as a tool to manually manage /etc/resolv.conf . apt install resolvconf Set dns as default in /etc/NetworkManager/NetworkManager.conf . vi /etc/NetworkManager/NetworkManager.conf dns=default Add nameserver 127.0.0.1 to /etc/resolvconf/resolv.conf.d/head . vi /etc/resolvconf/resolv.conf.d/head nameserver 127.0.0.1 Create /etc/coredns/Corefile and paste the configuration shown below. vi /etc/coredns/Corefile groophy.org:53 { forward . tls://2606:4700:4700::1111 tls://1.1.1.1 hosts /etc/hosts log errors Cache } .:53 { forward . tls://2606:4700:4700::1111 tls://1.1.1.1 log errors Cache } We are using Cloudflare as a DNS provider(1.1.1.1). Create a new user for CoreDNS and set some permissions on the /opt/coredns directory. sudo useradd -d /var/lib/coredns -m coredns sudo chown coredns:coredns /opt/coredns Download the SystemD service unit file from coredns to /etc/systemd/system/coredns.service . wget https://github.com/coredns/deployment/blob/master/systemd/coredns.service mv coredns.service /etc/systemd/system/ Disable SystemD's default DNS server. sudo systemctl stop systemd-resolved && sudo systemctl disable systemd-resolved Note From that moment on, you will not be able to resolve any web pages anymore, unless you enable DNS again Enable and start CoreDNS sudo systemctl enable coredns && sudo systemctl start coredns Now we can able to resolve domain names, again. E.g. try to dig +short kit.edu . If an IP address is printed, everything works fine. dig +short kit.edu Output 141.3.128.6 Try the below command nslookup dnsserver.groophy.org Output Server : 192.168.0.100 Address : 192.168.0.100 # 53 Name : dnsserver . cruise . com Address : 192.168.0.100 Try the below command nslookup google.com Output Server : 192.168.0.100 Address : 192.168.0.100 # 53 Non - authoritative answer : Name : google . com Address : 142.250.71.14 Name : google . com Address : 2404 : 6800 : 4007 : 824 :: 200 e Conclusion Able to resolve both internal servers and outside the world.","title":"Setup"},{"location":"dhcp/","text":"What is DHCP? \u00b6 The Dynamic Host Configuration Protocol (DHCP) is a network service that enables host computers to be automatically assigned settings from a server as opposed to manually configuring each network host. Computers configured to be DHCP clients have no control over the settings they receive from the DHCP server, and the configuration is transparent to the computer\u2019s user. Our Requirements Configure DHCP on a standalone server with a single NIC. Setup: \u00b6 Configuring VLAN: \u00b6 Add VLAN 700 to the eth1 device. ip link add link eth1 name eth4 address 00 :11:22:33:44:55 type vlan id 700 It is not necessary, but you can disable IPv6 on this particular VLAN interface. sysctl -w net.ipv6.conf.eth1/700.disable_ipv6 = 1 net.ipv6.conf.eth1/700.disable_ipv6 = 1 Add an IPv4 address. ip addr add 10 .100.10.77/24 dev eth1.700 Bring VLAN interface up. ip link set dev eth4 up ip -detail addr show eth4 eth4 : < BROADCAST , MULTICAST , UP , LOWER_UP > mtu 1500 qdisc noqueue state UP group default qlen 1000 link / ether 08 : 00 : 27 : fa : 4 b : 19 brd ff : ff : ff : ff : ff : ff promiscuity 0 minmtu 0 maxmtu 65535 vlan protocol 802.1 Q id 700 < REORDER_HDR > numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 inet 10.100.10.77 / 24 scope global eth1 .700 To make VLAN 700 on the eth1 interface at the boot time just add the following configuration. vi /etc/network/interfaces # add vlan 700 on eth1 - static IP address auto eth4 iface eth4 inet static address 192.168.10.2 netmask 255.255.255.0 pre - up sysctl - w net . ipv6 . conf . eth1 / 700. disable_ipv6 = 1 These interfaces will be brought up in the order in which they were listed. Configuring DHCP server: \u00b6 First, update the system repository index in your system and install the DHCP server apt update && apt-get install isc-dhcp-server -y Specify the interface on which the DHCP server will listen to. vi /etc/default/isc-dhcp-server INTERFACESv4 = \"eth4\" Add below the lines in the DHCP configuration file. vi /etc/dhcp/dhcpd.conf authoritative ; option domain - name \"groophy.org\" ; option domain - name - servers 192.168.0.100 ; default - lease - time 600 ; max - lease - time 7200 ; ddns - update - style none ; subnet 192.168.10.0 netmask 255.255.255.0 { range 192.168.10.11 192.168.10.240 ; option routers 192.168.10.254 ; option subnet - mask 255.255.255.0 ; option broadcast - address 192.168.10.255 ; } Enable and restart the DHCP server service. Issue the below command in Terminal to do so: systemctl restart isc-dhcp-server.service systemctl status isc-dhcp-server.service DHCP Client Configuration \u00b6 Find your network interface name and update the network interface in the configuration. ifconfig vi /etc/network/interfaces auto ens33 iface ens33 inet dhcp Restart the network-manager service and verify the new IP. systemctl restart network-manager.service ifconfig","title":"DHCP"},{"location":"dhcp/#what-is-dhcp","text":"The Dynamic Host Configuration Protocol (DHCP) is a network service that enables host computers to be automatically assigned settings from a server as opposed to manually configuring each network host. Computers configured to be DHCP clients have no control over the settings they receive from the DHCP server, and the configuration is transparent to the computer\u2019s user. Our Requirements Configure DHCP on a standalone server with a single NIC.","title":"What is DHCP?"},{"location":"dhcp/#setup","text":"","title":"Setup:"},{"location":"dhcp/#configuring-vlan","text":"Add VLAN 700 to the eth1 device. ip link add link eth1 name eth4 address 00 :11:22:33:44:55 type vlan id 700 It is not necessary, but you can disable IPv6 on this particular VLAN interface. sysctl -w net.ipv6.conf.eth1/700.disable_ipv6 = 1 net.ipv6.conf.eth1/700.disable_ipv6 = 1 Add an IPv4 address. ip addr add 10 .100.10.77/24 dev eth1.700 Bring VLAN interface up. ip link set dev eth4 up ip -detail addr show eth4 eth4 : < BROADCAST , MULTICAST , UP , LOWER_UP > mtu 1500 qdisc noqueue state UP group default qlen 1000 link / ether 08 : 00 : 27 : fa : 4 b : 19 brd ff : ff : ff : ff : ff : ff promiscuity 0 minmtu 0 maxmtu 65535 vlan protocol 802.1 Q id 700 < REORDER_HDR > numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 inet 10.100.10.77 / 24 scope global eth1 .700 To make VLAN 700 on the eth1 interface at the boot time just add the following configuration. vi /etc/network/interfaces # add vlan 700 on eth1 - static IP address auto eth4 iface eth4 inet static address 192.168.10.2 netmask 255.255.255.0 pre - up sysctl - w net . ipv6 . conf . eth1 / 700. disable_ipv6 = 1 These interfaces will be brought up in the order in which they were listed.","title":"Configuring VLAN:"},{"location":"dhcp/#configuring-dhcp-server","text":"First, update the system repository index in your system and install the DHCP server apt update && apt-get install isc-dhcp-server -y Specify the interface on which the DHCP server will listen to. vi /etc/default/isc-dhcp-server INTERFACESv4 = \"eth4\" Add below the lines in the DHCP configuration file. vi /etc/dhcp/dhcpd.conf authoritative ; option domain - name \"groophy.org\" ; option domain - name - servers 192.168.0.100 ; default - lease - time 600 ; max - lease - time 7200 ; ddns - update - style none ; subnet 192.168.10.0 netmask 255.255.255.0 { range 192.168.10.11 192.168.10.240 ; option routers 192.168.10.254 ; option subnet - mask 255.255.255.0 ; option broadcast - address 192.168.10.255 ; } Enable and restart the DHCP server service. Issue the below command in Terminal to do so: systemctl restart isc-dhcp-server.service systemctl status isc-dhcp-server.service","title":"Configuring DHCP server:"},{"location":"dhcp/#dhcp-client-configuration","text":"Find your network interface name and update the network interface in the configuration. ifconfig vi /etc/network/interfaces auto ens33 iface ens33 inet dhcp Restart the network-manager service and verify the new IP. systemctl restart network-manager.service ifconfig","title":"DHCP Client Configuration"},{"location":"getting_started/","text":"Our Requirements \u00b6 Requirements Configure CoreDNS with the domain name \u201cgroophy.org\". Each server can be resolve by any server in the cluster for that solution we are using CoreDNS with the host file.","title":"Getting started"},{"location":"getting_started/#our-requirements","text":"Requirements Configure CoreDNS with the domain name \u201cgroophy.org\". Each server can be resolve by any server in the cluster for that solution we are using CoreDNS with the host file.","title":"Our Requirements"},{"location":"guide/","text":"Getting started \u00b6 Material for MkDocs is a theme for MkDocs , a static site generator geared towards (technical) project documentation. If you're familiar with Python, you can install Material for MkDocs with pip , the Python package manager. If not, we recommended using docker . Installation \u00b6 with pip recommended \u00b6 Material for MkDocs can be installed with pip : pip install mkdocs-material This will automatically install compatible versions of all dependencies: MkDocs , Markdown , Pygments and Python Markdown Extensions . Material for MkDocs always strives to support the latest versions, so there's no need to install those packages separately. Docker \u00b6 with docker \u00b6 The official Docker image is a great way to get up and running in a few minutes, as it comes with all dependencies pre-installed. Pull the image for the latest version with: docker pull squidfunk/mkdocs-material The mkdocs executable is provided as an entry point and serve is the default command. If you're not familiar with Docker don't worry, we have you covered in the following sections. The following plugins are bundled with the Docker image: mkdocs-minify-plugin mkdocs-redirects How to add plugins to the Docker image? Material for MkDocs bundles useful and common plugins while trying not to blow up the size of the official image. If the plugin you want to use is not included, create a new Dockerfile and extend the official Docker image with your custom installation routine: FROM squidfunk/mkdocs-material RUN pip install ... Next, you can build the image with the following command: docker build -t squidfunk/mkdocs-material . The new image can be used exactly like the official image. Apple Silicon (M1) and Raspberry Pi The official Docker image is only available for linux/amd64 . We recommend the third-party image by @afritzler if you want to run Material for MkDocs via Docker on arm64 or armv7 , as it is automatically built on every release: docker pull ghcr.io/afritzler/mkdocs-material GIT \u00b6 with git \u00b6 Material for MkDocs can be directly used from GitHub by cloning the repository into a subfolder of your project root which might be useful if you want to use the very latest version: git clone https://github.com/squidfunk/mkdocs-material.git The theme will reside in the folder mkdocs-material/material . When cloning from git , you must install all required dependencies yourself: pip install -e mkdocs-material","title":"Getting started"},{"location":"guide/#getting-started","text":"Material for MkDocs is a theme for MkDocs , a static site generator geared towards (technical) project documentation. If you're familiar with Python, you can install Material for MkDocs with pip , the Python package manager. If not, we recommended using docker .","title":"Getting started"},{"location":"guide/#installation","text":"","title":"Installation"},{"location":"guide/#with-pip","text":"Material for MkDocs can be installed with pip : pip install mkdocs-material This will automatically install compatible versions of all dependencies: MkDocs , Markdown , Pygments and Python Markdown Extensions . Material for MkDocs always strives to support the latest versions, so there's no need to install those packages separately.","title":"with pip"},{"location":"guide/#docker","text":"","title":"Docker"},{"location":"guide/#with-docker","text":"The official Docker image is a great way to get up and running in a few minutes, as it comes with all dependencies pre-installed. Pull the image for the latest version with: docker pull squidfunk/mkdocs-material The mkdocs executable is provided as an entry point and serve is the default command. If you're not familiar with Docker don't worry, we have you covered in the following sections. The following plugins are bundled with the Docker image: mkdocs-minify-plugin mkdocs-redirects How to add plugins to the Docker image? Material for MkDocs bundles useful and common plugins while trying not to blow up the size of the official image. If the plugin you want to use is not included, create a new Dockerfile and extend the official Docker image with your custom installation routine: FROM squidfunk/mkdocs-material RUN pip install ... Next, you can build the image with the following command: docker build -t squidfunk/mkdocs-material . The new image can be used exactly like the official image. Apple Silicon (M1) and Raspberry Pi The official Docker image is only available for linux/amd64 . We recommend the third-party image by @afritzler if you want to run Material for MkDocs via Docker on arm64 or armv7 , as it is automatically built on every release: docker pull ghcr.io/afritzler/mkdocs-material","title":"with docker"},{"location":"guide/#git","text":"","title":"GIT"},{"location":"guide/#with-git","text":"Material for MkDocs can be directly used from GitHub by cloning the repository into a subfolder of your project root which might be useful if you want to use the very latest version: git clone https://github.com/squidfunk/mkdocs-material.git The theme will reside in the folder mkdocs-material/material . When cloning from git , you must install all required dependencies yourself: pip install -e mkdocs-material","title":"with git"},{"location":"k0s/","text":"Kubernetes Setup with k0S \u00b6 Cluster Build Tool -K0s For this activity, deploy 6 vm\u2019s instances Node Name VM Details HA Proxy 1 gb ram, 1 cpu Control Plane (2 Nodes) 2 gb ram, 2 cpu, 50 GB HD Worker (2-Nodes) 2 gb ram, 2 cpu, 50 GB HD Bastion Node 2 gb ram, 2 cpu Configure HA proxy to build a Multi-master Kubernetes cluster: \u00b6 Download and install the haproxy apt-get update && apt-get install haproxy -y Update haproxy configuration with as below with the details of the Ansible and Master nodes IP vi /etc/haproxy/haproxy.cfg listen kubernetes - apiserver - https bind < LoadBalancer_Hostname / IP >: 8383 mode tcp option log - health - checks timeout client 3 h timeout server 3 h server < Master01_Hostname > < Master01_IP >: 6443 check check - ssl verify none inter 10000 server < Master02_Hostname > < Master02_IP >: 6443 check check - ssl verify none inter 10000 server < Master03_Hostname > < Master03_IP >: 6443 check check - ssl verify none inter 10000 balance roundrobin systemctl restart haproxy netstat -atnlp Deploying Multi-master Kubernetes cluster with K0S: \u00b6 Download the k0sctl binary. wget https://github.com/k0sproject/k0sctl/releases/download/v0.11.4/k0sctl-linux-x64 Make the k0sctl binary executable and move it to the Bin directory mv k0sctl-linux-x64 /usr/bin/k0sctl chmod +x /usr/bin/k0sctl Initialize the cluster and build the con k0sctl init > k0s/clusterconfig.yaml apiVersion : k0sctl.k0sproject.io/v1beta1 kind : Cluster metadata : name : k0s-cluster spec : hosts : - ssh : address : <Master01_IP> user : opsadmin port : 22 keyPath : loginkey.pem role : controller - ssh : address : <Master02_IP> user : opsadmin port : 22 keyPath : loginkey.pem role : controller - ssh : address : <Worker01_IP> user : opsadmin port : 22 keyPath : loginkey.pem role : worker - ssh : address : <Worker02_IP> user : opsadmin port : 22 keyPath : loginkey.pem role : worker k0s : config : spec : api : externalAddress : <LoadBalancer_Hostname/IP> sans : - <LoadBalancer_Hostname/IP> version : 1.22.3+k0s.0 Apply the cluster config file using k0sctl. k0sctl apply --config k0s/clusterconfig.yaml Validate the cluster. k0sctl kubectl get nodes","title":"K0s"},{"location":"k0s/#kubernetes-setup-with-k0s","text":"Cluster Build Tool -K0s For this activity, deploy 6 vm\u2019s instances Node Name VM Details HA Proxy 1 gb ram, 1 cpu Control Plane (2 Nodes) 2 gb ram, 2 cpu, 50 GB HD Worker (2-Nodes) 2 gb ram, 2 cpu, 50 GB HD Bastion Node 2 gb ram, 2 cpu","title":"Kubernetes Setup with k0S"},{"location":"k0s/#configure-ha-proxy-to-build-a-multi-master-kubernetes-cluster","text":"Download and install the haproxy apt-get update && apt-get install haproxy -y Update haproxy configuration with as below with the details of the Ansible and Master nodes IP vi /etc/haproxy/haproxy.cfg listen kubernetes - apiserver - https bind < LoadBalancer_Hostname / IP >: 8383 mode tcp option log - health - checks timeout client 3 h timeout server 3 h server < Master01_Hostname > < Master01_IP >: 6443 check check - ssl verify none inter 10000 server < Master02_Hostname > < Master02_IP >: 6443 check check - ssl verify none inter 10000 server < Master03_Hostname > < Master03_IP >: 6443 check check - ssl verify none inter 10000 balance roundrobin systemctl restart haproxy netstat -atnlp","title":"Configure HA proxy to build a Multi-master Kubernetes cluster:"},{"location":"k0s/#deploying-multi-master-kubernetes-cluster-with-k0s","text":"Download the k0sctl binary. wget https://github.com/k0sproject/k0sctl/releases/download/v0.11.4/k0sctl-linux-x64 Make the k0sctl binary executable and move it to the Bin directory mv k0sctl-linux-x64 /usr/bin/k0sctl chmod +x /usr/bin/k0sctl Initialize the cluster and build the con k0sctl init > k0s/clusterconfig.yaml apiVersion : k0sctl.k0sproject.io/v1beta1 kind : Cluster metadata : name : k0s-cluster spec : hosts : - ssh : address : <Master01_IP> user : opsadmin port : 22 keyPath : loginkey.pem role : controller - ssh : address : <Master02_IP> user : opsadmin port : 22 keyPath : loginkey.pem role : controller - ssh : address : <Worker01_IP> user : opsadmin port : 22 keyPath : loginkey.pem role : worker - ssh : address : <Worker02_IP> user : opsadmin port : 22 keyPath : loginkey.pem role : worker k0s : config : spec : api : externalAddress : <LoadBalancer_Hostname/IP> sans : - <LoadBalancer_Hostname/IP> version : 1.22.3+k0s.0 Apply the cluster config file using k0sctl. k0sctl apply --config k0s/clusterconfig.yaml Validate the cluster. k0sctl kubectl get nodes","title":"Deploying Multi-master Kubernetes cluster with K0S:"},{"location":"k8s_backup_restore/","text":"Kubernetes Setup With Backup and Restiore Solution \u00b6 Cluster Build Tool -Kubespray Object Storage -Minio Backup Solution -Velero For this activity, deploy 5 AWS instances with the Security groups All traffic is enabled between the instances All traffic enabled for the testing machine(Our Laptop) Node Name Instance Details Ansible Node -t2.micro Control Plane (2-Nodes) -t2.medium Worker (2-Nodes) -t2.medium Download the pem-key and transfer the pem-key to ansible node. scp -i <pemkey> <pemkey> <user>@<IP>:/home/ubuntu/ cp /home/ubuntu/<pemkey> . chmod 400 <pemkey> vi /etc/hosts 172.31.27.136 cruise . org Configure HA proxy to build a Multi-master Kubernetes cluster: \u00b6 Download and install the haproxy apt-get update && apt-get install haproxy -y Update haproxy configuration with as below with the details of the Ansible and Master nodes IP vi /etc/haproxy/haproxy.cfg listen kubernetes - apiserver - https bind ansiblenodeIP : 8383 mode tcp option log - health - checks timeout client 3 h timeout server 3 h server master1 < IP1 >: 6443 check check - ssl verify none inter 10000 server master2 < IP2 >: 6443 check check - ssl verify none inter 10000 balance roundrobin systemctl restart haproxy netstat -atnlp Deploying Multi-master Kubernetes cluster with Kubespray: \u00b6 Install Ansible and PIP apt update && apt install software-properties-common -y add-apt-repository --yes --update ppa:ansible/ansible apt install ansible -y ansible --version apt install python3-pip -y Clone the Kubespray source code and install requirements through Pip. git clone https://github.com/kubernetes-sigs/kubespray.git cd kubespray/ pip3 install -r requirements.txt cp -rfp inventory/sample inventory/mycluster declare -a IPS =( <master01_IP> <master02_IP> <Worker01_IP> <Worker02_IP> ) CONFIG_FILE = inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${ IPS [@] } Update the HA-Proxy server details \u00b6 vi inventory/mycluster/group_vars/all/all.yml ## External LB example config apiserver_loadbalancer_domain_name : \"cruise.org\" loadbalancer_apiserver : address : 172.31.27.136 port : 8383 ## Internal loadbalancers for apiservers loadbalancer_apiserver_localhost : false Enable Kubernetes dashboard by updating the below files vi inventory/sample/group_vars/k8s_cluster/addons.yml vi inventory/mycluster/group_vars/k8s_cluster/addons.yml # Kubernetes dashboard # RBAC required. see docs/getting-started.md for access details. dashboard_enabled : true Deploy cluster with below Ansible command. ansible all -m ping -i inventory/mycluster/hosts.yaml --user ubuntu --private-key ../kmvelero.pem ansible-playbook -i inventory/mycluster/hosts.yaml --user ubuntu --private-key kmvelero.pem --become cluster.yml On Master Node: kubectl get nodes Output NAME STATUS ROLES AGE VERSION node1 Ready control - plane , master 5 h14m v1 .22.3 node2 Ready control - plane , master 5 h14m v1 .22.3 node3 Ready < none > 5 h13m v1 .22.3 node4 Ready < none > 5 h13m v1 .22.3 Configure Minio on the Ansible Node: \u00b6 wget https://dl.min.io/server/minio/release/linux-amd64/minio chmod +x minio ll /usr/bin/minio export MINIO_ROOT_USER = <Username> export MINIO_ROOT_PASSWORD = <Password> nohup minio server --console-address <AnsibleNodeIP>:<Port> /data > /dev/null 2 > & 1 & Manually create the bucket by accessing the Minio Console. Configure Velero: \u00b6 Switch to any master and download the velero source code and configure it. Kubernetes Master Node01 ssh -i kmvelero.pem <user>@<IP> wget https://github.com/vmware-tanzu/velero/releases/download/v1.7.0/velero-v1.7.0-linux-amd64.tar.gz tar -xvzf velero-v1.7.0-linux-amd64.tar.gz mv velero-v1.7.0-linux-amd64/velero /usr/local/bin/ wget https://github.com/digitalocean/velero-plugin/archive/refs/tags/v1.0.0.tar.gz nohup tar -xvzf v1.0.0.tar.gz source < ( velero completion bash ) Provide the username and password of the Minio console. cp velero-plugin-1.0.0/examples/cloud-credentials cloud-credentials vi cloud-credentials [default] aws_access_key_id = <MinioUsername> aws_secret_access_key = <MinioPassword> Initiate Velero to store the backup in the Minio buckets. velero install --provider aws --plugins velero/velero-plugin-for-aws:v1.0.0 --bucket <bucketname created in the minio console> -- backup-location-configregion = minio,s3ForcePathStyle = true,s3Url = http://ansible_host ( minio ) :9000 --secret-file <CredentialFile> Validate the access: velero get backup-location output NAME PROVIDER BUCKET / PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT default aws kmvbucket Available 2021-11-09 12 : 08 : 44 + 0000 UTC ReadWrite true Copy the kubeconfig file to the Ansible node. cp /etc/kubernetes/admin.conf /home/ubuntu/ chmod 755 /home/ubuntu/admin.conf scp -i kmvelero.pem <user>@<IP>:/home/ubuntu/admin.conf kubeconfig Configure Velero on the Ansible Node to access it from out of the cluster: wget https://github.com/vmware-tanzu/velero/releases/download/v1.7.0/velero-v1.7.0-linux-amd64.tar.gz nohup tar -xvzf velero-v1.7.0-linux-amd64.tar.gz mv velero-v1.7.0-linux-amd64/velero /usr/local/bin/ wget https://github.com/digitalocean/velero-plugin/archive/refs/tags/v1.0.0.tar.gz nohup tar -xvzf v1.0.0.tar.gz cp velero-plugin-1.0.0/examples/cloud-credentials cloud-credentials vi cloud-credentials source < ( velero completion bash ) vi /etc/hosts Validate the access from the Ansible node: \u00b6 velero get backup-locations --kubeconfig kubeconfig Output NAME PROVIDER BUCKET / PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT default aws kmvbucket Available 2021-11-09 12 : 08 : 44 + 0000 UTC ReadWrite true Backup & Restore: \u00b6 Backup: \u00b6 Creating a namespace and deployment in it. \u00b6 On Master01: \u00b6 kubectl create ns testing kubectl -n testing apply -f nginx.yaml ### Deploying 4 nginx pods On Ansible node: \u00b6 velero backup create firstbackup --include-namespaces testing --kubeconfig kubeconfig velero get backup --kubeconfig kubeconfig output NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR firstbackup Completed 0 0 2021-11-09 12 : 11 : 30 + 0000 UTC 29 d default < none > Restore: \u00b6 Deleting the Namespace Testing to perform restore On Master01: \u00b6 kubectl delete namespace testing On Ansible node: \u00b6 velero restore create firstbackup-restore --from-backup firstbackup --kubeconfig kubeconfig velero get restores --kubeconfig kubeconfig Output NAME BACKUP STATUS STARTED COMPLETED ERRORS WARNINGS CREATED SELECTOR firstbackup - restore firstbackup Completed 2021-11-08 11 : 20 : 35 + 0000 UTC 2021-11-08 11 : 20 : 36 + 0000 UTC 0 0 2021-11-08 11 : 20 : 35 + 0000 UTC < none > On Master01: \u00b6 kubectl get deployments.apps -n testing Output NAME READY UP - TO - DATE AVAILABLE AGE nginx - deployment 4 / 4 4 4 155 m Scheduling backup: \u00b6 velero create schedule initbackup1 --schedule = \"13 12 * * *\" --kubeconfig kubeconfig --include-namespaces testing velero get schedules --kubeconfig kubeconfig Note Velero commands can also be executed from the Master node.","title":"Kubernetes Setup With Backup and Restore Solution"},{"location":"k8s_backup_restore/#kubernetes-setup-with-backup-and-restiore-solution","text":"Cluster Build Tool -Kubespray Object Storage -Minio Backup Solution -Velero For this activity, deploy 5 AWS instances with the Security groups All traffic is enabled between the instances All traffic enabled for the testing machine(Our Laptop) Node Name Instance Details Ansible Node -t2.micro Control Plane (2-Nodes) -t2.medium Worker (2-Nodes) -t2.medium Download the pem-key and transfer the pem-key to ansible node. scp -i <pemkey> <pemkey> <user>@<IP>:/home/ubuntu/ cp /home/ubuntu/<pemkey> . chmod 400 <pemkey> vi /etc/hosts 172.31.27.136 cruise . org","title":"Kubernetes Setup With Backup and Restiore Solution"},{"location":"k8s_backup_restore/#configure-ha-proxy-to-build-a-multi-master-kubernetes-cluster","text":"Download and install the haproxy apt-get update && apt-get install haproxy -y Update haproxy configuration with as below with the details of the Ansible and Master nodes IP vi /etc/haproxy/haproxy.cfg listen kubernetes - apiserver - https bind ansiblenodeIP : 8383 mode tcp option log - health - checks timeout client 3 h timeout server 3 h server master1 < IP1 >: 6443 check check - ssl verify none inter 10000 server master2 < IP2 >: 6443 check check - ssl verify none inter 10000 balance roundrobin systemctl restart haproxy netstat -atnlp","title":"Configure HA proxy to build a Multi-master Kubernetes cluster:"},{"location":"k8s_backup_restore/#deploying-multi-master-kubernetes-cluster-with-kubespray","text":"Install Ansible and PIP apt update && apt install software-properties-common -y add-apt-repository --yes --update ppa:ansible/ansible apt install ansible -y ansible --version apt install python3-pip -y Clone the Kubespray source code and install requirements through Pip. git clone https://github.com/kubernetes-sigs/kubespray.git cd kubespray/ pip3 install -r requirements.txt cp -rfp inventory/sample inventory/mycluster declare -a IPS =( <master01_IP> <master02_IP> <Worker01_IP> <Worker02_IP> ) CONFIG_FILE = inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${ IPS [@] }","title":"Deploying Multi-master Kubernetes cluster with Kubespray:"},{"location":"k8s_backup_restore/#update-the-ha-proxy-server-details","text":"vi inventory/mycluster/group_vars/all/all.yml ## External LB example config apiserver_loadbalancer_domain_name : \"cruise.org\" loadbalancer_apiserver : address : 172.31.27.136 port : 8383 ## Internal loadbalancers for apiservers loadbalancer_apiserver_localhost : false Enable Kubernetes dashboard by updating the below files vi inventory/sample/group_vars/k8s_cluster/addons.yml vi inventory/mycluster/group_vars/k8s_cluster/addons.yml # Kubernetes dashboard # RBAC required. see docs/getting-started.md for access details. dashboard_enabled : true Deploy cluster with below Ansible command. ansible all -m ping -i inventory/mycluster/hosts.yaml --user ubuntu --private-key ../kmvelero.pem ansible-playbook -i inventory/mycluster/hosts.yaml --user ubuntu --private-key kmvelero.pem --become cluster.yml On Master Node: kubectl get nodes Output NAME STATUS ROLES AGE VERSION node1 Ready control - plane , master 5 h14m v1 .22.3 node2 Ready control - plane , master 5 h14m v1 .22.3 node3 Ready < none > 5 h13m v1 .22.3 node4 Ready < none > 5 h13m v1 .22.3","title":"Update the HA-Proxy server details"},{"location":"k8s_backup_restore/#configure-minio-on-the-ansible-node","text":"wget https://dl.min.io/server/minio/release/linux-amd64/minio chmod +x minio ll /usr/bin/minio export MINIO_ROOT_USER = <Username> export MINIO_ROOT_PASSWORD = <Password> nohup minio server --console-address <AnsibleNodeIP>:<Port> /data > /dev/null 2 > & 1 & Manually create the bucket by accessing the Minio Console.","title":"Configure Minio on the Ansible Node:"},{"location":"k8s_backup_restore/#configure-velero","text":"Switch to any master and download the velero source code and configure it. Kubernetes Master Node01 ssh -i kmvelero.pem <user>@<IP> wget https://github.com/vmware-tanzu/velero/releases/download/v1.7.0/velero-v1.7.0-linux-amd64.tar.gz tar -xvzf velero-v1.7.0-linux-amd64.tar.gz mv velero-v1.7.0-linux-amd64/velero /usr/local/bin/ wget https://github.com/digitalocean/velero-plugin/archive/refs/tags/v1.0.0.tar.gz nohup tar -xvzf v1.0.0.tar.gz source < ( velero completion bash ) Provide the username and password of the Minio console. cp velero-plugin-1.0.0/examples/cloud-credentials cloud-credentials vi cloud-credentials [default] aws_access_key_id = <MinioUsername> aws_secret_access_key = <MinioPassword> Initiate Velero to store the backup in the Minio buckets. velero install --provider aws --plugins velero/velero-plugin-for-aws:v1.0.0 --bucket <bucketname created in the minio console> -- backup-location-configregion = minio,s3ForcePathStyle = true,s3Url = http://ansible_host ( minio ) :9000 --secret-file <CredentialFile> Validate the access: velero get backup-location output NAME PROVIDER BUCKET / PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT default aws kmvbucket Available 2021-11-09 12 : 08 : 44 + 0000 UTC ReadWrite true Copy the kubeconfig file to the Ansible node. cp /etc/kubernetes/admin.conf /home/ubuntu/ chmod 755 /home/ubuntu/admin.conf scp -i kmvelero.pem <user>@<IP>:/home/ubuntu/admin.conf kubeconfig Configure Velero on the Ansible Node to access it from out of the cluster: wget https://github.com/vmware-tanzu/velero/releases/download/v1.7.0/velero-v1.7.0-linux-amd64.tar.gz nohup tar -xvzf velero-v1.7.0-linux-amd64.tar.gz mv velero-v1.7.0-linux-amd64/velero /usr/local/bin/ wget https://github.com/digitalocean/velero-plugin/archive/refs/tags/v1.0.0.tar.gz nohup tar -xvzf v1.0.0.tar.gz cp velero-plugin-1.0.0/examples/cloud-credentials cloud-credentials vi cloud-credentials source < ( velero completion bash ) vi /etc/hosts","title":"Configure Velero:"},{"location":"k8s_backup_restore/#validate-the-access-from-the-ansible-node","text":"velero get backup-locations --kubeconfig kubeconfig Output NAME PROVIDER BUCKET / PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT default aws kmvbucket Available 2021-11-09 12 : 08 : 44 + 0000 UTC ReadWrite true","title":"Validate the access from the Ansible node:"},{"location":"k8s_backup_restore/#backup-restore","text":"","title":"Backup &amp; Restore: "},{"location":"k8s_backup_restore/#backup","text":"","title":"Backup:"},{"location":"k8s_backup_restore/#creating-a-namespace-and-deployment-in-it","text":"","title":"Creating a namespace and deployment in it."},{"location":"k8s_backup_restore/#on-master01","text":"kubectl create ns testing kubectl -n testing apply -f nginx.yaml ### Deploying 4 nginx pods","title":"On Master01:"},{"location":"k8s_backup_restore/#on-ansible-node","text":"velero backup create firstbackup --include-namespaces testing --kubeconfig kubeconfig velero get backup --kubeconfig kubeconfig output NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR firstbackup Completed 0 0 2021-11-09 12 : 11 : 30 + 0000 UTC 29 d default < none >","title":"On Ansible node:"},{"location":"k8s_backup_restore/#restore","text":"Deleting the Namespace Testing to perform restore","title":"Restore:"},{"location":"k8s_backup_restore/#on-master01_1","text":"kubectl delete namespace testing","title":"On Master01:"},{"location":"k8s_backup_restore/#on-ansible-node_1","text":"velero restore create firstbackup-restore --from-backup firstbackup --kubeconfig kubeconfig velero get restores --kubeconfig kubeconfig Output NAME BACKUP STATUS STARTED COMPLETED ERRORS WARNINGS CREATED SELECTOR firstbackup - restore firstbackup Completed 2021-11-08 11 : 20 : 35 + 0000 UTC 2021-11-08 11 : 20 : 36 + 0000 UTC 0 0 2021-11-08 11 : 20 : 35 + 0000 UTC < none >","title":"On Ansible node:"},{"location":"k8s_backup_restore/#on-master01_2","text":"kubectl get deployments.apps -n testing Output NAME READY UP - TO - DATE AVAILABLE AGE nginx - deployment 4 / 4 4 4 155 m","title":"On Master01:"},{"location":"k8s_backup_restore/#scheduling-backup","text":"velero create schedule initbackup1 --schedule = \"13 12 * * *\" --kubeconfig kubeconfig --include-namespaces testing velero get schedules --kubeconfig kubeconfig Note Velero commands can also be executed from the Master node.","title":"Scheduling backup:"},{"location":"k8s_dashboard/","text":"Deploy the latest Kubernetes dashboard \u00b6 Once you\u2019ve set up your Kubernetes cluster or if you already had one running, we can get started. The first thing to know about the web UI is that it can only be accessed using localhost address on the machine it runs on. This means we need to have an SSH tunnel to the server. For most OS, you can create an SSH tunnel using this command. Replace the and with the relevant details to your Kubernetes cluster. ssh -L localhost:8001:127.0.0.1:8001 <user>@<master_public_IP> After you\u2019ve logged in, you can deploy the dashboard itself with the following single command. kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml If your cluster is working correctly, you should see an output confirming the creation of a bunch of Kubernetes components like in the example below. Output namespace / kubernetes - dashboard created serviceaccount / kubernetes - dashboard created service / kubernetes - dashboard created secret / kubernetes - dashboard - certs created secret / kubernetes - dashboard - csrf created secret / kubernetes - dashboard - key - holder created configmap / kubernetes - dashboard - settings created role . rbac . authorization . k8s . io / kubernetes - dashboard created clusterrole . rbac . authorization . k8s . io / kubernetes - dashboard created rolebinding . rbac . authorization . k8s . io / kubernetes - dashboard created clusterrolebinding . rbac . authorization . k8s . io / kubernetes - dashboard created deployment . apps / kubernetes - dashboard created service / dashboard - metrics - scraper created deployment . apps / dashboard - metrics - scraper created Afterwards, you should have two new pods running on your cluster. kubectl get pods -A Output ... kubernetes - dashboard dashboard - metrics - scraper -78u jd94gf7 - ff6h8 1 / 1 Running 0 30 m kubernetes - dashboard kubernetes - dashboard - g6hujkirf7 - df65g 1 / 1 Running 0 30 m You can then continue ahead with creating the required user accounts. Creating Admin user \u00b6 The Kubernetes dashboard supports a few ways to manage access control. In this example, we\u2019ll be creating an admin user account with full privileges to modify the cluster and using tokens. Start by making a new directory for the dashboard configuration files. mkdir ~/dashboard && cd ~/dashboard Create the following configuration and save it as dashboard-admin.yaml file. Note that indentation matters in the YAML files which should use two spaces in a regular text editor. vim dashboard-admin.yaml apiVersion : v1 kind : ServiceAccount metadata : name : admin-user namespace : kubernetes-dashboard --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : admin-user roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin subjects : - kind : ServiceAccount name : admin-user namespace : kubernetes-dashboard Once set, save the file and exit the editor. Then deploy the admin user role with the next command. kubectl apply -f dashboard-admin.yaml You should see a service account and a cluster role binding created. Output serviceaccount / admin - user created clusterrolebinding . rbac . authorization . k8s . io / admin - user created Using this method doesn\u2019t require setting up or memorising passwords, instead, accessing the dashboard will require a token. Get the admin token using the command below. kubectl get secret -n kubernetes-dashboard $( kubectl get serviceaccount admin-user -n kubernetes-dashboard -o jsonpath = \"{.secrets[0].name}\" ) -o jsonpath = \"{.data.token}\" | base64 --decode You\u2019ll then see an output of a long string of seemingly random characters like in the example below. Output eyJhbGciOiJSUzI1NiIsImtpZCI6Ilk2eEd2QjJMVkhIRWNfN2xTMlA5N2RNVlR5N0o1REFET0dp dkRmel90aWMifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlc y5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1Y mVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuL XEyZGJzIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZ SI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb 3VudC51aWQiOiI1ODI5OTUxMS1hN2ZlLTQzZTQtODk3MC0yMjllOTM1YmExNDkiLCJzdWIiOiJze XN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.GcUs MMx4GnSV1hxQv01zX1nxXMZdKO7tU2OCu0TbJpPhJ9NhEidttOw5ENRosx7EqiffD3zdLDptS22F gnDqRDW8OIpVZH2oQbR153EyP_l7ct9_kQVv1vFCL3fAmdrUwY5p1-YMC41OUYORy1JPo5wkpXrW OytnsfWUbZBF475Wd3Gq3WdBHMTY4w3FarlJsvk76WgalnCtec4AVsEGxM0hS0LgQ-cGug7iGbmf cY7odZDaz5lmxAflpE5S4m-AwsTvT42ENh_bq8PS7FsMd8mK9nELyQu_a-yocYUggju_m-BxLjgc 2cLh5WzVbTH_ztW7COlKWvSVg7yhjikrew The token is created each time the dashboard is deployed and is required to log into the dashboard. Note that the token will change if the dashboard is stopped and redeployed. Creating Read-Only user \u00b6 If you wish to provide access to your Kubernetes dashboard, for example, for demonstrative purposes, you can create a read-only view for the cluster. Similarly to the admin account, save the following configuration in dashboard-read-only.yaml vim dashboard-read-only.yaml apiVersion : v1 kind : ServiceAccount metadata : name : read-only-user namespace : kubernetes-dashboard --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : annotations : rbac.authorization.kubernetes.io/autoupdate : \"true\" labels : name : read-only-clusterrole namespace : default rules : - apiGroups : - \"\" resources : [ \"*\" ] verbs : - get - list - watch - apiGroups : - extensions resources : [ \"*\" ] verbs : - get - list - watch - apiGroups : - apps resources : [ \"*\" ] verbs : - get - list - watch --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : read-only-binding roleRef : kind : ClusterRole name : read-only-clusterrole apiGroup : rbac.authorization.k8s.io subjects : - kind : ServiceAccount name : read-only-user namespace : kubernetes-dashboard Once set, save the file and exit the editor. Then deploy the read-only user account with the command below. kubectl apply -f dashboard-read-only.yaml To allow users to log in via the read-only account, you\u2019ll need to provide a token which can be fetched using the next command. kubectl get secret -n kubernetes-dashboard $( kubectl get serviceaccount read-only-user -n kubernetes-dashboard -o jsonpath = \"{.secrets[0].name}\" ) -o jsonpath = \"{.data.token}\" | base64 --decode The toke will be a long series of characters and unique to the dashboard currently running. Accessing the dashboard \u00b6 We\u2019ve now deployed the dashboard and created user accounts for it. Next, we can get started managing the Kubernetes cluster itself. However, before we can log in to the dashboard, it needs to be made available by creating a proxy service on the localhost. Run the next command on your Kubernetes cluster. kubectl proxy This will start the server at 127.0.0.1:8001 as shown by the output. Output Starting to serve on 127.0.0.1:8001 Now, assuming that we have already established an SSH tunnel binding to the localhost port 8001 at both ends, open a browser to the link below. Link http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ If everything is running correctly, you should see the dashboard login window. Select the token authentication method and copy your admin token into the field below. Then click the Sign in button. You will then be greeted by the overview of your Kubernetes cluster. While signed in as an admin, you can deploy new pods and services quickly and easily by clicking the plus icon at the top right corner of the dashboard. Then either copy in any configuration file you wish, select the file directly from your machine or create a new configuration from a form.","title":"K8s Dashboard"},{"location":"k8s_dashboard/#deploy-the-latest-kubernetes-dashboard","text":"Once you\u2019ve set up your Kubernetes cluster or if you already had one running, we can get started. The first thing to know about the web UI is that it can only be accessed using localhost address on the machine it runs on. This means we need to have an SSH tunnel to the server. For most OS, you can create an SSH tunnel using this command. Replace the and with the relevant details to your Kubernetes cluster. ssh -L localhost:8001:127.0.0.1:8001 <user>@<master_public_IP> After you\u2019ve logged in, you can deploy the dashboard itself with the following single command. kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml If your cluster is working correctly, you should see an output confirming the creation of a bunch of Kubernetes components like in the example below. Output namespace / kubernetes - dashboard created serviceaccount / kubernetes - dashboard created service / kubernetes - dashboard created secret / kubernetes - dashboard - certs created secret / kubernetes - dashboard - csrf created secret / kubernetes - dashboard - key - holder created configmap / kubernetes - dashboard - settings created role . rbac . authorization . k8s . io / kubernetes - dashboard created clusterrole . rbac . authorization . k8s . io / kubernetes - dashboard created rolebinding . rbac . authorization . k8s . io / kubernetes - dashboard created clusterrolebinding . rbac . authorization . k8s . io / kubernetes - dashboard created deployment . apps / kubernetes - dashboard created service / dashboard - metrics - scraper created deployment . apps / dashboard - metrics - scraper created Afterwards, you should have two new pods running on your cluster. kubectl get pods -A Output ... kubernetes - dashboard dashboard - metrics - scraper -78u jd94gf7 - ff6h8 1 / 1 Running 0 30 m kubernetes - dashboard kubernetes - dashboard - g6hujkirf7 - df65g 1 / 1 Running 0 30 m You can then continue ahead with creating the required user accounts.","title":"Deploy the latest Kubernetes dashboard"},{"location":"k8s_dashboard/#creating-admin-user","text":"The Kubernetes dashboard supports a few ways to manage access control. In this example, we\u2019ll be creating an admin user account with full privileges to modify the cluster and using tokens. Start by making a new directory for the dashboard configuration files. mkdir ~/dashboard && cd ~/dashboard Create the following configuration and save it as dashboard-admin.yaml file. Note that indentation matters in the YAML files which should use two spaces in a regular text editor. vim dashboard-admin.yaml apiVersion : v1 kind : ServiceAccount metadata : name : admin-user namespace : kubernetes-dashboard --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : admin-user roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin subjects : - kind : ServiceAccount name : admin-user namespace : kubernetes-dashboard Once set, save the file and exit the editor. Then deploy the admin user role with the next command. kubectl apply -f dashboard-admin.yaml You should see a service account and a cluster role binding created. Output serviceaccount / admin - user created clusterrolebinding . rbac . authorization . k8s . io / admin - user created Using this method doesn\u2019t require setting up or memorising passwords, instead, accessing the dashboard will require a token. Get the admin token using the command below. kubectl get secret -n kubernetes-dashboard $( kubectl get serviceaccount admin-user -n kubernetes-dashboard -o jsonpath = \"{.secrets[0].name}\" ) -o jsonpath = \"{.data.token}\" | base64 --decode You\u2019ll then see an output of a long string of seemingly random characters like in the example below. Output eyJhbGciOiJSUzI1NiIsImtpZCI6Ilk2eEd2QjJMVkhIRWNfN2xTMlA5N2RNVlR5N0o1REFET0dp dkRmel90aWMifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlc y5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1Y mVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuL XEyZGJzIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZ SI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb 3VudC51aWQiOiI1ODI5OTUxMS1hN2ZlLTQzZTQtODk3MC0yMjllOTM1YmExNDkiLCJzdWIiOiJze XN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.GcUs MMx4GnSV1hxQv01zX1nxXMZdKO7tU2OCu0TbJpPhJ9NhEidttOw5ENRosx7EqiffD3zdLDptS22F gnDqRDW8OIpVZH2oQbR153EyP_l7ct9_kQVv1vFCL3fAmdrUwY5p1-YMC41OUYORy1JPo5wkpXrW OytnsfWUbZBF475Wd3Gq3WdBHMTY4w3FarlJsvk76WgalnCtec4AVsEGxM0hS0LgQ-cGug7iGbmf cY7odZDaz5lmxAflpE5S4m-AwsTvT42ENh_bq8PS7FsMd8mK9nELyQu_a-yocYUggju_m-BxLjgc 2cLh5WzVbTH_ztW7COlKWvSVg7yhjikrew The token is created each time the dashboard is deployed and is required to log into the dashboard. Note that the token will change if the dashboard is stopped and redeployed.","title":"Creating Admin user"},{"location":"k8s_dashboard/#creating-read-only-user","text":"If you wish to provide access to your Kubernetes dashboard, for example, for demonstrative purposes, you can create a read-only view for the cluster. Similarly to the admin account, save the following configuration in dashboard-read-only.yaml vim dashboard-read-only.yaml apiVersion : v1 kind : ServiceAccount metadata : name : read-only-user namespace : kubernetes-dashboard --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : annotations : rbac.authorization.kubernetes.io/autoupdate : \"true\" labels : name : read-only-clusterrole namespace : default rules : - apiGroups : - \"\" resources : [ \"*\" ] verbs : - get - list - watch - apiGroups : - extensions resources : [ \"*\" ] verbs : - get - list - watch - apiGroups : - apps resources : [ \"*\" ] verbs : - get - list - watch --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : read-only-binding roleRef : kind : ClusterRole name : read-only-clusterrole apiGroup : rbac.authorization.k8s.io subjects : - kind : ServiceAccount name : read-only-user namespace : kubernetes-dashboard Once set, save the file and exit the editor. Then deploy the read-only user account with the command below. kubectl apply -f dashboard-read-only.yaml To allow users to log in via the read-only account, you\u2019ll need to provide a token which can be fetched using the next command. kubectl get secret -n kubernetes-dashboard $( kubectl get serviceaccount read-only-user -n kubernetes-dashboard -o jsonpath = \"{.secrets[0].name}\" ) -o jsonpath = \"{.data.token}\" | base64 --decode The toke will be a long series of characters and unique to the dashboard currently running.","title":"Creating Read-Only user"},{"location":"k8s_dashboard/#accessing-the-dashboard","text":"We\u2019ve now deployed the dashboard and created user accounts for it. Next, we can get started managing the Kubernetes cluster itself. However, before we can log in to the dashboard, it needs to be made available by creating a proxy service on the localhost. Run the next command on your Kubernetes cluster. kubectl proxy This will start the server at 127.0.0.1:8001 as shown by the output. Output Starting to serve on 127.0.0.1:8001 Now, assuming that we have already established an SSH tunnel binding to the localhost port 8001 at both ends, open a browser to the link below. Link http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ If everything is running correctly, you should see the dashboard login window. Select the token authentication method and copy your admin token into the field below. Then click the Sign in button. You will then be greeted by the overview of your Kubernetes cluster. While signed in as an admin, you can deploy new pods and services quickly and easily by clicking the plus icon at the top right corner of the dashboard. Then either copy in any configuration file you wish, select the file directly from your machine or create a new configuration from a form.","title":"Accessing the dashboard"},{"location":"k8s_monitoring/","text":"K8S Monitoring \u00b6 Installing the Prometheus and Grafana using the Helm. Configuring the Prometheus on Nodeport service on port 32322 and Grafana on port 32323 Install HELM: \u00b6 curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh Install Prometheus: \u00b6 Create PVC to dynamically cat yaml_files/prometheus_pvc.yaml apiVersion : v1 kind : PersistentVolumeClaim metadata : name : vprodbvol - nfs - claim namespace : prometheus spec : accessModes : - ReadWriteMany storageClassName : managed - nfs - storage resources : requests : storage : 2 Gi kubectl get sc --no-headers | awk '{print $1}' storage-nfs Configuring Prometheus \u00b6 Add the Prometheus repo helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm inspect values prometheus-community/prometheus > /tmp/prometheus.values Provide the load balancer address, create a Nodeport service(Cluster IP is defined) and provide storageClass to dynamically claim the PV vi /tmp/prometheus.values Install the helm chart with defined variables. helm install prometheus prometheus-community/prometheus --values /tmp/prometheus.values --namespace prometheus Validate the prometheus pods and dynamically created PV. watch kubectl get pods -n prometheus kubectl get pvc -n prometheus Configuring Grafana: \u00b6 Add the Grafana repo helm repo add grafana https://grafana.github.io/helm-charts helm search repo grafana helm inspect values grafana/grafana > /tmp/grafana.values Provide the load balancer address, create a Nodeport service(Cluster IP is defined) and provide storageClass to dynamically claim the PV vi /tmp/grafana.values Install the grafana helm chart with defined variables. helm install grafana grafana/grafana --values /tmp/grafana.values --namespace grafana Validate the prometheus pods and dynamically created PV. watch kubectl get pods -n grafana kubectl get pvc -n grafana Reference: https://www.youtube.com/watch?v=CmPdyvgmw-A https://logz.io/blog/kubernetes-monitoring/ https://grafana.com/grafana/dashboards/6417","title":"Monitoring"},{"location":"k8s_monitoring/#k8s-monitoring","text":"Installing the Prometheus and Grafana using the Helm. Configuring the Prometheus on Nodeport service on port 32322 and Grafana on port 32323","title":"K8S Monitoring"},{"location":"k8s_monitoring/#install-helm","text":"curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh","title":"Install HELM:"},{"location":"k8s_monitoring/#install-prometheus","text":"Create PVC to dynamically cat yaml_files/prometheus_pvc.yaml apiVersion : v1 kind : PersistentVolumeClaim metadata : name : vprodbvol - nfs - claim namespace : prometheus spec : accessModes : - ReadWriteMany storageClassName : managed - nfs - storage resources : requests : storage : 2 Gi kubectl get sc --no-headers | awk '{print $1}' storage-nfs","title":"Install Prometheus:"},{"location":"k8s_monitoring/#configuring-prometheus","text":"Add the Prometheus repo helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm inspect values prometheus-community/prometheus > /tmp/prometheus.values Provide the load balancer address, create a Nodeport service(Cluster IP is defined) and provide storageClass to dynamically claim the PV vi /tmp/prometheus.values Install the helm chart with defined variables. helm install prometheus prometheus-community/prometheus --values /tmp/prometheus.values --namespace prometheus Validate the prometheus pods and dynamically created PV. watch kubectl get pods -n prometheus kubectl get pvc -n prometheus","title":"Configuring Prometheus"},{"location":"k8s_monitoring/#configuring-grafana","text":"Add the Grafana repo helm repo add grafana https://grafana.github.io/helm-charts helm search repo grafana helm inspect values grafana/grafana > /tmp/grafana.values Provide the load balancer address, create a Nodeport service(Cluster IP is defined) and provide storageClass to dynamically claim the PV vi /tmp/grafana.values Install the grafana helm chart with defined variables. helm install grafana grafana/grafana --values /tmp/grafana.values --namespace grafana Validate the prometheus pods and dynamically created PV. watch kubectl get pods -n grafana kubectl get pvc -n grafana Reference: https://www.youtube.com/watch?v=CmPdyvgmw-A https://logz.io/blog/kubernetes-monitoring/ https://grafana.com/grafana/dashboards/6417","title":"Configuring Grafana:"},{"location":"klusternetes/","text":"Klusternetes \u00b6 Cluster Build Tool -k0s Configure the cluster : \u00b6 Select the Kubernetes version to be installed. Select the size of the Cluster. Select the specs that need to be configured on the Cluster and click submit to get it ready. Download the Kubeconfig file to access the cluster. curl -sLO https://storage.googleapis.com/klusternetes-kubeconfigs/vc-e2mmnhrjaieg3npobulk/kluster.yaml export KUBECONFIG = $( pwd ) /kluster.yaml kubectl get nodes","title":"Klusternetes"},{"location":"klusternetes/#klusternetes","text":"Cluster Build Tool -k0s","title":"Klusternetes"},{"location":"klusternetes/#configure-the-cluster","text":"Select the Kubernetes version to be installed. Select the size of the Cluster. Select the specs that need to be configured on the Cluster and click submit to get it ready. Download the Kubeconfig file to access the cluster. curl -sLO https://storage.googleapis.com/klusternetes-kubeconfigs/vc-e2mmnhrjaieg3npobulk/kluster.yaml export KUBECONFIG = $( pwd ) /kluster.yaml kubectl get nodes","title":"Configure the cluster :"},{"location":"kubeadm_setup/","text":"Multi Master Node Cluster \u00b6 Configuring cluster with 3 masters, 3 workers and a load balancer with below configuration. Bastion host to access the cluster from outside. Category Configuration Count Load Balancer (HA-Proxy) CPUs: 2 RAM: 2GB Disk: 10Gb NIC: 1 1 Master Nodes CPUs: 2 RAM: 2GB Disk: 10GB NIC: 1 3 Worker Nodes CPUs: 2 RAM: 2GB Disk: 10GB NIC: 1 3 Bastion CPUs: 1 RAM: 2GB Disk: 10GB NIC: 1 1 Share the SSH public key of the Bastion host to the rest of the server for transferring the certificates. Installing the HAProxy load balancer On Load Balancer Host \u00b6 To deploy three Kubernetes master nodes, one needs to deploy an HAPRoxy load balancer in front of them to distribute the traffic. apt-get update apt-get upgrade apt-get install haproxy Note Add below lines in the haproxy configuration file and restart the service. Carefully update the hostnames and IPs to avoid the conflicts. If the DNS server is configured, the Load Balancer name can be defined. vim /etc/haproxy/haproxy.cfg frontend kubernetes bind LoadBalancerIP : 6443 option tcplog mode tcp default_backend kubernetes - master - nodes backend kubernetes - master - nodes mode tcp balance roundrobin option tcp - check server < Master01 Name > < Master01 IP >: 6443 check fall 3 rise 2 server < Master02 Name > < Master02 IP >: 6443 check fall 3 rise 2 server < Master03 Name > < Master03 IP >: 6443 check fall 3 rise 2 systemctl restart haproxy Installing the client tools on the Bastion Host \u00b6 Installing cfssl: \u00b6 Download the binaries and add the execution permission to the binaries. apt-get update wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 chmod +x cfssl* mv cfssl_linux-amd64 /usr/local/bin/cfssl mv cfssljson_linux-amd64 /usr/local/bin/cfssljson cfssl version Installing kubectl: \u00b6 Download the kubectl binaries, add the execution permission to the binaries. wget https://storage.googleapis.com/kubernetes-release/release/v1.12.1/bin/linux/amd64/kubectl chmod +x kubectl sudo mv kubectl /usr/local/bin kubectl version Generating the TLS certificates \u00b6 Creating a certificate authority: \u00b6 Create the certificate authority and certificate authority signing request configuration file. vim ca-config.json { \"signing\" : { \"default\" : { \"expiry\" : \"8760h\" }, \"profiles\" : { \"kubernetes\" : { \"usages\" : [ \"signing\" , \"key encipherment\" , \"server auth\" , \"client auth\" ], \"expiry\" : \"8760h\" } } } } vim ca-csr.json { \"CN\" : \"Kubernetes\" , \"key\" : { \"algo\" : \"rsa\" , \"size\" : 2048 }, \"names\" : [ { \"C\" : \"IE\" , \"L\" : \"Cork\" , \"O\" : \"Kubernetes\" , \"OU\" : \"CA\" , \"ST\" : \"Cork Co.\" } ] } Generate the certificate authority certificate and private key. cfssl gencert -initca ca-csr.json | cfssljson -bare ca ls -la Creating the certificate for the Etcd cluster: \u00b6 Create the certificate signing request configuration file. vim kubernetes-csr.json { \"CN\" : \"kubernetes\" , \"key\" : { \"algo\" : \"rsa\" , \"size\" : 2048 }, \"names\" : [ { \"C\" : \"IE\" , \"L\" : \"Cork\" , \"O\" : \"Kubernetes\" , \"OU\" : \"Kubernetes\" , \"ST\" : \"Cork Co.\" } ] } Generate the certificate and private key: cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -hostname = <Master01_IP>,<Master02_IP>,<Master03_IP>,<LoadBalncer_IP>,127.0.0.1,kubernetes.default \\ -profile = kubernetes kubernetes-csr.json | \\ cfssljson -bare kubernetes !!! \"Copy the certificate to the both Master and Worker node:\" sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~ Preparing the Master and Worker nodes for kubeadm \u00b6 Installing Docker: \u00b6 Add the Docker repository key and Docker repository. curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - add-apt-repository \\ \"deb https://download.docker.com/linux/ $( . /etc/os-release ; echo \" $ID \" ) \\ $( lsb_release -cs ) \\ stable\" Update the list of packages and install the docker apt-get update apt-get install -y docker-ce echo '{\"exec-opts\": [\"native.cgroupdriver=systemd\"]}' | sudo tee /etc/docker/daemon.json systemctl daemon-reload systemctl restart docker systemctl enable docker Installing kubeadm, kublet, and kubectl: \u00b6 Add the Google repository key and Google repository curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - vim /etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io kubernetes-xenial main Update the list of packages. And install kubelet, kubeadm and kubectl. apt-get update apt-get install kubelet kubeadm kubectl -y Disable the swap. swapoff -a sed -i '/ swap / s/^/#/' /etc/fstab Installing and configuring Etcd on the Master Nodes. \u00b6 Create a configuration directory for Etcd and move the certificates into it. sudo mkdir /etc/etcd /var/lib/etcd sudo mv ~/ca.pem ~/kubernetes.pem ~/kubernetes-key.pem /etc/etcd Download and extract the etcd archive and move it to /usr/local/bin. wget https://github.com/coreos/etcd/releases/download/v3.3.9/etcd-v3.3.9-linux-amd64.tar.gz tar xvzf etcd-v3.3.9-linux-amd64.tar.gz sudo mv etcd-v3.3.9-linux-amd64/etcd* /usr/local/bin/ Create an etcd systemd unit file. sudo vim /etc/systemd/system/etcd.service [ Unit ] Description = etcd Documentation = https : //github.com/coreos [ Service ] ExecStart =/ usr / local / bin / etcd \\ -- name < host_IP > \\ -- cert - file =/ etc / etcd / kubernetes . pem \\ -- key - file =/ etc / etcd / kubernetes - key . pem \\ -- peer - cert - file =/ etc / etcd / kubernetes . pem \\ -- peer - key - file =/ etc / etcd / kubernetes - key . pem \\ -- trusted - ca - file =/ etc / etcd / ca . pem \\ -- peer - trusted - ca - file =/ etc / etcd / ca . pem \\ -- peer - client - cert - auth \\ -- client - cert - auth \\ -- initial - advertise - peer - urls https : //<host_IP>:2380 \\ --listen-peer-urls https://<host_IP>:2380 \\ --listen-client-urls https://<host_IP>:2379,http://127.0.0.1:2379 \\ --advertise-client-urls https://<host_IP>:2379 \\ --initial-cluster-token etcd-cluster-0 \\ --initial-cluster <Master01_IP>=https://<Master01_IP>:2380,<Master02_IP>=https://<Master02_IP>:2380,<Master03_IP>=https://<Master03_IP>:2380 \\ --initial-cluster-state new \\ --data-dir=/var/lib/etcd Restart = on - failure RestartSec = 5 [ Install ] WantedBy = multi - user . target Reload the daemon configuration, Enable and start the etcd. sudo systemctl daemon-reload sudo systemctl enable etcd sudo systemctl start etcd Verify that the cluster is up and running. ETCDCTL_API = 3 etcdctl member list Initializing the master nodes \u00b6 Initializing the master01 node: \u00b6 Create the configuration file for kubeadm. vim config.yaml sudo kubeadm init --config = config.yaml sudo scp -r /etc/kubernetes/pki opsadmin@<Master02_IP>:~ sudo scp -r /etc/kubernetes/pki opsadmin@<Master03_IP>:~ sudo scp -r config.yaml opsadmin@<Master03_IP>:~ Configure Master02 and 03 \u00b6 Remove the apiserver.crt and apiserver.key. rm ~/pki/apiserver.* Move the certificates to the /etc/kubernetes directory. sudo mv ~/pki /etc/kubernetes/ sudo kubeadm init --config = config.yaml Initializing the worker nodes \u00b6 Copy the \"kubeadm join\" command from the lastly created master node and execute on the worker nodes. sudo kubeadm join 10 .10.40.93:6443 --token [ your_token ] --discovery-token-ca-cert-hash sha256: [ your_token_ca_cert_hash ] Configuring kubectl on the client machine sudo chmod +r /etc/kubernetes/admin.conf scp opsadmin@<Master01_IP>:/etc/kubernetes/admin.conf . mkdir ~/.kube mv admin.conf ~/.kube/config chmod 600 ~/.kube/config Deploying the Calico as overlay network kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml kubectl create -f https://docs.projectcalico.org/manifests/custom-resources.yamlwatch kubectl get pods -n calico-system Validate nodes are in Ready state. kubectl get nodes References: https://blog.inkubate.io/install-and-configure-a-multi-master-kubernetes-cluster-with-kubeadm/ Analysis: Config file error This version of kubeadm only supports deploying clusters with the control plane version >= 1.16.0. Current version: v1.12.0 Kubernetes https://stackoverflow.com/questions/61926846/this-version-of-kubeadm-only-supports-deploying-clusters-with-the-control-plane https://pkg.go.dev/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2 Error [kubelet-check] It seems like the kubelet isn't running or healthy. [kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz ' failed with error: Get http://localhost:10248/healthz : dial tcp 127.0.0.1:10248: connect: connection refused. Solution: Updating the /etc/docker/daemon.json file https://www.myfreax.com/how-to-solve-dial-tcp-127-0-0-1-10248-connect-connection-refused-during-kubeadm-init-initialization/ Disable swap on all the master and worker nodes. Carefully give the hostnames and IPs while configuring configuration files, generating ETCD certificates, hosts and hostname file, HA proxy config file. DNS server entry in the resolv.conf and /etc/resolvconf/resolv.conf.d/head. kubernetes node not ready but pod running. Due to wrong hostname given in HA config file and on the node https://stackoverflow.com/questions/47107117/how-to-debug-when-kubernetes-nodes-are-in-not-ready-state https://stackoverflow.com/questions/49112336/container-runtime-network-not-ready-cni-config-uninitialized https://github.com/kubernetes/kubernetes/issues/96459","title":"Kubeadm Cluster Setup"},{"location":"kubeadm_setup/#multi-master-node-cluster","text":"Configuring cluster with 3 masters, 3 workers and a load balancer with below configuration. Bastion host to access the cluster from outside. Category Configuration Count Load Balancer (HA-Proxy) CPUs: 2 RAM: 2GB Disk: 10Gb NIC: 1 1 Master Nodes CPUs: 2 RAM: 2GB Disk: 10GB NIC: 1 3 Worker Nodes CPUs: 2 RAM: 2GB Disk: 10GB NIC: 1 3 Bastion CPUs: 1 RAM: 2GB Disk: 10GB NIC: 1 1 Share the SSH public key of the Bastion host to the rest of the server for transferring the certificates.","title":"Multi Master Node Cluster"},{"location":"kubeadm_setup/#installing-the-haproxy-load-balancer-on-load-balancer-host","text":"To deploy three Kubernetes master nodes, one needs to deploy an HAPRoxy load balancer in front of them to distribute the traffic. apt-get update apt-get upgrade apt-get install haproxy Note Add below lines in the haproxy configuration file and restart the service. Carefully update the hostnames and IPs to avoid the conflicts. If the DNS server is configured, the Load Balancer name can be defined. vim /etc/haproxy/haproxy.cfg frontend kubernetes bind LoadBalancerIP : 6443 option tcplog mode tcp default_backend kubernetes - master - nodes backend kubernetes - master - nodes mode tcp balance roundrobin option tcp - check server < Master01 Name > < Master01 IP >: 6443 check fall 3 rise 2 server < Master02 Name > < Master02 IP >: 6443 check fall 3 rise 2 server < Master03 Name > < Master03 IP >: 6443 check fall 3 rise 2 systemctl restart haproxy","title":"Installing the HAProxy load balancer On Load Balancer Host"},{"location":"kubeadm_setup/#installing-the-client-tools-on-the-bastion-host","text":"","title":"Installing the client tools on the Bastion Host"},{"location":"kubeadm_setup/#installing-cfssl","text":"Download the binaries and add the execution permission to the binaries. apt-get update wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 chmod +x cfssl* mv cfssl_linux-amd64 /usr/local/bin/cfssl mv cfssljson_linux-amd64 /usr/local/bin/cfssljson cfssl version","title":"Installing cfssl:"},{"location":"kubeadm_setup/#installing-kubectl","text":"Download the kubectl binaries, add the execution permission to the binaries. wget https://storage.googleapis.com/kubernetes-release/release/v1.12.1/bin/linux/amd64/kubectl chmod +x kubectl sudo mv kubectl /usr/local/bin kubectl version","title":"Installing kubectl:"},{"location":"kubeadm_setup/#generating-the-tls-certificates","text":"","title":"Generating the TLS certificates"},{"location":"kubeadm_setup/#creating-a-certificate-authority","text":"Create the certificate authority and certificate authority signing request configuration file. vim ca-config.json { \"signing\" : { \"default\" : { \"expiry\" : \"8760h\" }, \"profiles\" : { \"kubernetes\" : { \"usages\" : [ \"signing\" , \"key encipherment\" , \"server auth\" , \"client auth\" ], \"expiry\" : \"8760h\" } } } } vim ca-csr.json { \"CN\" : \"Kubernetes\" , \"key\" : { \"algo\" : \"rsa\" , \"size\" : 2048 }, \"names\" : [ { \"C\" : \"IE\" , \"L\" : \"Cork\" , \"O\" : \"Kubernetes\" , \"OU\" : \"CA\" , \"ST\" : \"Cork Co.\" } ] } Generate the certificate authority certificate and private key. cfssl gencert -initca ca-csr.json | cfssljson -bare ca ls -la","title":"Creating a certificate authority:"},{"location":"kubeadm_setup/#creating-the-certificate-for-the-etcd-cluster","text":"Create the certificate signing request configuration file. vim kubernetes-csr.json { \"CN\" : \"kubernetes\" , \"key\" : { \"algo\" : \"rsa\" , \"size\" : 2048 }, \"names\" : [ { \"C\" : \"IE\" , \"L\" : \"Cork\" , \"O\" : \"Kubernetes\" , \"OU\" : \"Kubernetes\" , \"ST\" : \"Cork Co.\" } ] } Generate the certificate and private key: cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -hostname = <Master01_IP>,<Master02_IP>,<Master03_IP>,<LoadBalncer_IP>,127.0.0.1,kubernetes.default \\ -profile = kubernetes kubernetes-csr.json | \\ cfssljson -bare kubernetes !!! \"Copy the certificate to the both Master and Worker node:\" sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~","title":"Creating the certificate for the Etcd cluster:"},{"location":"kubeadm_setup/#preparing-the-master-and-worker-nodes-for-kubeadm","text":"","title":"Preparing the Master and Worker nodes for kubeadm"},{"location":"kubeadm_setup/#installing-docker","text":"Add the Docker repository key and Docker repository. curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - add-apt-repository \\ \"deb https://download.docker.com/linux/ $( . /etc/os-release ; echo \" $ID \" ) \\ $( lsb_release -cs ) \\ stable\" Update the list of packages and install the docker apt-get update apt-get install -y docker-ce echo '{\"exec-opts\": [\"native.cgroupdriver=systemd\"]}' | sudo tee /etc/docker/daemon.json systemctl daemon-reload systemctl restart docker systemctl enable docker","title":"Installing Docker:"},{"location":"kubeadm_setup/#installing-kubeadm-kublet-and-kubectl","text":"Add the Google repository key and Google repository curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - vim /etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io kubernetes-xenial main Update the list of packages. And install kubelet, kubeadm and kubectl. apt-get update apt-get install kubelet kubeadm kubectl -y Disable the swap. swapoff -a sed -i '/ swap / s/^/#/' /etc/fstab","title":"Installing kubeadm, kublet, and kubectl:"},{"location":"kubeadm_setup/#installing-and-configuring-etcd-on-the-master-nodes","text":"Create a configuration directory for Etcd and move the certificates into it. sudo mkdir /etc/etcd /var/lib/etcd sudo mv ~/ca.pem ~/kubernetes.pem ~/kubernetes-key.pem /etc/etcd Download and extract the etcd archive and move it to /usr/local/bin. wget https://github.com/coreos/etcd/releases/download/v3.3.9/etcd-v3.3.9-linux-amd64.tar.gz tar xvzf etcd-v3.3.9-linux-amd64.tar.gz sudo mv etcd-v3.3.9-linux-amd64/etcd* /usr/local/bin/ Create an etcd systemd unit file. sudo vim /etc/systemd/system/etcd.service [ Unit ] Description = etcd Documentation = https : //github.com/coreos [ Service ] ExecStart =/ usr / local / bin / etcd \\ -- name < host_IP > \\ -- cert - file =/ etc / etcd / kubernetes . pem \\ -- key - file =/ etc / etcd / kubernetes - key . pem \\ -- peer - cert - file =/ etc / etcd / kubernetes . pem \\ -- peer - key - file =/ etc / etcd / kubernetes - key . pem \\ -- trusted - ca - file =/ etc / etcd / ca . pem \\ -- peer - trusted - ca - file =/ etc / etcd / ca . pem \\ -- peer - client - cert - auth \\ -- client - cert - auth \\ -- initial - advertise - peer - urls https : //<host_IP>:2380 \\ --listen-peer-urls https://<host_IP>:2380 \\ --listen-client-urls https://<host_IP>:2379,http://127.0.0.1:2379 \\ --advertise-client-urls https://<host_IP>:2379 \\ --initial-cluster-token etcd-cluster-0 \\ --initial-cluster <Master01_IP>=https://<Master01_IP>:2380,<Master02_IP>=https://<Master02_IP>:2380,<Master03_IP>=https://<Master03_IP>:2380 \\ --initial-cluster-state new \\ --data-dir=/var/lib/etcd Restart = on - failure RestartSec = 5 [ Install ] WantedBy = multi - user . target Reload the daemon configuration, Enable and start the etcd. sudo systemctl daemon-reload sudo systemctl enable etcd sudo systemctl start etcd Verify that the cluster is up and running. ETCDCTL_API = 3 etcdctl member list","title":"Installing and configuring Etcd on the Master Nodes."},{"location":"kubeadm_setup/#initializing-the-master-nodes","text":"","title":"Initializing the master nodes"},{"location":"kubeadm_setup/#initializing-the-master01-node","text":"Create the configuration file for kubeadm. vim config.yaml sudo kubeadm init --config = config.yaml sudo scp -r /etc/kubernetes/pki opsadmin@<Master02_IP>:~ sudo scp -r /etc/kubernetes/pki opsadmin@<Master03_IP>:~ sudo scp -r config.yaml opsadmin@<Master03_IP>:~","title":"Initializing the master01 node:"},{"location":"kubeadm_setup/#configure-master02-and-03","text":"Remove the apiserver.crt and apiserver.key. rm ~/pki/apiserver.* Move the certificates to the /etc/kubernetes directory. sudo mv ~/pki /etc/kubernetes/ sudo kubeadm init --config = config.yaml","title":"Configure Master02 and 03"},{"location":"kubeadm_setup/#initializing-the-worker-nodes","text":"Copy the \"kubeadm join\" command from the lastly created master node and execute on the worker nodes. sudo kubeadm join 10 .10.40.93:6443 --token [ your_token ] --discovery-token-ca-cert-hash sha256: [ your_token_ca_cert_hash ] Configuring kubectl on the client machine sudo chmod +r /etc/kubernetes/admin.conf scp opsadmin@<Master01_IP>:/etc/kubernetes/admin.conf . mkdir ~/.kube mv admin.conf ~/.kube/config chmod 600 ~/.kube/config Deploying the Calico as overlay network kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml kubectl create -f https://docs.projectcalico.org/manifests/custom-resources.yamlwatch kubectl get pods -n calico-system Validate nodes are in Ready state. kubectl get nodes References: https://blog.inkubate.io/install-and-configure-a-multi-master-kubernetes-cluster-with-kubeadm/ Analysis: Config file error This version of kubeadm only supports deploying clusters with the control plane version >= 1.16.0. Current version: v1.12.0 Kubernetes https://stackoverflow.com/questions/61926846/this-version-of-kubeadm-only-supports-deploying-clusters-with-the-control-plane https://pkg.go.dev/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2 Error [kubelet-check] It seems like the kubelet isn't running or healthy. [kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz ' failed with error: Get http://localhost:10248/healthz : dial tcp 127.0.0.1:10248: connect: connection refused. Solution: Updating the /etc/docker/daemon.json file https://www.myfreax.com/how-to-solve-dial-tcp-127-0-0-1-10248-connect-connection-refused-during-kubeadm-init-initialization/ Disable swap on all the master and worker nodes. Carefully give the hostnames and IPs while configuring configuration files, generating ETCD certificates, hosts and hostname file, HA proxy config file. DNS server entry in the resolv.conf and /etc/resolvconf/resolv.conf.d/head. kubernetes node not ready but pod running. Due to wrong hostname given in HA config file and on the node https://stackoverflow.com/questions/47107117/how-to-debug-when-kubernetes-nodes-are-in-not-ready-state https://stackoverflow.com/questions/49112336/container-runtime-network-not-ready-cni-config-uninitialized https://github.com/kubernetes/kubernetes/issues/96459","title":"Initializing the worker nodes"},{"location":"kubespray/","text":"Kubernetes Setup with kubespray \u00b6 Cluster Build Tool -Kubespray For this activity, deploy 5 vm\u2019s instances Node Name VM Details HA Proxy 1 gb ram, 1 cpu KScontroller(Ansible) 2 gb ram, 2 cpu Control Plane (2 Nodes) 3 gb ram, 2 cpu, 50 GB HD Worker (2-Nodes) 3 gb ram, 2 cpu, 50 GB HD Configure HA proxy to build a Multi-master Kubernetes cluster: \u00b6 Download and install the haproxy apt-get update && apt-get install haproxy -y Update haproxy configuration with as below with the details of the Ansible and Master nodes IP vi /etc/haproxy/haproxy.cfg listen kubernetes - apiserver - https bind ansiblenodeIP : 8383 mode tcp option log - health - checks timeout client 3 h timeout server 3 h server master1 < IP1 >: 6443 check check - ssl verify none inter 10000 server master2 < IP2 >: 6443 check check - ssl verify none inter 10000 balance roundrobin systemctl restart haproxy netstat -atnlp Deploying Multi-master Kubernetes cluster with Kubespray: \u00b6 Install Ansible and PIP apt update && apt install software-properties-common -y add-apt-repository --yes --update ppa:ansible/ansible apt install ansible -y ansible --version apt install python3-pip -y Clone the Kubespray source code and install requirements through Pip. git clone https://github.com/kubernetes-sigs/kubespray.git cd kubespray/ pip3 install -r requirements.txt cp -rfp inventory/sample inventory/mycluster declare -a IPS =( <master01_IP> <master02_IP> <Worker01_IP> <Worker02_IP> ) CONFIG_FILE = inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${ IPS [@] } Update the HA-Proxy server details \u00b6 vi inventory/mycluster/group_vars/all/all.yml ## External LB example config apiserver_loadbalancer_domain_name : \"cruise.org\" loadbalancer_apiserver : address : 172.31.27.136 port : 8383 ## Internal loadbalancers for apiservers loadbalancer_apiserver_localhost : false Enable Kubernetes dashboard by updating the below files vi inventory/sample/group_vars/k8s_cluster/addons.yml vi inventory/mycluster/group_vars/k8s_cluster/addons.yml # Kubernetes dashboard # RBAC required. see docs/getting-started.md for access details. dashboard_enabled : true Deploy cluster with below Ansible command. ansible all -m ping -i inventory/mycluster/hosts.yaml --user ubuntu --private-key ../kmvelero.pem ansible-playbook -i inventory/mycluster/hosts.yaml --user ubuntu --private-key kmvelero.pem --become cluster.yml On Master Node: kubectl get nodes Output NAME STATUS ROLES AGE VERSION node1 Ready control - plane , master 5 h14m v1 .22.3 node2 Ready control - plane , master 5 h14m v1 .22.3 node3 Ready < none > 5 h13m v1 .22.3 node4 Ready < none > 5 h13m v1 .22.3","title":"Kube Spray"},{"location":"kubespray/#kubernetes-setup-with-kubespray","text":"Cluster Build Tool -Kubespray For this activity, deploy 5 vm\u2019s instances Node Name VM Details HA Proxy 1 gb ram, 1 cpu KScontroller(Ansible) 2 gb ram, 2 cpu Control Plane (2 Nodes) 3 gb ram, 2 cpu, 50 GB HD Worker (2-Nodes) 3 gb ram, 2 cpu, 50 GB HD","title":"Kubernetes Setup with kubespray"},{"location":"kubespray/#configure-ha-proxy-to-build-a-multi-master-kubernetes-cluster","text":"Download and install the haproxy apt-get update && apt-get install haproxy -y Update haproxy configuration with as below with the details of the Ansible and Master nodes IP vi /etc/haproxy/haproxy.cfg listen kubernetes - apiserver - https bind ansiblenodeIP : 8383 mode tcp option log - health - checks timeout client 3 h timeout server 3 h server master1 < IP1 >: 6443 check check - ssl verify none inter 10000 server master2 < IP2 >: 6443 check check - ssl verify none inter 10000 balance roundrobin systemctl restart haproxy netstat -atnlp","title":"Configure HA proxy to build a Multi-master Kubernetes cluster:"},{"location":"kubespray/#deploying-multi-master-kubernetes-cluster-with-kubespray","text":"Install Ansible and PIP apt update && apt install software-properties-common -y add-apt-repository --yes --update ppa:ansible/ansible apt install ansible -y ansible --version apt install python3-pip -y Clone the Kubespray source code and install requirements through Pip. git clone https://github.com/kubernetes-sigs/kubespray.git cd kubespray/ pip3 install -r requirements.txt cp -rfp inventory/sample inventory/mycluster declare -a IPS =( <master01_IP> <master02_IP> <Worker01_IP> <Worker02_IP> ) CONFIG_FILE = inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${ IPS [@] }","title":"Deploying Multi-master Kubernetes cluster with Kubespray:"},{"location":"kubespray/#update-the-ha-proxy-server-details","text":"vi inventory/mycluster/group_vars/all/all.yml ## External LB example config apiserver_loadbalancer_domain_name : \"cruise.org\" loadbalancer_apiserver : address : 172.31.27.136 port : 8383 ## Internal loadbalancers for apiservers loadbalancer_apiserver_localhost : false Enable Kubernetes dashboard by updating the below files vi inventory/sample/group_vars/k8s_cluster/addons.yml vi inventory/mycluster/group_vars/k8s_cluster/addons.yml # Kubernetes dashboard # RBAC required. see docs/getting-started.md for access details. dashboard_enabled : true Deploy cluster with below Ansible command. ansible all -m ping -i inventory/mycluster/hosts.yaml --user ubuntu --private-key ../kmvelero.pem ansible-playbook -i inventory/mycluster/hosts.yaml --user ubuntu --private-key kmvelero.pem --become cluster.yml On Master Node: kubectl get nodes Output NAME STATUS ROLES AGE VERSION node1 Ready control - plane , master 5 h14m v1 .22.3 node2 Ready control - plane , master 5 h14m v1 .22.3 node3 Ready < none > 5 h13m v1 .22.3 node4 Ready < none > 5 h13m v1 .22.3","title":"Update the HA-Proxy server details"},{"location":"ldap/","text":"Install and Configure OpenLDAP Server on Ubuntu \u00b6 Set hostname for the Ubuntu server \u00b6 sudo hostnamectl set-hostname ldap.k8sengineers.com Add the IP and FQDN to file /etc/hosts vim /etc/hosts 192 .168.0.151 ldap.k8sengineers.com Note Replace ldap.k8sengineers.com with your correct hostname/valid domain name. Install OpenLDAP Server on Ubuntu \u00b6 sudo apt update sudo apt -y install slapd ldap-utils During the installation, you\u2019ll be prompted to set LDAP admin password, provide your desired password, then press Confirm the password and continue installation by selecting You can confirm that your installation was successful using the commandslapcat to output SLAPD database contents. sudo slapcat dn: dc = k8sengineers,dc = com objectClass: top objectClass: dcObject objectClass: organization o: k8sengineers.com dc: k8sengineers structuralObjectClass: organization entryUUID: 0139eef2-f01c-103b-8cf7-cb31a244bcdd creatorsName: cn = admin,dc = k8sengineers,dc = com createTimestamp: 20211213045057Z entryCSN: 20211213045057 .125264Z#000000#000#000000 modifiersName: cn = admin,dc = k8sengineers,dc = com modifyTimestamp: 20211213045057Z dn: cn = admin,dc = k8sengineers,dc = com objectClass: simpleSecurityObject objectClass: organizationalRole cn: admin description: LDAP administrator userPassword:: e1NTSEF9aXpJOXpJT2tTUUNIUzkrVzJpUVQ5L1M4WVRzZjMvUU4 = structuralObjectClass: organizationalRole entryUUID: 013a5ebe-f01c-103b-8cf8-cb31a244bcdd creatorsName: cn = admin,dc = k8sengineers,dc = com createTimestamp: 20211213045057Z entryCSN: 20211213045057 .128175Z#000000#000#000000 modifiersName: cn = admin,dc = k8sengineers,dc = com modifyTimestamp: 20211213045057Z Add base dn for Users and Groups \u00b6 The next step is adding a base DN for users and groups. Create a file named basedn.ldif with below contents: vim basedn.ldif dn: ou = people,dc = k8sengineers,dc = com objectClass: organizationalUnit ou: people dn: ou = groups,dc = k8sengineers,dc = com objectClass: organizationalUnit ou: groups Replace k8sengineers and com with your correct domain components. Now add the file by running the command: ldapadd -x -D cn = admin,dc = k8sengineers,dc = com -W -f basedn.ldif Enter LDAP Password: Output adding new entry \"ou=people,dc=k8sengineers,dc=com\" adding new entry \"ou=groups,dc=k8sengineers,dc=com\" Add User Accounts and Groups \u00b6 Generate a password for the user account to add. sudo slappasswd Set the Password Output {SSHA}ls2kqhrLiMuKg9w3JcMq4y1Cefi0amAx Create ldif file for adding users. vim ldapusers.ldif dn: uid = admin,ou = people,dc = k8sengineers,dc = com objectClass: inetOrgPerson objectClass: posixAccount objectClass: shadowAccount cn: admin sn: Wiz userPassword: { SSHA } ls2kqhrLiMuKg9w3JcMq4y1Cefi0amAx loginShell: /bin/bash uidNumber: 2000 gidNumber: 2000 homeDirectory: /home/admin Replace admin with the username to add dc=k8sengineers,dc=com with your correct domain values. cn & sn with your Username Values {SSHA}ls2kqhrLiMuKg9w3JcMq4y1Cefi0amAx with your hashed password When done with edit, add account by running. ldapadd -x -D cn = admin,dc = k8sengineers,dc = com -W -f ldapusers.ldif Output adding new entry \"uid=admin,ou=people,dc=k8sengineers,dc=com\" Do the same of group. Create ldif file: vim ldapgroups.ldif dn: cn = admin,ou = groups,dc = k8sengineers,dc = com objectClass: posixGroup cn: admin gidNumber: 2000 memberUid: admin Add group: ldapadd -x -D cn = admin,dc = k8sengineers,dc = com -W -f ldapgroups.ldif Output adding new entry \"cn=admin,ou=groups,dc=k8sengineers,dc=com\" You can combine the two into single file. Install LDAP Account Manager on Ubuntu \u00b6 Install Apache Web server & PHP sudo apt -y install apache2 php php-cgi libapache2-mod-php php-mbstring php-common php-pear For Ubuntu 22.04:sudo a2enconf php8.0-cgi For Ubuntu 20.04:sudo a2enconf php7.4-cgi For Ubuntu 18.04: sudo a2enconf php7.2-cgi Here Im using Ubuntu 20.04: sudo a2enconf php7.4-cgi sudo systemctl reload apache2 Install LDAP Account Manager sudo apt -y install ldap-account-manager Configure LDAP Account Manager http://< IP address >/lam The LDAP Account Manager Login form will be shown. We need to set our LDAP server profile by clicking on[LAM configuration] at the upper right corner. Then click on,Edit server profiles This will ask you for LAM Profile name Password Note Default password is lam The first thing to change is Profile Password, this is at the end of General Settings page. Next is to set LDAP Server address and Tree suffix. Mine looks like below, you need to use your Domain components as set in server hostname. Set Dashboard login by specifying the admin user account and domain components under \u201cSecurity settings\u201d section Switch to \u201cAccount types\u201d page and set Active account types LDAP suffix and List attributes. Add user accounts and groups with LDAP Account Manager \u00b6 Login with the accountadmin to LAM dashboard to start managing user accounts and groups. Add User Group Give the group a name, optional group ID and description. Add User Accounts Once you have the groups for user accounts to be added, click on Users > New user to add a new user account to your LDAP server. You have three sections for user management: Personal \u2013 This contains user\u2019s personal information like the first name, last name, email, phone, department, address e.t.c Configure your Ubuntu 22.04|20.04|18.04 as LDAP Client \u00b6 The last step is to configure the systems in your network to authenticate against the LDAP server we\u2019ve just configured: \u00b6 Add LDAP server address to /etc/hosts file if you don\u2019t have an active DNS server in your network. sudo vim /etc/hosts 192 .168.18.50 ldap.k8sengineers.com Install LDAP client utilities on your Ubuntu system: sudo apt -y install libnss-ldap libpam-ldap ldap-utils Begin configuring the settings to look like below * Set LDAP URI- This can be IP address or hostname Set a Distinguished name of the search base Select LDAP version 3 Select Yes for Make local root Database admin Answer No for Does the LDAP database require login? Set LDAP account for root, something like cn=admin,cd=k8sengineers,cn=com Provide LDAP root account Password After the installation, edit /etc/nsswitch.conf and add ldap authentication to passwd and group lines. vim /etc/nsswitch.conf passwd: compat systemd ldap group: compat systemd ldap shadow: compat Modify the file /etc/pam.d/common-password Remove use_authtok on line 26 to look like below. vim /etc/pam.d/common-password password [ success = 1 user_unknown = ignore default = die ] pam_ldap.so try_first_pass Enable creation of home directory on first login by adding the following line to the end of file /etc/pam.d/common-session session optional pam_mkhomedir.so skel = /etc/skel umask = 077 See below screenshot: Test by switching to a user account on LDAP sudo su - <username>","title":"LDAP"},{"location":"ldap/#install-and-configure-openldap-server-on-ubuntu","text":"","title":"Install and Configure OpenLDAP Server on Ubuntu"},{"location":"ldap/#set-hostname-for-the-ubuntu-server","text":"sudo hostnamectl set-hostname ldap.k8sengineers.com Add the IP and FQDN to file /etc/hosts vim /etc/hosts 192 .168.0.151 ldap.k8sengineers.com Note Replace ldap.k8sengineers.com with your correct hostname/valid domain name.","title":"Set hostname for the Ubuntu server"},{"location":"ldap/#install-openldap-server-on-ubuntu","text":"sudo apt update sudo apt -y install slapd ldap-utils During the installation, you\u2019ll be prompted to set LDAP admin password, provide your desired password, then press Confirm the password and continue installation by selecting You can confirm that your installation was successful using the commandslapcat to output SLAPD database contents. sudo slapcat dn: dc = k8sengineers,dc = com objectClass: top objectClass: dcObject objectClass: organization o: k8sengineers.com dc: k8sengineers structuralObjectClass: organization entryUUID: 0139eef2-f01c-103b-8cf7-cb31a244bcdd creatorsName: cn = admin,dc = k8sengineers,dc = com createTimestamp: 20211213045057Z entryCSN: 20211213045057 .125264Z#000000#000#000000 modifiersName: cn = admin,dc = k8sengineers,dc = com modifyTimestamp: 20211213045057Z dn: cn = admin,dc = k8sengineers,dc = com objectClass: simpleSecurityObject objectClass: organizationalRole cn: admin description: LDAP administrator userPassword:: e1NTSEF9aXpJOXpJT2tTUUNIUzkrVzJpUVQ5L1M4WVRzZjMvUU4 = structuralObjectClass: organizationalRole entryUUID: 013a5ebe-f01c-103b-8cf8-cb31a244bcdd creatorsName: cn = admin,dc = k8sengineers,dc = com createTimestamp: 20211213045057Z entryCSN: 20211213045057 .128175Z#000000#000#000000 modifiersName: cn = admin,dc = k8sengineers,dc = com modifyTimestamp: 20211213045057Z","title":"Install OpenLDAP Server on Ubuntu"},{"location":"ldap/#add-base-dn-for-users-and-groups","text":"The next step is adding a base DN for users and groups. Create a file named basedn.ldif with below contents: vim basedn.ldif dn: ou = people,dc = k8sengineers,dc = com objectClass: organizationalUnit ou: people dn: ou = groups,dc = k8sengineers,dc = com objectClass: organizationalUnit ou: groups Replace k8sengineers and com with your correct domain components. Now add the file by running the command: ldapadd -x -D cn = admin,dc = k8sengineers,dc = com -W -f basedn.ldif Enter LDAP Password: Output adding new entry \"ou=people,dc=k8sengineers,dc=com\" adding new entry \"ou=groups,dc=k8sengineers,dc=com\"","title":"Add base dn for Users and Groups"},{"location":"ldap/#add-user-accounts-and-groups","text":"Generate a password for the user account to add. sudo slappasswd Set the Password Output {SSHA}ls2kqhrLiMuKg9w3JcMq4y1Cefi0amAx Create ldif file for adding users. vim ldapusers.ldif dn: uid = admin,ou = people,dc = k8sengineers,dc = com objectClass: inetOrgPerson objectClass: posixAccount objectClass: shadowAccount cn: admin sn: Wiz userPassword: { SSHA } ls2kqhrLiMuKg9w3JcMq4y1Cefi0amAx loginShell: /bin/bash uidNumber: 2000 gidNumber: 2000 homeDirectory: /home/admin Replace admin with the username to add dc=k8sengineers,dc=com with your correct domain values. cn & sn with your Username Values {SSHA}ls2kqhrLiMuKg9w3JcMq4y1Cefi0amAx with your hashed password When done with edit, add account by running. ldapadd -x -D cn = admin,dc = k8sengineers,dc = com -W -f ldapusers.ldif Output adding new entry \"uid=admin,ou=people,dc=k8sengineers,dc=com\" Do the same of group. Create ldif file: vim ldapgroups.ldif dn: cn = admin,ou = groups,dc = k8sengineers,dc = com objectClass: posixGroup cn: admin gidNumber: 2000 memberUid: admin Add group: ldapadd -x -D cn = admin,dc = k8sengineers,dc = com -W -f ldapgroups.ldif Output adding new entry \"cn=admin,ou=groups,dc=k8sengineers,dc=com\" You can combine the two into single file.","title":"Add User Accounts and Groups"},{"location":"ldap/#install-ldap-account-manager-on-ubuntu","text":"Install Apache Web server & PHP sudo apt -y install apache2 php php-cgi libapache2-mod-php php-mbstring php-common php-pear For Ubuntu 22.04:sudo a2enconf php8.0-cgi For Ubuntu 20.04:sudo a2enconf php7.4-cgi For Ubuntu 18.04: sudo a2enconf php7.2-cgi Here Im using Ubuntu 20.04: sudo a2enconf php7.4-cgi sudo systemctl reload apache2 Install LDAP Account Manager sudo apt -y install ldap-account-manager Configure LDAP Account Manager http://< IP address >/lam The LDAP Account Manager Login form will be shown. We need to set our LDAP server profile by clicking on[LAM configuration] at the upper right corner. Then click on,Edit server profiles This will ask you for LAM Profile name Password Note Default password is lam The first thing to change is Profile Password, this is at the end of General Settings page. Next is to set LDAP Server address and Tree suffix. Mine looks like below, you need to use your Domain components as set in server hostname. Set Dashboard login by specifying the admin user account and domain components under \u201cSecurity settings\u201d section Switch to \u201cAccount types\u201d page and set Active account types LDAP suffix and List attributes.","title":"Install LDAP Account Manager on Ubuntu"},{"location":"ldap/#add-user-accounts-and-groups-with-ldap-account-manager","text":"Login with the accountadmin to LAM dashboard to start managing user accounts and groups. Add User Group Give the group a name, optional group ID and description. Add User Accounts Once you have the groups for user accounts to be added, click on Users > New user to add a new user account to your LDAP server. You have three sections for user management: Personal \u2013 This contains user\u2019s personal information like the first name, last name, email, phone, department, address e.t.c","title":"Add user accounts and groups with LDAP Account Manager"},{"location":"ldap/#configure-your-ubuntu-220420041804-as-ldap-client","text":"","title":"Configure your Ubuntu 22.04|20.04|18.04 as LDAP Client"},{"location":"ldap/#the-last-step-is-to-configure-the-systems-in-your-network-to-authenticate-against-the-ldap-server-weve-just-configured","text":"Add LDAP server address to /etc/hosts file if you don\u2019t have an active DNS server in your network. sudo vim /etc/hosts 192 .168.18.50 ldap.k8sengineers.com Install LDAP client utilities on your Ubuntu system: sudo apt -y install libnss-ldap libpam-ldap ldap-utils Begin configuring the settings to look like below * Set LDAP URI- This can be IP address or hostname Set a Distinguished name of the search base Select LDAP version 3 Select Yes for Make local root Database admin Answer No for Does the LDAP database require login? Set LDAP account for root, something like cn=admin,cd=k8sengineers,cn=com Provide LDAP root account Password After the installation, edit /etc/nsswitch.conf and add ldap authentication to passwd and group lines. vim /etc/nsswitch.conf passwd: compat systemd ldap group: compat systemd ldap shadow: compat Modify the file /etc/pam.d/common-password Remove use_authtok on line 26 to look like below. vim /etc/pam.d/common-password password [ success = 1 user_unknown = ignore default = die ] pam_ldap.so try_first_pass Enable creation of home directory on first login by adding the following line to the end of file /etc/pam.d/common-session session optional pam_mkhomedir.so skel = /etc/skel umask = 077 See below screenshot: Test by switching to a user account on LDAP sudo su - <username>","title":"The last step is to configure the systems in your network to authenticate against the LDAP server we\u2019ve just configured:"},{"location":"metalLB/","text":"MetalLB with Ingress for service of type LoadBalancer in OnPremise setup. \u00b6 Requirements MetalLB requires the following to function: A Kubernetes cluster, running Kubernetes 1.13.0 or later, that does not already have network load-balancing functionality. A cluster network configuration that can coexist with MetalLB. Some IPv4 addresses for MetalLB to hand out. When using the BGP operating mode, you will need one or more routers capable of speaking BGP. Traffic on port 7946 (TCP & UDP) must be allowed between nodes, as required by memberlist. Preparation \u00b6 If you\u2019re using kube-proxy in IPVS mode, since Kubernetes v1.14.2 you have to enable strict ARP mode. Note you don\u2019t need this if you\u2019re using kube-router as service-proxy because it is enabling strict ARP by default. You can achieve this by editing kube-proxy config in current cluster: kubectl edit configmap -n kube-system kube-proxy and set: apiVersion : kubeproxy.config.k8s.io/v1alpha1 kind : KubeProxy Configuration mode : \"ipvs\" ipvs : strictARP : true Installation By Manifest \u00b6 To install MetalLB, apply the manifest: kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.11.0/manifests/namespace.yaml kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.11.0/manifests/metallb.yaml Layer 2 Configuration \u00b6 Layer 2 mode is the simplest to configure: in many cases, you don\u2019t need any protocol-specific configuration, only IP addresses. Layer 2 mode does not require the IPs to be bound to the network interfaces of your worker nodes. It works by responding to ARP requests on your local network directly, to give the machine\u2019s MAC address to clients. For example, the following configuration gives MetalLB control over IPs from 192.168.1.240 to 192.168.1.250 , and configures Layer 2 mode: apiVersion : v1 kind : ConfigMap metadata : namespace : metallb-system name : config data : config : | address-pools : - name : default protocol : layer2 addresses : - 192.168.1.240-192.168.1.250 Install Ingress for metalLB \u00b6 helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm repo list helm search repo ingress helm show values ingress-nginx/ingress-nginx > ingress-nginxValues.yaml Update values file with changes mentioned below vim ingress-nginxValues.yaml hostNetwork : true ## Use host ports 80 and 443 ## Disabled by default ## hostPort : enabled : true ## DaemonSet or Deployment ## kind : DaemonSet helm install cruise-ingress ingress-nginx/ingress-nginx -n ingress-nginx --values ingress-nginxValues.yaml","title":"Metal LB"},{"location":"metalLB/#metallb-with-ingress-for-service-of-type-loadbalancer-in-onpremise-setup","text":"Requirements MetalLB requires the following to function: A Kubernetes cluster, running Kubernetes 1.13.0 or later, that does not already have network load-balancing functionality. A cluster network configuration that can coexist with MetalLB. Some IPv4 addresses for MetalLB to hand out. When using the BGP operating mode, you will need one or more routers capable of speaking BGP. Traffic on port 7946 (TCP & UDP) must be allowed between nodes, as required by memberlist.","title":"MetalLB with Ingress for service of type LoadBalancer in OnPremise setup."},{"location":"metalLB/#preparation","text":"If you\u2019re using kube-proxy in IPVS mode, since Kubernetes v1.14.2 you have to enable strict ARP mode. Note you don\u2019t need this if you\u2019re using kube-router as service-proxy because it is enabling strict ARP by default. You can achieve this by editing kube-proxy config in current cluster: kubectl edit configmap -n kube-system kube-proxy and set: apiVersion : kubeproxy.config.k8s.io/v1alpha1 kind : KubeProxy Configuration mode : \"ipvs\" ipvs : strictARP : true","title":"Preparation"},{"location":"metalLB/#installation-by-manifest","text":"To install MetalLB, apply the manifest: kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.11.0/manifests/namespace.yaml kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.11.0/manifests/metallb.yaml","title":"Installation By Manifest"},{"location":"metalLB/#layer-2-configuration","text":"Layer 2 mode is the simplest to configure: in many cases, you don\u2019t need any protocol-specific configuration, only IP addresses. Layer 2 mode does not require the IPs to be bound to the network interfaces of your worker nodes. It works by responding to ARP requests on your local network directly, to give the machine\u2019s MAC address to clients. For example, the following configuration gives MetalLB control over IPs from 192.168.1.240 to 192.168.1.250 , and configures Layer 2 mode: apiVersion : v1 kind : ConfigMap metadata : namespace : metallb-system name : config data : config : | address-pools : - name : default protocol : layer2 addresses : - 192.168.1.240-192.168.1.250","title":"Layer 2 Configuration"},{"location":"metalLB/#install-ingress-for-metallb","text":"helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm repo list helm search repo ingress helm show values ingress-nginx/ingress-nginx > ingress-nginxValues.yaml Update values file with changes mentioned below vim ingress-nginxValues.yaml hostNetwork : true ## Use host ports 80 and 443 ## Disabled by default ## hostPort : enabled : true ## DaemonSet or Deployment ## kind : DaemonSet helm install cruise-ingress ingress-nginx/ingress-nginx -n ingress-nginx --values ingress-nginxValues.yaml","title":"Install Ingress for metalLB"},{"location":"minio/","text":"Minio Configuration On The Storage Server \u00b6 Download the Minio Binary. wget https://dl.min.io/server/minio/release/linux-amd64/minio Make the minio binary as executable and move it to the Bin directory. chmod +x minio mv minio /usr/bin/minio ll /usr/bin/minio Store the Minio credentials as a variable and start it with Nohup export MINIO_ROOT_USER = <Username> export MINIO_ROOT_PASSWORD = <Password> nohup minio server --console-address <AnsibleNodeIP>:<Port> /data > /dev/null 2 > & 1 &","title":"Minio Setup"},{"location":"minio/#minio-configuration-on-the-storage-server","text":"Download the Minio Binary. wget https://dl.min.io/server/minio/release/linux-amd64/minio Make the minio binary as executable and move it to the Bin directory. chmod +x minio mv minio /usr/bin/minio ll /usr/bin/minio Store the Minio credentials as a variable and start it with Nohup export MINIO_ROOT_USER = <Username> export MINIO_ROOT_PASSWORD = <Password> nohup minio server --console-address <AnsibleNodeIP>:<Port> /data > /dev/null 2 > & 1 &","title":"Minio Configuration On The Storage Server"},{"location":"nexus/","text":"Nexus Server Setup \u00b6 Requirement 4 gb Ram, 2 cpu, 50 gb HD #!/bin/bash yum install java-1.8.0-openjdk.x86_64 wget -y mkdir -p /opt/nexus/ mkdir -p /tmp/nexus/ cd /tmp/nexus NEXUSURL = \"https://download.sonatype.com/nexus/3/latest-unix.tar.gz\" wget $NEXUSURL -O nexus.tar.gz EXTOUT = ` tar xzvf nexus.tar.gz ` NEXUSDIR = ` echo $EXTOUT | cut -d '/' -f1 ` rm -rf /tmp/nexus/nexus.tar.gz rsync -avzh /tmp/nexus/ /opt/nexus/ useradd nexus chown -R nexus.nexus /opt/nexus cat <<EOT>> /etc/systemd/system/nexus.service [Unit] Description=nexus service After=network.target [Service] Type=forking LimitNOFILE=65536 ExecStart=/opt/nexus/$NEXUSDIR/bin/nexus start ExecStop=/opt/nexus/$NEXUSDIR/bin/nexus stop User=nexus Restart=on-abort [Install] WantedBy=multi-user.target EOT echo 'run_as_user=\"nexus\"' > /opt/nexus/ $NEXUSDIR /bin/nexus.rc systemctl daemon-reload systemctl start nexus systemctl enable nexus Errors Docker-private-registry-x509-certificate-signed-by-unknown-authority Failed to pull image http: server gave HTTP response to HTTPS client SSL for Nexus repository \u00b6 keytool -genkeypair -keystore keystore.jks -storepass admin123 -keypass admin123 -alias jetty -keyalg RSA -keysize 2048 -validity 5000 -dname \"CN=*.cruise.org, OU=cruise, O=Sonatype, L=Unspecified, ST=Unspecified, C=IN\" -ext \"SAN=DNS:repository.cruise.org,IP:192.168.0.124\" -ext \"BC=ca:true\" cp keystore.jks <NExusHomeDir>/etc/ssl/ Vim jetty/jetty-https.xml < Set name = \"KeyStorePath\" >< Property name = \"ssl.etc\" />/ keystore . jks </ Set > < Set name = \"KeyStorePassword\" > admin123 </ Set > < Set name = \"KeyManagerPassword\" > admin123 </ Set > < Set name = \"TrustStorePath\" >< Property name = \"ssl.etc\" />/ keystore . jks </ Set > < Set name = \"TrustStorePassword\" > admin123 </ Set > vim cat nexus-default.properties application - port = 8081 application - port - ssl = 8443 application - host = 0.0.0.0 nexus - args = $ { jetty . etc } / jetty . xml , $ { jetty . etc } / jetty - http . xml , $ { jetty . etc } / jetty - https . xml , $ { jetty . etc } / jetty - requestlog . xml systemctl restart nexus Set https port for the repository from NExus webpage \u00b6 Set an insecure registry option in the worker and master node. \u00b6 Login to all the nodes in k8s cluster cat /etc/docker/daemon.json Output: \u00b6 { \"insecure-registries\":[\"repository.cruise.org:8084\"] } cat /etc/default/docker | grep DOCKER_OPTS Output: \u00b6 DOCKER_OPTS=\"--config-file=/etc/docker/daemon.json\" systemctl restart docker","title":"Nexus Server Setup"},{"location":"nexus/#nexus-server-setup","text":"Requirement 4 gb Ram, 2 cpu, 50 gb HD #!/bin/bash yum install java-1.8.0-openjdk.x86_64 wget -y mkdir -p /opt/nexus/ mkdir -p /tmp/nexus/ cd /tmp/nexus NEXUSURL = \"https://download.sonatype.com/nexus/3/latest-unix.tar.gz\" wget $NEXUSURL -O nexus.tar.gz EXTOUT = ` tar xzvf nexus.tar.gz ` NEXUSDIR = ` echo $EXTOUT | cut -d '/' -f1 ` rm -rf /tmp/nexus/nexus.tar.gz rsync -avzh /tmp/nexus/ /opt/nexus/ useradd nexus chown -R nexus.nexus /opt/nexus cat <<EOT>> /etc/systemd/system/nexus.service [Unit] Description=nexus service After=network.target [Service] Type=forking LimitNOFILE=65536 ExecStart=/opt/nexus/$NEXUSDIR/bin/nexus start ExecStop=/opt/nexus/$NEXUSDIR/bin/nexus stop User=nexus Restart=on-abort [Install] WantedBy=multi-user.target EOT echo 'run_as_user=\"nexus\"' > /opt/nexus/ $NEXUSDIR /bin/nexus.rc systemctl daemon-reload systemctl start nexus systemctl enable nexus Errors Docker-private-registry-x509-certificate-signed-by-unknown-authority Failed to pull image http: server gave HTTP response to HTTPS client","title":"Nexus Server Setup"},{"location":"nexus/#ssl-for-nexus-repository","text":"keytool -genkeypair -keystore keystore.jks -storepass admin123 -keypass admin123 -alias jetty -keyalg RSA -keysize 2048 -validity 5000 -dname \"CN=*.cruise.org, OU=cruise, O=Sonatype, L=Unspecified, ST=Unspecified, C=IN\" -ext \"SAN=DNS:repository.cruise.org,IP:192.168.0.124\" -ext \"BC=ca:true\" cp keystore.jks <NExusHomeDir>/etc/ssl/ Vim jetty/jetty-https.xml < Set name = \"KeyStorePath\" >< Property name = \"ssl.etc\" />/ keystore . jks </ Set > < Set name = \"KeyStorePassword\" > admin123 </ Set > < Set name = \"KeyManagerPassword\" > admin123 </ Set > < Set name = \"TrustStorePath\" >< Property name = \"ssl.etc\" />/ keystore . jks </ Set > < Set name = \"TrustStorePassword\" > admin123 </ Set > vim cat nexus-default.properties application - port = 8081 application - port - ssl = 8443 application - host = 0.0.0.0 nexus - args = $ { jetty . etc } / jetty . xml , $ { jetty . etc } / jetty - http . xml , $ { jetty . etc } / jetty - https . xml , $ { jetty . etc } / jetty - requestlog . xml systemctl restart nexus","title":"SSL for Nexus repository"},{"location":"nexus/#set-https-port-for-the-repository-from-nexus-webpage","text":"","title":"Set https port for the repository from NExus webpage"},{"location":"nexus/#set-an-insecure-registry-option-in-the-worker-and-master-node","text":"Login to all the nodes in k8s cluster cat /etc/docker/daemon.json","title":"Set an insecure registry option in the worker and master node."},{"location":"nexus/#output","text":"{ \"insecure-registries\":[\"repository.cruise.org:8084\"] } cat /etc/default/docker | grep DOCKER_OPTS","title":"Output:"},{"location":"nexus/#output_1","text":"DOCKER_OPTS=\"--config-file=/etc/docker/daemon.json\" systemctl restart docker","title":"Output:"},{"location":"nfs/","text":"NFS Server Setup \u00b6 How to Install and Configure an NFS Server on Ubuntu 18.04 Click Here NFS client installation on all worker node apt install nfs-common Clone nfs provisioner source code git clone https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner.git Setup provisioner and storage class cd nfs-subdir-external-provisioner/ kubectl create -f deploy/rbac.yaml vim deploy/deployment.yaml # Update nfs server ip and exported dir kubectl create -f deploy/deployment.yaml vim deploy/class.yaml # update storage class name & reclaimPolicy: Retain Create PVC definition cat vprodb-pvc.yaml --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : vprodbvol-nfs-claim spec : accessModes : - ReadWriteMany storageClassName : managed-nfs-storage resources : requests : storage : 500Mi POD Definition claiming pvc cat vprodbdep.yml apiVersion : apps/v1 kind : Deployment metadata : name : vprodb labels : app : vprodb spec : selector : matchLabels : app : vprodb replicas : 1 template : metadata : labels : app : vprodb spec : containers : - name : vprodb image : imranvisualpath/8pmdbimg:v1 ports : - name : vprodb-port containerPort : 3306 volumeMounts : - name : vprodbvol-nfs mountPath : /var/lib/mysql env : - name : MYSQL_ROOT_PASSWORD valueFrom : secretKeyRef : name : app-secret key : db-pass volumes : - name : vprodbvol-nfs persistentVolumeClaim : claimName : vprodbvol-nfs-claim","title":"NFS Server Setup"},{"location":"nfs/#nfs-server-setup","text":"How to Install and Configure an NFS Server on Ubuntu 18.04 Click Here NFS client installation on all worker node apt install nfs-common Clone nfs provisioner source code git clone https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner.git Setup provisioner and storage class cd nfs-subdir-external-provisioner/ kubectl create -f deploy/rbac.yaml vim deploy/deployment.yaml # Update nfs server ip and exported dir kubectl create -f deploy/deployment.yaml vim deploy/class.yaml # update storage class name & reclaimPolicy: Retain Create PVC definition cat vprodb-pvc.yaml --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : vprodbvol-nfs-claim spec : accessModes : - ReadWriteMany storageClassName : managed-nfs-storage resources : requests : storage : 500Mi POD Definition claiming pvc cat vprodbdep.yml apiVersion : apps/v1 kind : Deployment metadata : name : vprodb labels : app : vprodb spec : selector : matchLabels : app : vprodb replicas : 1 template : metadata : labels : app : vprodb spec : containers : - name : vprodb image : imranvisualpath/8pmdbimg:v1 ports : - name : vprodb-port containerPort : 3306 volumeMounts : - name : vprodbvol-nfs mountPath : /var/lib/mysql env : - name : MYSQL_ROOT_PASSWORD valueFrom : secretKeyRef : name : app-secret key : db-pass volumes : - name : vprodbvol-nfs persistentVolumeClaim : claimName : vprodbvol-nfs-claim","title":"NFS Server Setup"},{"location":"nginx-ingress/","text":"Installation Guide \u00b6 There are multiple ways to install the NGINX ingress controller: - with Helm , using the project repository chart; - with kubectl apply , using YAML manifests; On most Kubernetes clusters, the ingress controller will work without requiring any extra configuration. If you want to get started as fast as possible, you can check the quick start instructions. However, in many environments, you can improve the performance or get better logs by enabling extra features. we recommend that you check the environment-specific instructions for details about optimizing the ingress controller for your particular environment or cloud provider. Quick start \u00b6 You can deploy the ingress controller with the following command: helm upgrade --install ingress-nginx ingress-nginx \\ --repo https://kubernetes.github.io/ingress-nginx \\ --namespace ingress-nginx --create-namespace It will install the controller in the ingress-nginx namespace, creating that namespace if it doesn't already exist. Info This command is idempotent : - if the ingress controller is not installed, it will install it, - if the ingress controller is already installed, it will upgrade it. This requires Helm version 3. If you prefer to use a YAML manifest, you can run the following command instead: Attention Before running the command at your terminal, make sure Kubernetes is enabled at Docker settings kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.0/deploy/static/provider/cloud/deploy.yaml Info The YAML manifest in the command above was generated with helm template , so you will end up with almost the same resources as if you had used Helm to install the controller. If you are running an old version of Kubernetes (1.18 or earlier), please read this paragraph for specific instructions. Pre-flight check \u00b6 A few pods should start in the ingress-nginx namespace: kubectl get pods --namespace=ingress-nginx After a while, they should all be running. The following command will wait for the ingress controller pod to be up, running, and ready: kubectl wait --namespace ingress-nginx \\ --for=condition=ready pod \\ --selector=app.kubernetes.io/component=controller \\ --timeout=120s Local testing \u00b6 Let's create a simple web server and the associated service: kubectl create deployment demo --image=httpd --port=80 kubectl expose deployment demo Then create an ingress resource. The following example uses an host that maps to localhost : kubectl create ingress demo-localhost --class=nginx \\ --rule=demo.localdev.me/*=demo:80 Now, forward a local port to the ingress controller: kubectl port-forward --namespace=ingress-nginx service/ingress-nginx-controller 8080:80 At this point, if you access http://demo.localdev.me:8080/ , you should see an HTML page telling you \"It works!\". Online testing \u00b6 If your Kubernetes cluster is a \"real\" cluster that supports services of type LoadBalancer , it will have allocated an external IP address or FQDN to the ingress controller. You can see that IP address or FQDN with the following command: kubectl get service ingress-nginx-controller --namespace=ingress-nginx Set up a DNS record pointing to that IP address or FQDN; then create an ingress resource. The following example assumes that you have set up a DNS record for www.demo.io : kubectl create ingress demo --class=nginx \\ --rule=\"www.demo.io/*=demo:80\" Alternatively, the above command can be rewritten as follows for the --rule command and below. kubectl create ingress demo --class=nginx \\ --rule www.demo.io/=demo:80 You should then be able to see the \"It works!\" page when you connect to http://www.demo.io/ . Congratulations, you are serving a public web site hosted on a Kubernetes cluster! \ud83c\udf89","title":"Nginx"},{"location":"nginx-ingress/#installation-guide","text":"There are multiple ways to install the NGINX ingress controller: - with Helm , using the project repository chart; - with kubectl apply , using YAML manifests; On most Kubernetes clusters, the ingress controller will work without requiring any extra configuration. If you want to get started as fast as possible, you can check the quick start instructions. However, in many environments, you can improve the performance or get better logs by enabling extra features. we recommend that you check the environment-specific instructions for details about optimizing the ingress controller for your particular environment or cloud provider.","title":"Installation Guide"},{"location":"nginx-ingress/#quick-start","text":"You can deploy the ingress controller with the following command: helm upgrade --install ingress-nginx ingress-nginx \\ --repo https://kubernetes.github.io/ingress-nginx \\ --namespace ingress-nginx --create-namespace It will install the controller in the ingress-nginx namespace, creating that namespace if it doesn't already exist. Info This command is idempotent : - if the ingress controller is not installed, it will install it, - if the ingress controller is already installed, it will upgrade it. This requires Helm version 3. If you prefer to use a YAML manifest, you can run the following command instead: Attention Before running the command at your terminal, make sure Kubernetes is enabled at Docker settings kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.0/deploy/static/provider/cloud/deploy.yaml Info The YAML manifest in the command above was generated with helm template , so you will end up with almost the same resources as if you had used Helm to install the controller. If you are running an old version of Kubernetes (1.18 or earlier), please read this paragraph for specific instructions.","title":"Quick start"},{"location":"nginx-ingress/#pre-flight-check","text":"A few pods should start in the ingress-nginx namespace: kubectl get pods --namespace=ingress-nginx After a while, they should all be running. The following command will wait for the ingress controller pod to be up, running, and ready: kubectl wait --namespace ingress-nginx \\ --for=condition=ready pod \\ --selector=app.kubernetes.io/component=controller \\ --timeout=120s","title":"Pre-flight check"},{"location":"nginx-ingress/#local-testing","text":"Let's create a simple web server and the associated service: kubectl create deployment demo --image=httpd --port=80 kubectl expose deployment demo Then create an ingress resource. The following example uses an host that maps to localhost : kubectl create ingress demo-localhost --class=nginx \\ --rule=demo.localdev.me/*=demo:80 Now, forward a local port to the ingress controller: kubectl port-forward --namespace=ingress-nginx service/ingress-nginx-controller 8080:80 At this point, if you access http://demo.localdev.me:8080/ , you should see an HTML page telling you \"It works!\".","title":"Local testing"},{"location":"nginx-ingress/#online-testing","text":"If your Kubernetes cluster is a \"real\" cluster that supports services of type LoadBalancer , it will have allocated an external IP address or FQDN to the ingress controller. You can see that IP address or FQDN with the following command: kubectl get service ingress-nginx-controller --namespace=ingress-nginx Set up a DNS record pointing to that IP address or FQDN; then create an ingress resource. The following example assumes that you have set up a DNS record for www.demo.io : kubectl create ingress demo --class=nginx \\ --rule=\"www.demo.io/*=demo:80\" Alternatively, the above command can be rewritten as follows for the --rule command and below. kubectl create ingress demo --class=nginx \\ --rule www.demo.io/=demo:80 You should then be able to see the \"It works!\" page when you connect to http://www.demo.io/ . Congratulations, you are serving a public web site hosted on a Kubernetes cluster! \ud83c\udf89","title":"Online testing"},{"location":"setupv1/","text":"Setup \u00b6 Configure the host file with the hostname and IPs. \u00b6 cat /etc/hosts 192.168.0.100 dnsserver . cruise . com 192.168.135.21 test01 . cruise . com 192.168.0.100 dnsserver 127.0.0.1 localhost Download CoreDNS from the website, unpack the binary to /usr/local/bin and make it executable. \u00b6 wget https://github.com/coredns/coredns/releases/download/v1.8.6/coredns_1.8.6_linux_amd64.tgz tar -xvzf coredns_1.8.6_linux_amd64.tgz cp coredns /usr/local/bin/ sudo chmod +x /usr/local/bin/coredns ll /usr/local/bin/coredns Install resolvconf as a tool to manually manage /etc/resolv.conf . \u00b6 apt install resolvconf Set dns as default in /etc/NetworkManager/NetworkManager.conf . \u00b6 vi /etc/NetworkManager/NetworkManager.conf dns=default Add nameserver 127.0.0.1 to /etc/resolvconf/resolv.conf.d/head . \u00b6 vi /etc/resolvconf/resolv.conf.d/head nameserver 127.0.0.1 Create /etc/coredns/Corefile and paste the configuration shown below. \u00b6 vi /etc/coredns/Corefile groophy.org:53 { forward . tls://2606:4700:4700::1111 tls://1.1.1.1 hosts /etc/hosts log errors Cache } .:53 { forward . tls://2606:4700:4700::1111 tls://1.1.1.1 log errors Cache } We are using Cloudflare as a DNS provider(1.1.1.1). \u00b6 Create a new user for CoreDNS and set some permissions on the /opt/coredns directory. \u00b6 sudo useradd -d /var/lib/coredns -m coredns sudo chown coredns:coredns /opt/coredns Download the SystemD service unit file from coredns to /etc/systemd/system/coredns.service . \u00b6 wget https://github.com/coredns/deployment/blob/master/systemd/coredns.service mv coredns.service /etc/systemd/system/ Disable SystemD's default DNS server. \u00b6 sudo systemctl stop systemd-resolved && sudo systemctl disable systemd-resolved Note From that moment on, you will not be able to resolve any web pages anymore, unless you enable DNS again Enable and start CoreDNS \u00b6 sudo systemctl enable coredns && sudo systemctl start coredns Now we can able to resolve domain names, again. E.g. try to dig +short kit.edu . If an IP address is printed, everything works fine. \u00b6 dig +short kit.edu Output \u00b6 141.3.128.6 nslookup dnsserver.groophy.org Output \u00b6 Server : 192.168.0.100 Address : 192.168.0.100 # 53 Name : dnsserver . cruise . com Address : 192.168.0.100 nslookup google.com Output \u00b6 Server : 192.168.0.100 Address : 192.168.0.100 # 53 Non - authoritative answer : Name : google . com Address : 142.250.71.14 Name : google . com Address : 2404 : 6800 : 4007 : 824 :: 200 e Conclusion Able to resolve both internal servers and outside the world.","title":"Setupv1"},{"location":"setupv1/#setup","text":"","title":"Setup"},{"location":"setupv1/#configure-the-host-file-with-the-hostname-and-ips","text":"cat /etc/hosts 192.168.0.100 dnsserver . cruise . com 192.168.135.21 test01 . cruise . com 192.168.0.100 dnsserver 127.0.0.1 localhost","title":"Configure the host file with the hostname and IPs."},{"location":"setupv1/#download-coredns-from-the-website-unpack-the-binary-to-usrlocalbin-and-make-it-executable","text":"wget https://github.com/coredns/coredns/releases/download/v1.8.6/coredns_1.8.6_linux_amd64.tgz tar -xvzf coredns_1.8.6_linux_amd64.tgz cp coredns /usr/local/bin/ sudo chmod +x /usr/local/bin/coredns ll /usr/local/bin/coredns","title":"Download CoreDNS from the website, unpack the binary to /usr/local/bin and make it executable."},{"location":"setupv1/#install-resolvconf-as-a-tool-to-manually-manage-etcresolvconf","text":"apt install resolvconf","title":"Install resolvconf as a tool to manually manage /etc/resolv.conf."},{"location":"setupv1/#set-dns-as-default-in-etcnetworkmanagernetworkmanagerconf","text":"vi /etc/NetworkManager/NetworkManager.conf dns=default","title":"Set dns as default in /etc/NetworkManager/NetworkManager.conf."},{"location":"setupv1/#add-nameserver-127001-to-etcresolvconfresolvconfdhead","text":"vi /etc/resolvconf/resolv.conf.d/head nameserver 127.0.0.1","title":"Add nameserver 127.0.0.1 to /etc/resolvconf/resolv.conf.d/head."},{"location":"setupv1/#create-etccorednscorefile-and-paste-the-configuration-shown-below","text":"vi /etc/coredns/Corefile groophy.org:53 { forward . tls://2606:4700:4700::1111 tls://1.1.1.1 hosts /etc/hosts log errors Cache } .:53 { forward . tls://2606:4700:4700::1111 tls://1.1.1.1 log errors Cache }","title":"Create /etc/coredns/Corefile and paste the configuration shown below."},{"location":"setupv1/#we-are-using-cloudflare-as-a-dns-provider1111","text":"","title":"We are using Cloudflare as a DNS provider(1.1.1.1)."},{"location":"setupv1/#create-a-new-user-for-coredns-and-set-some-permissions-on-the-optcoredns-directory","text":"sudo useradd -d /var/lib/coredns -m coredns sudo chown coredns:coredns /opt/coredns","title":"Create a new user for CoreDNS and set some permissions on the /opt/coredns directory."},{"location":"setupv1/#download-the-systemd-service-unit-file-from-coredns-to-etcsystemdsystemcorednsservice","text":"wget https://github.com/coredns/deployment/blob/master/systemd/coredns.service mv coredns.service /etc/systemd/system/","title":"Download the SystemD service unit file from coredns to /etc/systemd/system/coredns.service."},{"location":"setupv1/#disable-systemds-default-dns-server","text":"sudo systemctl stop systemd-resolved && sudo systemctl disable systemd-resolved Note From that moment on, you will not be able to resolve any web pages anymore, unless you enable DNS again","title":"Disable SystemD's default DNS server."},{"location":"setupv1/#enable-and-start-coredns","text":"sudo systemctl enable coredns && sudo systemctl start coredns","title":"Enable and start CoreDNS"},{"location":"setupv1/#now-we-can-able-to-resolve-domain-names-again-eg-try-to-dig-short-kitedu-if-an-ip-address-is-printed-everything-works-fine","text":"dig +short kit.edu","title":"Now we can able to resolve domain names, again. E.g. try to dig +short kit.edu. If an IP address is printed, everything works fine."},{"location":"setupv1/#output","text":"141.3.128.6 nslookup dnsserver.groophy.org","title":"Output"},{"location":"setupv1/#output_1","text":"Server : 192.168.0.100 Address : 192.168.0.100 # 53 Name : dnsserver . cruise . com Address : 192.168.0.100 nslookup google.com","title":"Output"},{"location":"setupv1/#output_2","text":"Server : 192.168.0.100 Address : 192.168.0.100 # 53 Non - authoritative answer : Name : google . com Address : 142.250.71.14 Name : google . com Address : 2404 : 6800 : 4007 : 824 :: 200 e Conclusion Able to resolve both internal servers and outside the world.","title":"Output"},{"location":"setupv2/","text":"Setup \u00b6 Configure the host file with the hostname and IPs. cat /etc/hosts 192.168.0.100 dnsserver . cruise . com 192.168.135.21 test01 . cruise . com 192.168.0.100 dnsserver 127.0.0.1 localhost Download CoreDNS from the website, unpack the binary to /usr/local/bin and make it executable. wget https://github.com/coredns/coredns/releases/download/v1.8.6/coredns_1.8.6_linux_amd64.tgz tar -xvzf coredns_1.8.6_linux_amd64.tgz cp coredns /usr/local/bin/ sudo chmod +x /usr/local/bin/coredns ll /usr/local/bin/coredns Install resolvconf as a tool to manually manage /etc/resolv.conf . apt install resolvconf Set dns as default in /etc/NetworkManager/NetworkManager.conf . vi /etc/NetworkManager/NetworkManager.conf dns=default Add nameserver 127.0.0.1 to /etc/resolvconf/resolv.conf.d/head . vi /etc/resolvconf/resolv.conf.d/head nameserver 127.0.0.1 Create /etc/coredns/Corefile and paste the configuration shown below. vi /etc/coredns/Corefile groophy.org:53 { forward . tls://2606:4700:4700::1111 tls://1.1.1.1 hosts /etc/hosts log errors Cache } .:53 { forward . tls://2606:4700:4700::1111 tls://1.1.1.1 log errors Cache } We are using Cloudflare as a DNS provider(1.1.1.1). Create a new user for CoreDNS and set some permissions on the /opt/coredns directory. sudo useradd -d /var/lib/coredns -m coredns sudo chown coredns:coredns /opt/coredns Download the SystemD service unit file from coredns to /etc/systemd/system/coredns.service . wget https://github.com/coredns/deployment/blob/master/systemd/coredns.service mv coredns.service /etc/systemd/system/ Disable SystemD's default DNS server. sudo systemctl stop systemd-resolved && sudo systemctl disable systemd-resolved Note From that moment on, you will not be able to resolve any web pages anymore, unless you enable DNS again Enable and start CoreDNS sudo systemctl enable coredns && sudo systemctl start coredns Now we can able to resolve domain names, again. E.g. try to dig +short kit.edu . If an IP address is printed, everything works fine. dig +short kit.edu Output 141.3.128.6 Try the below command nslookup dnsserver.groophy.org Output Server : 192.168.0.100 Address : 192.168.0.100 # 53 Name : dnsserver . cruise . com Address : 192.168.0.100 Try the below command nslookup google.com Output Server : 192.168.0.100 Address : 192.168.0.100 # 53 Non - authoritative answer : Name : google . com Address : 142.250.71.14 Name : google . com Address : 2404 : 6800 : 4007 : 824 :: 200 e Conclusion Able to resolve both internal servers and outside the world.","title":"Setupv2"},{"location":"setupv2/#setup","text":"Configure the host file with the hostname and IPs. cat /etc/hosts 192.168.0.100 dnsserver . cruise . com 192.168.135.21 test01 . cruise . com 192.168.0.100 dnsserver 127.0.0.1 localhost Download CoreDNS from the website, unpack the binary to /usr/local/bin and make it executable. wget https://github.com/coredns/coredns/releases/download/v1.8.6/coredns_1.8.6_linux_amd64.tgz tar -xvzf coredns_1.8.6_linux_amd64.tgz cp coredns /usr/local/bin/ sudo chmod +x /usr/local/bin/coredns ll /usr/local/bin/coredns Install resolvconf as a tool to manually manage /etc/resolv.conf . apt install resolvconf Set dns as default in /etc/NetworkManager/NetworkManager.conf . vi /etc/NetworkManager/NetworkManager.conf dns=default Add nameserver 127.0.0.1 to /etc/resolvconf/resolv.conf.d/head . vi /etc/resolvconf/resolv.conf.d/head nameserver 127.0.0.1 Create /etc/coredns/Corefile and paste the configuration shown below. vi /etc/coredns/Corefile groophy.org:53 { forward . tls://2606:4700:4700::1111 tls://1.1.1.1 hosts /etc/hosts log errors Cache } .:53 { forward . tls://2606:4700:4700::1111 tls://1.1.1.1 log errors Cache } We are using Cloudflare as a DNS provider(1.1.1.1). Create a new user for CoreDNS and set some permissions on the /opt/coredns directory. sudo useradd -d /var/lib/coredns -m coredns sudo chown coredns:coredns /opt/coredns Download the SystemD service unit file from coredns to /etc/systemd/system/coredns.service . wget https://github.com/coredns/deployment/blob/master/systemd/coredns.service mv coredns.service /etc/systemd/system/ Disable SystemD's default DNS server. sudo systemctl stop systemd-resolved && sudo systemctl disable systemd-resolved Note From that moment on, you will not be able to resolve any web pages anymore, unless you enable DNS again Enable and start CoreDNS sudo systemctl enable coredns && sudo systemctl start coredns Now we can able to resolve domain names, again. E.g. try to dig +short kit.edu . If an IP address is printed, everything works fine. dig +short kit.edu Output 141.3.128.6 Try the below command nslookup dnsserver.groophy.org Output Server : 192.168.0.100 Address : 192.168.0.100 # 53 Name : dnsserver . cruise . com Address : 192.168.0.100 Try the below command nslookup google.com Output Server : 192.168.0.100 Address : 192.168.0.100 # 53 Non - authoritative answer : Name : google . com Address : 142.250.71.14 Name : google . com Address : 2404 : 6800 : 4007 : 824 :: 200 e Conclusion Able to resolve both internal servers and outside the world.","title":"Setup"},{"location":"testing/","text":"Multi Master Node Cluster Configuring cluster with 3 masters, 3 workers and a load balancer with below configuration. Bastion host to access the cluster from outside. Category Configuration Count Load Balancer(HA-Proxy) CPUs: 2RAM: 2GBDisk: 10GbNIC: 1 1 Master Nodes CPUs: 2RAM: 2GBDisk: 10GBNIC: 1 3 Worker Nodes CPUs: 2RAM: 2GBDisk: 10GBNIC: 1 3 Bastion CPUs: 1RAM: 2GBDisk: 10GBNIC: 1 1 Share the SSH public key of the Bastion host to the rest of the server for transferring the certificates. \u00b6 \u00b6 \u00b6 \u00b6 \u00b6 Installing the HAProxy load balancer On Load Balancer Host \u00b6 To deploy three Kubernetes master nodes, one needs to deploy an HAPRoxy load balancer in front of them to distribute the traffic. apt-get update apt-get upgrade apt-get install haproxy Add below lines in the haproxy configuration file and restart the service. Carefully update the hostnames and IPs to avoid the conflicts. If the DNS server is configured, the Load Balancer name can be defined. vim /etc/haproxy/haproxy.cfg frontend kubernetes bind LoadBalancerIP_ :6443_ option tcplog mode tcp default_backend kubernetes-master-nodes backend kubernetes-master-nodes mode tcp balance roundrobin option tcp-check server \\<Master01 Name\\> _ \\<Master01 IP\\>__ :6443 check fall 3 rise 2_ server \\<Master02 Name\\> _ \\<Master02 IP\\>__ :6443 check fall 3 rise 2_ server \\<Master03 Name\\> _ \\<Master03 IP\\>__ :6443 check fall 3 rise 2_ systemctl restart haproxy \u00b6 \u00b6 \u00b6 \u00b6 \u00b6 \u00b6 Installing the client tools on the Bastion Host \u00b6 Installing cfssl: \u00b6 Download the binaries and add the execution permission to the binaries. apt-get update wget https: / /pkg.cfssl.org/R1 . 2 /cfssl_linux-amd64 wget https: / /pkg.cfssl.org/R1 . 2 /cfssl_linux-amd64 chmod +x cfssl* mv cfssl_linux-amd64 /usr/local/bin/cfssl mv cfssljson_linux-amd64 /usr/local/bin/cfssljson cfssl version Installing kubectl: \u00b6 Download the kubectl binaries, add the execution permission to the binaries. wget https://storage.googleapis.com/kubernetes-release/release/v1.12.1/bin/linux/amd64/kubectl chmod +x kubectl sudo mv kubectl /usr/local/bin kubectl version Generating the TLS certificates \u00b6 Creating a certificate authority: \u00b6 Create the certificate authority and certificate authority signing request configuration file. vim ca-config.json { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } vim ca-csr.json { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"IE\", \"L\": \"Cork\", \"O\": \"Kubernetes\", \"OU\": \"CA\", \"ST\": \"Cork Co.\" } ] } Generate the certificate authority certificate and private key. cfssl gencert -initca ca-csr.json | cfssljson -bare ca ls -la Creating the certificate for the Etcd cluster: \u00b6 Create the certificate signing request configuration file. vim kubernetes-csr.json { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"IE\", \"L\": \"Cork\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes\", \"ST\": \"Cork Co.\" } ] } Generate the certificate and private key: _cfssl gencert _ _-ca=ca.pem _ _-ca-key=ca-key.pem _ _-config=ca-config.json _ -hostname= _ \\<Master01_IP\\>__ , \\<Master02_IP\\> , \\<Master03_IP\\> , \\<LoadBalncer_IP\\> ,127.0.0.1,kubernetes.default _ _-profile=kubernetes kubernetes-csr.json | _ cfssljson -bare kubernetes Copy the certificate to the both Master and Worker node: sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ _ \\<Master01_IP\\>__ :~_ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ _ \\<Master02_IP\\>__ :~_ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ _ \\<Master03_IP\\>__ :~_ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ _ \\<Worker01_IP\\>__ :~_ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ _ \\<Worker02_IP\\>__ :~_ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ _ \\<worker03_IP\\>__ :~_ Preparing the Master and Worker nodes for kubeadm \u00b6 Installing Docker: \u00b6 Add the Docker repository key and Docker repository. curl -fsSL https:// download.docker.com/linux/ubuntu/gpg | apt-key add - _add-apt-repository _ \"deb https://download.docker.com/linux/ _ \\((. /etc/os-release; echo \"\\) ID\") _ _$(lsb_release -cs) _ stable\" Update the list of packages and install the docker apt-get update apt-get install -y docker-ce echo '{\"exec-opts\": [\"native.cgroupdriver=systemd\"]}' | sudo tee /etc/docker/daemon.json systemctl daemon-reload systemctl restart docker systemctl enable docker Installing kubeadm, kublet, and kubectl: \u00b6 Add the Google repository key and Google repository curl -s https:// packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - vim /etc/apt/sources.list.d/kubernetes.list _ deb http://apt.kubernetes.io _kubernetes-xenial main Update the list of packages. And install kubelet, kubeadm and kubectl. apt-get update apt-get install kubelet kubeadm kubectl -y Disable the swap. swapoff -a sed -i '/ swap / s/^/#/' /etc/fstab Installing and configuring Etcd on the Master Nodes. \u00b6 Create a configuration directory for Etcd and move the certificates into it. sudo mkdir /etc/etcd /var/lib/etcd sudo mv ~/ca.pem ~/kubernetes.pem ~/kubernetes-key.pem /etc/etcd Download and extract the etcd archive and move it to /usr/local/bin. wget https:// github.com/coreos /etcd/releases/download/v3.3.9/etcd-v3.3.9-linux-amd64.tar.gz tar xvzf etcd-v3.3.9-linux-amd64.tar.gz sudo mv etcd-v3.3.9-linux-amd64/etcd* /usr/local/bin/ Create an etcd systemd unit file. sudo vim /etc/systemd/system/etcd.service [Unit] Description=etcd Documentation=https:// github.com/coreos [Service] _ExecStart=/usr/local/bin/etcd _ --name \\<host_IP\\> __ _--cert-file=/etc/etcd/kubernetes.pem _ _--key-file=/etc/etcd/kubernetes-key.pem _ _--peer-cert-file=/etc/etcd/kubernetes.pem _ _--peer-key-file=/etc/etcd/kubernetes-key.pem _ _--trusted-ca-file=/etc/etcd/ca.pem _ _--peer-trusted-ca-file=/etc/etcd/ca.pem _ _--peer-client-cert-auth _ _--client-cert-auth _ --initial-advertise-peer-urls https:// _ \\<host_IP\\>__ :2380 _ --listen-peer-urls https:// _ \\<host_IP\\>__ :2380 _ --listen-client-urls https:// _ \\<host_IP\\>__ :2379, http://127.0.0.1:2379 _ --advertise-client-urls https:// _ \\<host_IP\\>__ :2379 _ _--initial-cluster-token etcd-cluster-0 _ --initial-cluster \\<Master01_IP\\> _ =https://__ \\<Master01_IP\\> :2380, \\<Master02_IP\\> =https:// \\<Master02_IP\\> :2380, \\<Master03_IP\\> =https:// \\<Master03_IP\\>__:2380 _ _--initial-cluster-state new _ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target Reload the daemon configuration, Enable and start the etcd. sudo systemctl daemon-reload sudo systemctl enable etcd sudo systemctl start etcd Verify that the cluster is up and running. ETCDCTL_API=__3 etcdctl member list \u00b6 Initializing the master nodes \u00b6 Initializing the master01 node: \u00b6 Create the configuration file for kubeadm. vim config.yaml sudo kubeadm init --config=config.yaml sudo scp -r /etc/kubernetes/pki opsadmin@ _ \\<Master02_IP\\>__ :~_ sudo scp -r /etc/kubernetes/pki opsadmin@ _ \\<Master03_IP\\>__ :~_ sudo scp -r config.yaml opsadmin@ _ \\<Master03_IP\\>__ :~_ Configure Master02 and 03 Remove the apiserver.crt and apiserver.key. rm ~ _ /pki/apiserver__.*_ Move the certificates to the /etc/kubernetes directory. sudo mv ~/pki /etc/kubernetes/ sudo kubeadm init --config=config.yaml Initializing the worker nodes \u00b6 Copy the \"kubeadm join\" command from the lastly created master node and execute on the worker nodes. sudo __kubeadm_ join_ 10 _ .10.40.93:6443__ --token__[your_token] --discovery-token-ca-cert-hash __sha256 :__[your_token_ca_cert_hash]_ Configuring kubectl on the client machine \u00b6 sudo chmod +r /etc/kubernetes/admin.conf scp opsadmin@ _ \\<Master01_IP\\>__ :/etc/kubernetes/admin.conf ._ _ mkdir _ ~/.kube mv admin.conf ~/.kube/config chmod 600 ~/.kube/config Deploying the Calico as overlay network \u00b6 kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml kubectl create -f https://docs.projectcalico.org/manifests/custom-resources.yamlwatch kubectl get pods -n calico-system Validate nodes are in Ready state. kubectl get nodes References: https://blog.inkubate.io/install-and-configure-a-multi-master-kubernetes-cluster-with-kubeadm/ Analysis: Config file error This version of kubeadm only supports deploying clusters with the control plane version \\>= 1.16.0. Current version: v1.12.0 Kubernetes \u00b6 https://stackoverflow.com/questions/61926846/this-version-of-kubeadm-only-supports-deploying-clusters-with-the-control-plane https://pkg.go.dev/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2 1. Error \u00b6 [kubelet-check] It seems like the kubelet isn't running or healthy. [kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz&#39 ; failed with error: Get http://localhost:10248/healthz : dial tcp 127.0.0.1:10248: connect: connection refused. Solution: Updating the /etc/docker/daemon.json file https://www.myfreax.com/how-to-solve-dial-tcp-127-0-0-1-10248-connect-connection-refused-during-kubeadm-init-initialization/ Disable swap on all the master and worker nodes. Carefully give the hostnames and IPs while configuring configuration files, generating ETCD certificates, hosts and hostname file, HA proxy config file. DNS server entry in the resolv.conf and /etc/resolvconf/resolv.conf.d/head. kubernetes node not ready but pod running. Due to wrong hostname given in HA config file and on the node. https://stackoverflow.com/questions/47107117/how-to-debug-when-kubernetes-nodes-are-in-not-ready-state https://stackoverflow.com/questions/49112336/container-runtime-network-not-ready-cni-config-uninitialized https://github.com/kubernetes/kubernetes/issues/96459","title":"Testing"},{"location":"testing/#share-the-ssh-public-key-of-the-bastion-host-to-the-rest-of-the-server-for-transferring-the-certificates","text":"","title":"Share the SSH public key of the Bastion host to the rest of the server for transferring the certificates."},{"location":"testing/#_1","text":"","title":""},{"location":"testing/#_2","text":"","title":""},{"location":"testing/#_3","text":"","title":""},{"location":"testing/#_4","text":"","title":""},{"location":"testing/#installing-the-haproxy-load-balancer-on-load-balancer-host","text":"To deploy three Kubernetes master nodes, one needs to deploy an HAPRoxy load balancer in front of them to distribute the traffic. apt-get update apt-get upgrade apt-get install haproxy Add below lines in the haproxy configuration file and restart the service. Carefully update the hostnames and IPs to avoid the conflicts. If the DNS server is configured, the Load Balancer name can be defined. vim /etc/haproxy/haproxy.cfg frontend kubernetes bind LoadBalancerIP_ :6443_ option tcplog mode tcp default_backend kubernetes-master-nodes backend kubernetes-master-nodes mode tcp balance roundrobin option tcp-check server \\<Master01 Name\\> _ \\<Master01 IP\\>__ :6443 check fall 3 rise 2_ server \\<Master02 Name\\> _ \\<Master02 IP\\>__ :6443 check fall 3 rise 2_ server \\<Master03 Name\\> _ \\<Master03 IP\\>__ :6443 check fall 3 rise 2_ systemctl restart haproxy","title":"Installing the HAProxy load balancer On Load Balancer Host"},{"location":"testing/#_5","text":"","title":""},{"location":"testing/#_6","text":"","title":""},{"location":"testing/#_7","text":"","title":""},{"location":"testing/#_8","text":"","title":""},{"location":"testing/#_9","text":"","title":""},{"location":"testing/#_10","text":"","title":""},{"location":"testing/#installing-the-client-tools-on-the-bastion-host","text":"","title":"Installing the client tools on the Bastion Host"},{"location":"testing/#installing-cfssl","text":"Download the binaries and add the execution permission to the binaries. apt-get update wget https: / /pkg.cfssl.org/R1 . 2 /cfssl_linux-amd64 wget https: / /pkg.cfssl.org/R1 . 2 /cfssl_linux-amd64 chmod +x cfssl* mv cfssl_linux-amd64 /usr/local/bin/cfssl mv cfssljson_linux-amd64 /usr/local/bin/cfssljson cfssl version","title":"Installing cfssl:"},{"location":"testing/#installing-kubectl","text":"Download the kubectl binaries, add the execution permission to the binaries. wget https://storage.googleapis.com/kubernetes-release/release/v1.12.1/bin/linux/amd64/kubectl chmod +x kubectl sudo mv kubectl /usr/local/bin kubectl version","title":"Installing kubectl:"},{"location":"testing/#generating-the-tls-certificates","text":"","title":"Generating the TLS certificates"},{"location":"testing/#creating-a-certificate-authority","text":"Create the certificate authority and certificate authority signing request configuration file. vim ca-config.json { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } vim ca-csr.json { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"IE\", \"L\": \"Cork\", \"O\": \"Kubernetes\", \"OU\": \"CA\", \"ST\": \"Cork Co.\" } ] } Generate the certificate authority certificate and private key. cfssl gencert -initca ca-csr.json | cfssljson -bare ca ls -la","title":"Creating a certificate authority:"},{"location":"testing/#creating-the-certificate-for-the-etcd-cluster","text":"Create the certificate signing request configuration file. vim kubernetes-csr.json { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"IE\", \"L\": \"Cork\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes\", \"ST\": \"Cork Co.\" } ] } Generate the certificate and private key: _cfssl gencert _ _-ca=ca.pem _ _-ca-key=ca-key.pem _ _-config=ca-config.json _ -hostname= _ \\<Master01_IP\\>__ , \\<Master02_IP\\> , \\<Master03_IP\\> , \\<LoadBalncer_IP\\> ,127.0.0.1,kubernetes.default _ _-profile=kubernetes kubernetes-csr.json | _ cfssljson -bare kubernetes Copy the certificate to the both Master and Worker node: sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ _ \\<Master01_IP\\>__ :~_ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ _ \\<Master02_IP\\>__ :~_ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ _ \\<Master03_IP\\>__ :~_ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ _ \\<Worker01_IP\\>__ :~_ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ _ \\<Worker02_IP\\>__ :~_ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ _ \\<worker03_IP\\>__ :~_","title":"Creating the certificate for the Etcd cluster:"},{"location":"testing/#preparing-the-master-and-worker-nodes-for-kubeadm","text":"","title":"Preparing the Master and Worker nodes for kubeadm"},{"location":"testing/#installing-docker","text":"Add the Docker repository key and Docker repository. curl -fsSL https:// download.docker.com/linux/ubuntu/gpg | apt-key add - _add-apt-repository _ \"deb https://download.docker.com/linux/ _ \\((. /etc/os-release; echo \"\\) ID\") _ _$(lsb_release -cs) _ stable\" Update the list of packages and install the docker apt-get update apt-get install -y docker-ce echo '{\"exec-opts\": [\"native.cgroupdriver=systemd\"]}' | sudo tee /etc/docker/daemon.json systemctl daemon-reload systemctl restart docker systemctl enable docker","title":"Installing Docker:"},{"location":"testing/#installing-kubeadm-kublet-and-kubectl","text":"Add the Google repository key and Google repository curl -s https:// packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - vim /etc/apt/sources.list.d/kubernetes.list _ deb http://apt.kubernetes.io _kubernetes-xenial main Update the list of packages. And install kubelet, kubeadm and kubectl. apt-get update apt-get install kubelet kubeadm kubectl -y Disable the swap. swapoff -a sed -i '/ swap / s/^/#/' /etc/fstab","title":"Installing kubeadm, kublet, and kubectl:"},{"location":"testing/#installing-and-configuring-etcd-on-the-master-nodes","text":"Create a configuration directory for Etcd and move the certificates into it. sudo mkdir /etc/etcd /var/lib/etcd sudo mv ~/ca.pem ~/kubernetes.pem ~/kubernetes-key.pem /etc/etcd Download and extract the etcd archive and move it to /usr/local/bin. wget https:// github.com/coreos /etcd/releases/download/v3.3.9/etcd-v3.3.9-linux-amd64.tar.gz tar xvzf etcd-v3.3.9-linux-amd64.tar.gz sudo mv etcd-v3.3.9-linux-amd64/etcd* /usr/local/bin/ Create an etcd systemd unit file. sudo vim /etc/systemd/system/etcd.service [Unit] Description=etcd Documentation=https:// github.com/coreos [Service] _ExecStart=/usr/local/bin/etcd _ --name \\<host_IP\\> __ _--cert-file=/etc/etcd/kubernetes.pem _ _--key-file=/etc/etcd/kubernetes-key.pem _ _--peer-cert-file=/etc/etcd/kubernetes.pem _ _--peer-key-file=/etc/etcd/kubernetes-key.pem _ _--trusted-ca-file=/etc/etcd/ca.pem _ _--peer-trusted-ca-file=/etc/etcd/ca.pem _ _--peer-client-cert-auth _ _--client-cert-auth _ --initial-advertise-peer-urls https:// _ \\<host_IP\\>__ :2380 _ --listen-peer-urls https:// _ \\<host_IP\\>__ :2380 _ --listen-client-urls https:// _ \\<host_IP\\>__ :2379, http://127.0.0.1:2379 _ --advertise-client-urls https:// _ \\<host_IP\\>__ :2379 _ _--initial-cluster-token etcd-cluster-0 _ --initial-cluster \\<Master01_IP\\> _ =https://__ \\<Master01_IP\\> :2380, \\<Master02_IP\\> =https:// \\<Master02_IP\\> :2380, \\<Master03_IP\\> =https:// \\<Master03_IP\\>__:2380 _ _--initial-cluster-state new _ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target Reload the daemon configuration, Enable and start the etcd. sudo systemctl daemon-reload sudo systemctl enable etcd sudo systemctl start etcd Verify that the cluster is up and running. ETCDCTL_API=__3 etcdctl member list","title":"Installing and configuring Etcd on the Master Nodes."},{"location":"testing/#_11","text":"","title":""},{"location":"testing/#initializing-the-master-nodes","text":"","title":"Initializing the master nodes"},{"location":"testing/#initializing-the-master01-node","text":"Create the configuration file for kubeadm. vim config.yaml sudo kubeadm init --config=config.yaml sudo scp -r /etc/kubernetes/pki opsadmin@ _ \\<Master02_IP\\>__ :~_ sudo scp -r /etc/kubernetes/pki opsadmin@ _ \\<Master03_IP\\>__ :~_ sudo scp -r config.yaml opsadmin@ _ \\<Master03_IP\\>__ :~_ Configure Master02 and 03 Remove the apiserver.crt and apiserver.key. rm ~ _ /pki/apiserver__.*_ Move the certificates to the /etc/kubernetes directory. sudo mv ~/pki /etc/kubernetes/ sudo kubeadm init --config=config.yaml","title":"Initializing the master01 node:"},{"location":"testing/#initializing-the-worker-nodes","text":"Copy the \"kubeadm join\" command from the lastly created master node and execute on the worker nodes. sudo __kubeadm_ join_ 10 _ .10.40.93:6443__ --token__[your_token] --discovery-token-ca-cert-hash __sha256 :__[your_token_ca_cert_hash]_","title":"Initializing the worker nodes"},{"location":"testing/#configuring-kubectl-on-the-client-machine","text":"sudo chmod +r /etc/kubernetes/admin.conf scp opsadmin@ _ \\<Master01_IP\\>__ :/etc/kubernetes/admin.conf ._ _ mkdir _ ~/.kube mv admin.conf ~/.kube/config chmod 600 ~/.kube/config","title":"Configuring kubectl on the client machine"},{"location":"testing/#deploying-the-calico-as-overlay-network","text":"kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml kubectl create -f https://docs.projectcalico.org/manifests/custom-resources.yamlwatch kubectl get pods -n calico-system Validate nodes are in Ready state. kubectl get nodes References: https://blog.inkubate.io/install-and-configure-a-multi-master-kubernetes-cluster-with-kubeadm/ Analysis: Config file error","title":"Deploying the Calico as overlay network"},{"location":"testing/#this-version-of-kubeadm-only-supports-deploying-clusters-with-the-control-plane-version-1160-current-version-v1120-kubernetes","text":"https://stackoverflow.com/questions/61926846/this-version-of-kubeadm-only-supports-deploying-clusters-with-the-control-plane https://pkg.go.dev/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2 1.","title":"This version of kubeadm only supports deploying clusters with the control plane version \\&gt;= 1.16.0. Current version: v1.12.0 Kubernetes"},{"location":"testing/#error","text":"[kubelet-check] It seems like the kubelet isn't running or healthy. [kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz&#39 ; failed with error: Get http://localhost:10248/healthz : dial tcp 127.0.0.1:10248: connect: connection refused. Solution: Updating the /etc/docker/daemon.json file https://www.myfreax.com/how-to-solve-dial-tcp-127-0-0-1-10248-connect-connection-refused-during-kubeadm-init-initialization/ Disable swap on all the master and worker nodes. Carefully give the hostnames and IPs while configuring configuration files, generating ETCD certificates, hosts and hostname file, HA proxy config file. DNS server entry in the resolv.conf and /etc/resolvconf/resolv.conf.d/head. kubernetes node not ready but pod running. Due to wrong hostname given in HA config file and on the node. https://stackoverflow.com/questions/47107117/how-to-debug-when-kubernetes-nodes-are-in-not-ready-state https://stackoverflow.com/questions/49112336/container-runtime-network-not-ready-cni-config-uninitialized https://github.com/kubernetes/kubernetes/issues/96459","title":"Error"},{"location":"testing2/","text":"Multi Master Node Cluster Configuring cluster with 3 masters, 3 workers and a load balancer with below configuration. Bastion host to access the cluster from outside. Category Configuration Count Load Balancer (HA-Proxy) CPUs: 2 RAM: 2GB Disk: 10Gb NIC: 1 1 Master Nodes CPUs: 2 RAM: 2GB Disk: 10GB NIC: 1 3 Worker Nodes CPUs: 2 RAM: 2GB Disk: 10GB NIC: 1 3 Bastion CPUs: 1 RAM: 2GB Disk: 10GB NIC: 1 1 Share the SSH public key of the Bastion host to the rest of the server for transferring the certificates. \u00b6 \u00b6 \u00b6 \u00b6 \u00b6 Installing the HAProxy load balancer On Load Balancer Host \u00b6 To deploy three Kubernetes master nodes, one needs to deploy an HAPRoxy load balancer in front of them to distribute the traffic. apt-get update apt-get upgrade apt-get install haproxy Add below lines in the haproxy configuration file and restart the service. Carefully update the hostnames and IPs to avoid the conflicts. If the DNS server is configured, the Load Balancer name can be defined. vim /etc/haproxy/haproxy.cfg frontend kubernetes bind LoadBalancerIP:6443 option tcplog mode tcp default_backend kubernetes-master-nodes backend kubernetes-master-nodes mode tcp balance roundrobin option tcp-check server :6443 check fall 3 rise 2 server :6443 check fall 3 rise 2 server :6443 check fall 3 rise 2 systemctl restart haproxy \u00b6 \u00b6 \u00b6 \u00b6 \u00b6 \u00b6 Installing the client tools on the Bastion Host \u00b6 Installing cfssl: \u00b6 Download the binaries and add the execution permission to the binaries. apt-get update wget https: / /pkg.cfssl.org/R1 . 2 /cfssl_linux-amd64* wget https: / /pkg.cfssl.org/R1 . 2 /cfssl_linux-amd64* chmod +x cfssl* mv cfssl_linux-amd64 /usr/local/bin/cfssl mv cfssljson_linux-amd64 /usr/local/bin/cfssljson cfssl version Installing kubectl: \u00b6 Download the kubectl binaries, add the execution permission to the binaries. wget https://storage.googleapis.com/kubernetes-release/release/v1.12.1/bin/linux/amd64/kubectl chmod +x kubectl sudo mv kubectl /usr/local/bin kubectl version Generating the TLS certificates \u00b6 Creating a certificate authority: \u00b6 Create the certificate authority and certificate authority signing request configuration file. vim ca-config.json { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } vim ca-csr.json { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"IE\", \"L\": \"Cork\", \"O\": \"Kubernetes\", \"OU\": \"CA\", \"ST\": \"Cork Co.\" } ] } Generate the certificate authority certificate and private key. cfssl gencert -initca ca-csr.json | cfssljson -bare ca ls -la Creating the certificate for the Etcd cluster: \u00b6 Create the certificate signing request configuration file. vim kubernetes-csr.json { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"IE\", \"L\": \"Cork\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes\", \"ST\": \"Cork Co.\" } ] } Generate the certificate and private key: *cfssl gencert * *-ca=ca.pem * *-ca-key=ca-key.pem * *-config=ca-config.json * *-hostname= , , , ,127.0.0.1,kubernetes.default * *-profile=kubernetes kubernetes-csr.json | * cfssljson -bare kubernetes Copy the certificate to the both Master and Worker node: sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~ Preparing the Master and Worker nodes for kubeadm \u00b6 Installing Docker: \u00b6 Add the Docker repository key and Docker repository. curl -fsSL https:// download.docker.com/linux/ubuntu/gpg | apt-key add - *add-apt-repository * *\"deb https://download.docker.com/linux/ \\((. /etc/os-release; echo \"\\) ID\") * *$(lsb_release -cs) * stable\" Update the list of packages and install the docker apt-get update apt-get install -y docker-ce echo '{\"exec-opts\": [\"native.cgroupdriver=systemd\"]}' | sudo tee /etc/docker/daemon.json systemctl daemon-reload systemctl restart docker systemctl enable docker Installing kubeadm, kublet, and kubectl: \u00b6 Add the Google repository key and Google repository curl -s https:// packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - vim /etc/apt/sources.list.d/kubernetes.list deb ** http://apt.kubernetes.io kubernetes-xenial main Update the list of packages. And install kubelet, kubeadm and kubectl. apt-get update apt-get install kubelet kubeadm kubectl -y Disable the swap. swapoff -a sed -i '/ swap / s/^/#/' /etc/fstab Installing and configuring Etcd on the Master Nodes. \u00b6 Create a configuration directory for Etcd and move the certificates into it. sudo mkdir /etc/etcd /var/lib/etcd sudo mv ~/ca.pem ~/kubernetes.pem ~/kubernetes-key.pem /etc/etcd Download and extract the etcd archive and move it to /usr/local/bin. wget https:// github.com/coreos /etcd/releases/download/v3.3.9/etcd-v3.3.9-linux-amd64.tar.gz tar xvzf etcd-v3.3.9-linux-amd64.tar.gz sudo mv etcd-v3.3.9-linux-amd64/etcd* /usr/local/bin/ Create an etcd systemd unit file. sudo vim /etc/systemd/system/etcd.service [Unit] Description=etcd Documentation=https:// github.com/coreos* [Service] *ExecStart=/usr/local/bin/etcd * *--name * *--cert-file=/etc/etcd/kubernetes.pem * *--key-file=/etc/etcd/kubernetes-key.pem * *--peer-cert-file=/etc/etcd/kubernetes.pem * *--peer-key-file=/etc/etcd/kubernetes-key.pem * *--trusted-ca-file=/etc/etcd/ca.pem * *--peer-trusted-ca-file=/etc/etcd/ca.pem * *--peer-client-cert-auth * *--client-cert-auth * *--initial-advertise-peer-urls https:// :2380 * *--listen-peer-urls https:// :2380 * *--listen-client-urls https:// :2379, http://127.0.0.1:2379 * *--advertise-client-urls https:// :2379 * *--initial-cluster-token etcd-cluster-0 * *--initial-cluster =https:// :2380, =https:// :2380, =https:// :2380 * *--initial-cluster-state new * --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target Reload the daemon configuration, Enable and start the etcd. sudo systemctl daemon-reload sudo systemctl enable etcd sudo systemctl start etcd Verify that the cluster is up and running. ETCDCTL_API=3 etcdctl member list \u00b6 Initializing the master nodes \u00b6 Initializing the master01 node: \u00b6 Create the configuration file for kubeadm. vim config.yaml sudo kubeadm init --config=config.yaml sudo scp -r /etc/kubernetes/pki opsadmin@ :~ sudo scp -r /etc/kubernetes/pki opsadmin@ :~ sudo scp -r config.yaml opsadmin@ :~ Configure Master02 and 03 Remove the apiserver.crt and apiserver.key. rm ~/pki/apiserver.* Move the certificates to the /etc/kubernetes directory. sudo mv ~/pki /etc/kubernetes/ sudo kubeadm init --config=config.yaml Initializing the worker nodes \u00b6 Copy the \"kubeadm join\" command from the lastly created master node and execute on the worker nodes. sudo kubeadm join 10.10.40.93:6443 --token [your_token] --discovery-token-ca-cert-hash sha256:[your_token_ca_cert_hash] Configuring kubectl on the client machine \u00b6 sudo chmod +r /etc/kubernetes/admin.conf scp opsadmin@ :/etc/kubernetes/admin.conf . mkdir ~/.kube mv admin.conf ~/.kube/config chmod 600 ~/.kube/config Deploying the Calico as overlay network \u00b6 kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml kubectl create -f https://docs.projectcalico.org/manifests/custom-resources.yamlwatch kubectl get pods -n calico-system Validate nodes are in Ready state. kubectl get nodes References: https://blog.inkubate.io/install-and-configure-a-multi-master-kubernetes-cluster-with-kubeadm/ Analysis: Config file error This version of kubeadm only supports deploying clusters with the control plane version >= 1.16.0. Current version: v1.12.0 Kubernetes \u00b6 https://stackoverflow.com/questions/61926846/this-version-of-kubeadm-only-supports-deploying-clusters-with-the-control-plane https://pkg.go.dev/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2 1. ## Error [kubelet-check] It seems like the kubelet isn't running or healthy. [kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz ' failed with error: Get http://localhost:10248/healthz : dial tcp 127.0.0.1:10248: connect: connection refused. Solution: Updating the /etc/docker/daemon.json file https://www.myfreax.com/how-to-solve-dial-tcp-127-0-0-1-10248-connect-connection-refused-during-kubeadm-init-initialization/ Disable swap on all the master and worker nodes. Carefully give the hostnames and IPs while configuring configuration files, generating ETCD certificates, hosts and hostname file, HA proxy config file. DNS server entry in the resolv.conf and /etc/resolvconf/resolv.conf.d/head. kubernetes node not ready but pod running. Due to wrong hostname given in HA config file and on the node. https://stackoverflow.com/questions/47107117/how-to-debug-when-kubernetes-nodes-are-in-not-ready-state https://stackoverflow.com/questions/49112336/container-runtime-network-not-ready-cni-config-uninitialized https://github.com/kubernetes/kubernetes/issues/96459","title":"Testing2"},{"location":"testing2/#share-the-ssh-public-key-of-the-bastion-host-to-the-rest-of-the-server-for-transferring-the-certificates","text":"","title":"Share the SSH public key of the Bastion host to the rest of the server for transferring the certificates."},{"location":"testing2/#_1","text":"","title":""},{"location":"testing2/#_2","text":"","title":""},{"location":"testing2/#_3","text":"","title":""},{"location":"testing2/#_4","text":"","title":""},{"location":"testing2/#installing-the-haproxy-load-balancer-on-load-balancer-host","text":"To deploy three Kubernetes master nodes, one needs to deploy an HAPRoxy load balancer in front of them to distribute the traffic. apt-get update apt-get upgrade apt-get install haproxy Add below lines in the haproxy configuration file and restart the service. Carefully update the hostnames and IPs to avoid the conflicts. If the DNS server is configured, the Load Balancer name can be defined. vim /etc/haproxy/haproxy.cfg frontend kubernetes bind LoadBalancerIP:6443 option tcplog mode tcp default_backend kubernetes-master-nodes backend kubernetes-master-nodes mode tcp balance roundrobin option tcp-check server :6443 check fall 3 rise 2 server :6443 check fall 3 rise 2 server :6443 check fall 3 rise 2 systemctl restart haproxy","title":"Installing the HAProxy load balancer On Load Balancer Host"},{"location":"testing2/#_5","text":"","title":""},{"location":"testing2/#_6","text":"","title":""},{"location":"testing2/#_7","text":"","title":""},{"location":"testing2/#_8","text":"","title":""},{"location":"testing2/#_9","text":"","title":""},{"location":"testing2/#_10","text":"","title":""},{"location":"testing2/#installing-the-client-tools-on-the-bastion-host","text":"","title":"Installing the client tools on the Bastion Host"},{"location":"testing2/#installing-cfssl","text":"Download the binaries and add the execution permission to the binaries. apt-get update wget https: / /pkg.cfssl.org/R1 . 2 /cfssl_linux-amd64* wget https: / /pkg.cfssl.org/R1 . 2 /cfssl_linux-amd64* chmod +x cfssl* mv cfssl_linux-amd64 /usr/local/bin/cfssl mv cfssljson_linux-amd64 /usr/local/bin/cfssljson cfssl version","title":"Installing cfssl:"},{"location":"testing2/#installing-kubectl","text":"Download the kubectl binaries, add the execution permission to the binaries. wget https://storage.googleapis.com/kubernetes-release/release/v1.12.1/bin/linux/amd64/kubectl chmod +x kubectl sudo mv kubectl /usr/local/bin kubectl version","title":"Installing kubectl:"},{"location":"testing2/#generating-the-tls-certificates","text":"","title":"Generating the TLS certificates"},{"location":"testing2/#creating-a-certificate-authority","text":"Create the certificate authority and certificate authority signing request configuration file. vim ca-config.json { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } vim ca-csr.json { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"IE\", \"L\": \"Cork\", \"O\": \"Kubernetes\", \"OU\": \"CA\", \"ST\": \"Cork Co.\" } ] } Generate the certificate authority certificate and private key. cfssl gencert -initca ca-csr.json | cfssljson -bare ca ls -la","title":"Creating a certificate authority:"},{"location":"testing2/#creating-the-certificate-for-the-etcd-cluster","text":"Create the certificate signing request configuration file. vim kubernetes-csr.json { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"IE\", \"L\": \"Cork\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes\", \"ST\": \"Cork Co.\" } ] } Generate the certificate and private key: *cfssl gencert * *-ca=ca.pem * *-ca-key=ca-key.pem * *-config=ca-config.json * *-hostname= , , , ,127.0.0.1,kubernetes.default * *-profile=kubernetes kubernetes-csr.json | * cfssljson -bare kubernetes Copy the certificate to the both Master and Worker node: sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~ sudo scp ca.pem kubernetes.pem kubernetes-key.pem opsadmin@ :~","title":"Creating the certificate for the Etcd cluster:"},{"location":"testing2/#preparing-the-master-and-worker-nodes-for-kubeadm","text":"","title":"Preparing the Master and Worker nodes for kubeadm"},{"location":"testing2/#installing-docker","text":"Add the Docker repository key and Docker repository. curl -fsSL https:// download.docker.com/linux/ubuntu/gpg | apt-key add - *add-apt-repository * *\"deb https://download.docker.com/linux/ \\((. /etc/os-release; echo \"\\) ID\") * *$(lsb_release -cs) * stable\" Update the list of packages and install the docker apt-get update apt-get install -y docker-ce echo '{\"exec-opts\": [\"native.cgroupdriver=systemd\"]}' | sudo tee /etc/docker/daemon.json systemctl daemon-reload systemctl restart docker systemctl enable docker","title":"Installing Docker:"},{"location":"testing2/#installing-kubeadm-kublet-and-kubectl","text":"Add the Google repository key and Google repository curl -s https:// packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - vim /etc/apt/sources.list.d/kubernetes.list deb ** http://apt.kubernetes.io kubernetes-xenial main Update the list of packages. And install kubelet, kubeadm and kubectl. apt-get update apt-get install kubelet kubeadm kubectl -y Disable the swap. swapoff -a sed -i '/ swap / s/^/#/' /etc/fstab","title":"Installing kubeadm, kublet, and kubectl:"},{"location":"testing2/#installing-and-configuring-etcd-on-the-master-nodes","text":"Create a configuration directory for Etcd and move the certificates into it. sudo mkdir /etc/etcd /var/lib/etcd sudo mv ~/ca.pem ~/kubernetes.pem ~/kubernetes-key.pem /etc/etcd Download and extract the etcd archive and move it to /usr/local/bin. wget https:// github.com/coreos /etcd/releases/download/v3.3.9/etcd-v3.3.9-linux-amd64.tar.gz tar xvzf etcd-v3.3.9-linux-amd64.tar.gz sudo mv etcd-v3.3.9-linux-amd64/etcd* /usr/local/bin/ Create an etcd systemd unit file. sudo vim /etc/systemd/system/etcd.service [Unit] Description=etcd Documentation=https:// github.com/coreos* [Service] *ExecStart=/usr/local/bin/etcd * *--name * *--cert-file=/etc/etcd/kubernetes.pem * *--key-file=/etc/etcd/kubernetes-key.pem * *--peer-cert-file=/etc/etcd/kubernetes.pem * *--peer-key-file=/etc/etcd/kubernetes-key.pem * *--trusted-ca-file=/etc/etcd/ca.pem * *--peer-trusted-ca-file=/etc/etcd/ca.pem * *--peer-client-cert-auth * *--client-cert-auth * *--initial-advertise-peer-urls https:// :2380 * *--listen-peer-urls https:// :2380 * *--listen-client-urls https:// :2379, http://127.0.0.1:2379 * *--advertise-client-urls https:// :2379 * *--initial-cluster-token etcd-cluster-0 * *--initial-cluster =https:// :2380, =https:// :2380, =https:// :2380 * *--initial-cluster-state new * --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target Reload the daemon configuration, Enable and start the etcd. sudo systemctl daemon-reload sudo systemctl enable etcd sudo systemctl start etcd Verify that the cluster is up and running. ETCDCTL_API=3 etcdctl member list","title":"Installing and configuring Etcd on the Master Nodes."},{"location":"testing2/#_11","text":"","title":""},{"location":"testing2/#initializing-the-master-nodes","text":"","title":"Initializing the master nodes"},{"location":"testing2/#initializing-the-master01-node","text":"Create the configuration file for kubeadm. vim config.yaml sudo kubeadm init --config=config.yaml sudo scp -r /etc/kubernetes/pki opsadmin@ :~ sudo scp -r /etc/kubernetes/pki opsadmin@ :~ sudo scp -r config.yaml opsadmin@ :~ Configure Master02 and 03 Remove the apiserver.crt and apiserver.key. rm ~/pki/apiserver.* Move the certificates to the /etc/kubernetes directory. sudo mv ~/pki /etc/kubernetes/ sudo kubeadm init --config=config.yaml","title":"Initializing the master01 node:"},{"location":"testing2/#initializing-the-worker-nodes","text":"Copy the \"kubeadm join\" command from the lastly created master node and execute on the worker nodes. sudo kubeadm join 10.10.40.93:6443 --token [your_token] --discovery-token-ca-cert-hash sha256:[your_token_ca_cert_hash]","title":"Initializing the worker nodes"},{"location":"testing2/#configuring-kubectl-on-the-client-machine","text":"sudo chmod +r /etc/kubernetes/admin.conf scp opsadmin@ :/etc/kubernetes/admin.conf . mkdir ~/.kube mv admin.conf ~/.kube/config chmod 600 ~/.kube/config","title":"Configuring kubectl on the client machine"},{"location":"testing2/#deploying-the-calico-as-overlay-network","text":"kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml kubectl create -f https://docs.projectcalico.org/manifests/custom-resources.yamlwatch kubectl get pods -n calico-system Validate nodes are in Ready state. kubectl get nodes References: https://blog.inkubate.io/install-and-configure-a-multi-master-kubernetes-cluster-with-kubeadm/ Analysis: Config file error","title":"Deploying the Calico as overlay network"},{"location":"testing2/#this-version-of-kubeadm-only-supports-deploying-clusters-with-the-control-plane-version-1160-current-version-v1120-kubernetes","text":"https://stackoverflow.com/questions/61926846/this-version-of-kubeadm-only-supports-deploying-clusters-with-the-control-plane https://pkg.go.dev/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2 1. ## Error [kubelet-check] It seems like the kubelet isn't running or healthy. [kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz ' failed with error: Get http://localhost:10248/healthz : dial tcp 127.0.0.1:10248: connect: connection refused. Solution: Updating the /etc/docker/daemon.json file https://www.myfreax.com/how-to-solve-dial-tcp-127-0-0-1-10248-connect-connection-refused-during-kubeadm-init-initialization/ Disable swap on all the master and worker nodes. Carefully give the hostnames and IPs while configuring configuration files, generating ETCD certificates, hosts and hostname file, HA proxy config file. DNS server entry in the resolv.conf and /etc/resolvconf/resolv.conf.d/head. kubernetes node not ready but pod running. Due to wrong hostname given in HA config file and on the node. https://stackoverflow.com/questions/47107117/how-to-debug-when-kubernetes-nodes-are-in-not-ready-state https://stackoverflow.com/questions/49112336/container-runtime-network-not-ready-cni-config-uninitialized https://github.com/kubernetes/kubernetes/issues/96459","title":"This version of kubeadm only supports deploying clusters with the control plane version &gt;= 1.16.0. Current version: v1.12.0 Kubernetes"}]}