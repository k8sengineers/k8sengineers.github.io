{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Decoding Kubernetes \u00b6","title":"Home"},{"location":"#decoding-kubernetes","text":"","title":"Decoding Kubernetes"},{"location":"about/","text":"What is CoreDNS? \u00b6 CoreDNS is a DNS server. It is written in Go. CoreDNS is different from other DNS servers, such as (all excellent) BIND, Knot, PowerDNS and Unbound (technically a resolver, but still worth a mention), because it is very flexible, and almost all functionality is outsourced into plugins. Plugins can be stand-alone or work together to perform a \u201cDNS function\u201d. So what\u2019s a \u201cDNS function\u201d? For the purpose of CoreDNS, we define it as a piece of software that implements the CoreDNS Plugin API. The functionality implemented can wildly deviate. There are plugins that don\u2019t themselves create a response, such as metrics or cache, but that add functionality. Then there are plugins that do generate a response. These can also do anything: There are plugins that communicate with Kubernetes to provide service discovery, plugins that read data from a file or a database. There are currently about 30 plugins included in the default CoreDNS install, but there are also a whole bunch of external plugins that you can compile into CoreDNS to extend its functionality. CoreDNS is powered by plugins. Writing new plugins should be easy enough, but requires knowing Go and having some insight into how DNS works. CoreDNS abstracts away a lot of DNS details so that you can just focus on writing the plugin functionality you need.","title":"About"},{"location":"about/#what-is-coredns","text":"CoreDNS is a DNS server. It is written in Go. CoreDNS is different from other DNS servers, such as (all excellent) BIND, Knot, PowerDNS and Unbound (technically a resolver, but still worth a mention), because it is very flexible, and almost all functionality is outsourced into plugins. Plugins can be stand-alone or work together to perform a \u201cDNS function\u201d. So what\u2019s a \u201cDNS function\u201d? For the purpose of CoreDNS, we define it as a piece of software that implements the CoreDNS Plugin API. The functionality implemented can wildly deviate. There are plugins that don\u2019t themselves create a response, such as metrics or cache, but that add functionality. Then there are plugins that do generate a response. These can also do anything: There are plugins that communicate with Kubernetes to provide service discovery, plugins that read data from a file or a database. There are currently about 30 plugins included in the default CoreDNS install, but there are also a whole bunch of external plugins that you can compile into CoreDNS to extend its functionality. CoreDNS is powered by plugins. Writing new plugins should be easy enough, but requires knowing Go and having some insight into how DNS works. CoreDNS abstracts away a lot of DNS details so that you can just focus on writing the plugin functionality you need.","title":"What is CoreDNS?"},{"location":"dhcp/","text":"What is DHCP? \u00b6 The Dynamic Host Configuration Protocol (DHCP) is a network service that enables host computers to be automatically assigned settings from a server as opposed to manually configuring each network host. Computers configured to be DHCP clients have no control over the settings they receive from the DHCP server, and the configuration is transparent to the computer\u2019s user. Our Requirements Configure DHCP on a standalone server with a single NIC. Setup: \u00b6 Configuring VLAN: \u00b6 Add VLAN 700 to the eth1 device. ip link add link eth1 name eth4 address 00 :11:22:33:44:55 type vlan id 700 It is not necessary, but you can disable IPv6 on this particular VLAN interface. sysctl -w net.ipv6.conf.eth1/700.disable_ipv6 = 1 net.ipv6.conf.eth1/700.disable_ipv6 = 1 Add an IPv4 address. ip addr add 10 .100.10.77/24 dev eth1.700 Bring VLAN interface up. ip link set dev eth4 up ip -detail addr show eth4 eth4 : < BROADCAST , MULTICAST , UP , LOWER_UP > mtu 1500 qdisc noqueue state UP group default qlen 1000 link / ether 08 : 00 : 27 : fa : 4 b : 19 brd ff : ff : ff : ff : ff : ff promiscuity 0 minmtu 0 maxmtu 65535 vlan protocol 802.1 Q id 700 < REORDER_HDR > numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 inet 10.100.10.77 / 24 scope global eth1 .700 To make VLAN 700 on the eth1 interface at the boot time just add the following configuration. vi /etc/network/interfaces # add vlan 700 on eth1 - static IP address auto eth4 iface eth4 inet static address 192.168.10.2 netmask 255.255.255.0 pre - up sysctl - w net . ipv6 . conf . eth1 / 700. disable_ipv6 = 1 These interfaces will be brought up in the order in which they were listed. Configuring DHCP server: \u00b6 First, update the system repository index in your system and install the DHCP server apt update && apt-get install isc-dhcp-server -y Specify the interface on which the DHCP server will listen to. vi /etc/default/isc-dhcp-server INTERFACESv4 = \"eth4\" Add below the lines in the DHCP configuration file. vi /etc/dhcp/dhcpd.conf authoritative ; option domain - name \"groophy.org\" ; option domain - name - servers 192.168.0.100 ; default - lease - time 600 ; max - lease - time 7200 ; ddns - update - style none ; subnet 192.168.10.0 netmask 255.255.255.0 { range 192.168.10.11 192.168.10.240 ; option routers 192.168.10.254 ; option subnet - mask 255.255.255.0 ; option broadcast - address 192.168.10.255 ; } Enable and restart the DHCP server service. Issue the below command in Terminal to do so: systemctl restart isc-dhcp-server.service systemctl status isc-dhcp-server.service DHCP Client Configuration \u00b6 Find your network interface name and update the network interface in the configuration. ifconfig vi /etc/network/interfaces auto ens33 iface ens33 inet dhcp Restart the network-manager service and verify the new IP. systemctl restart network-manager.service ifconfig","title":"DHCP"},{"location":"dhcp/#what-is-dhcp","text":"The Dynamic Host Configuration Protocol (DHCP) is a network service that enables host computers to be automatically assigned settings from a server as opposed to manually configuring each network host. Computers configured to be DHCP clients have no control over the settings they receive from the DHCP server, and the configuration is transparent to the computer\u2019s user. Our Requirements Configure DHCP on a standalone server with a single NIC.","title":"What is DHCP?"},{"location":"dhcp/#setup","text":"","title":"Setup:"},{"location":"dhcp/#configuring-vlan","text":"Add VLAN 700 to the eth1 device. ip link add link eth1 name eth4 address 00 :11:22:33:44:55 type vlan id 700 It is not necessary, but you can disable IPv6 on this particular VLAN interface. sysctl -w net.ipv6.conf.eth1/700.disable_ipv6 = 1 net.ipv6.conf.eth1/700.disable_ipv6 = 1 Add an IPv4 address. ip addr add 10 .100.10.77/24 dev eth1.700 Bring VLAN interface up. ip link set dev eth4 up ip -detail addr show eth4 eth4 : < BROADCAST , MULTICAST , UP , LOWER_UP > mtu 1500 qdisc noqueue state UP group default qlen 1000 link / ether 08 : 00 : 27 : fa : 4 b : 19 brd ff : ff : ff : ff : ff : ff promiscuity 0 minmtu 0 maxmtu 65535 vlan protocol 802.1 Q id 700 < REORDER_HDR > numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 inet 10.100.10.77 / 24 scope global eth1 .700 To make VLAN 700 on the eth1 interface at the boot time just add the following configuration. vi /etc/network/interfaces # add vlan 700 on eth1 - static IP address auto eth4 iface eth4 inet static address 192.168.10.2 netmask 255.255.255.0 pre - up sysctl - w net . ipv6 . conf . eth1 / 700. disable_ipv6 = 1 These interfaces will be brought up in the order in which they were listed.","title":"Configuring VLAN:"},{"location":"dhcp/#configuring-dhcp-server","text":"First, update the system repository index in your system and install the DHCP server apt update && apt-get install isc-dhcp-server -y Specify the interface on which the DHCP server will listen to. vi /etc/default/isc-dhcp-server INTERFACESv4 = \"eth4\" Add below the lines in the DHCP configuration file. vi /etc/dhcp/dhcpd.conf authoritative ; option domain - name \"groophy.org\" ; option domain - name - servers 192.168.0.100 ; default - lease - time 600 ; max - lease - time 7200 ; ddns - update - style none ; subnet 192.168.10.0 netmask 255.255.255.0 { range 192.168.10.11 192.168.10.240 ; option routers 192.168.10.254 ; option subnet - mask 255.255.255.0 ; option broadcast - address 192.168.10.255 ; } Enable and restart the DHCP server service. Issue the below command in Terminal to do so: systemctl restart isc-dhcp-server.service systemctl status isc-dhcp-server.service","title":"Configuring DHCP server:"},{"location":"dhcp/#dhcp-client-configuration","text":"Find your network interface name and update the network interface in the configuration. ifconfig vi /etc/network/interfaces auto ens33 iface ens33 inet dhcp Restart the network-manager service and verify the new IP. systemctl restart network-manager.service ifconfig","title":"DHCP Client Configuration"},{"location":"getting_started/","text":"Our Requirements \u00b6 Requirements Configure CoreDNS with the domain name \u201cgroophy.org\". Each server can be resolve by any server in the cluster for that solution we are using CoreDNS with the host file.","title":"Getting Started"},{"location":"getting_started/#our-requirements","text":"Requirements Configure CoreDNS with the domain name \u201cgroophy.org\". Each server can be resolve by any server in the cluster for that solution we are using CoreDNS with the host file.","title":"Our Requirements"},{"location":"guide/","text":"Getting started \u00b6 Material for MkDocs is a theme for MkDocs , a static site generator geared towards (technical) project documentation. If you're familiar with Python, you can install Material for MkDocs with pip , the Python package manager. If not, we recommended using docker . Installation \u00b6 with pip recommended \u00b6 Material for MkDocs can be installed with pip : pip install mkdocs-material This will automatically install compatible versions of all dependencies: MkDocs , Markdown , Pygments and Python Markdown Extensions . Material for MkDocs always strives to support the latest versions, so there's no need to install those packages separately. Docker \u00b6 with docker \u00b6 The official Docker image is a great way to get up and running in a few minutes, as it comes with all dependencies pre-installed. Pull the image for the latest version with: docker pull squidfunk/mkdocs-material The mkdocs executable is provided as an entry point and serve is the default command. If you're not familiar with Docker don't worry, we have you covered in the following sections. The following plugins are bundled with the Docker image: mkdocs-minify-plugin mkdocs-redirects How to add plugins to the Docker image? Material for MkDocs bundles useful and common plugins while trying not to blow up the size of the official image. If the plugin you want to use is not included, create a new Dockerfile and extend the official Docker image with your custom installation routine: FROM squidfunk/mkdocs-material RUN pip install ... Next, you can build the image with the following command: docker build -t squidfunk/mkdocs-material . The new image can be used exactly like the official image. Apple Silicon (M1) and Raspberry Pi The official Docker image is only available for linux/amd64 . We recommend the third-party image by @afritzler if you want to run Material for MkDocs via Docker on arm64 or armv7 , as it is automatically built on every release: docker pull ghcr.io/afritzler/mkdocs-material GIT \u00b6 with git \u00b6 Material for MkDocs can be directly used from GitHub by cloning the repository into a subfolder of your project root which might be useful if you want to use the very latest version: git clone https://github.com/squidfunk/mkdocs-material.git The theme will reside in the folder mkdocs-material/material . When cloning from git , you must install all required dependencies yourself: pip install -e mkdocs-material","title":"Getting started"},{"location":"guide/#getting-started","text":"Material for MkDocs is a theme for MkDocs , a static site generator geared towards (technical) project documentation. If you're familiar with Python, you can install Material for MkDocs with pip , the Python package manager. If not, we recommended using docker .","title":"Getting started"},{"location":"guide/#installation","text":"","title":"Installation"},{"location":"guide/#with-pip","text":"Material for MkDocs can be installed with pip : pip install mkdocs-material This will automatically install compatible versions of all dependencies: MkDocs , Markdown , Pygments and Python Markdown Extensions . Material for MkDocs always strives to support the latest versions, so there's no need to install those packages separately.","title":"with pip"},{"location":"guide/#docker","text":"","title":"Docker"},{"location":"guide/#with-docker","text":"The official Docker image is a great way to get up and running in a few minutes, as it comes with all dependencies pre-installed. Pull the image for the latest version with: docker pull squidfunk/mkdocs-material The mkdocs executable is provided as an entry point and serve is the default command. If you're not familiar with Docker don't worry, we have you covered in the following sections. The following plugins are bundled with the Docker image: mkdocs-minify-plugin mkdocs-redirects How to add plugins to the Docker image? Material for MkDocs bundles useful and common plugins while trying not to blow up the size of the official image. If the plugin you want to use is not included, create a new Dockerfile and extend the official Docker image with your custom installation routine: FROM squidfunk/mkdocs-material RUN pip install ... Next, you can build the image with the following command: docker build -t squidfunk/mkdocs-material . The new image can be used exactly like the official image. Apple Silicon (M1) and Raspberry Pi The official Docker image is only available for linux/amd64 . We recommend the third-party image by @afritzler if you want to run Material for MkDocs via Docker on arm64 or armv7 , as it is automatically built on every release: docker pull ghcr.io/afritzler/mkdocs-material","title":"with docker"},{"location":"guide/#git","text":"","title":"GIT"},{"location":"guide/#with-git","text":"Material for MkDocs can be directly used from GitHub by cloning the repository into a subfolder of your project root which might be useful if you want to use the very latest version: git clone https://github.com/squidfunk/mkdocs-material.git The theme will reside in the folder mkdocs-material/material . When cloning from git , you must install all required dependencies yourself: pip install -e mkdocs-material","title":"with git"},{"location":"k8s_backup_restore/","text":"Kubernetes Setup With Backup and Restiore Solution \u00b6 Cluster Build Tool -Kubespray Object Storage -Minio Backup Solution -Velero For this activity, deploy 5 AWS instances with the Security groups All traffic is enabled between the instances All traffic enabled for the testing machine(Our Laptop) Node Name Instance Details Ansible Node -t2.micro Control Plane (2-Nodes) -t2.medium Worker (2-Nodes) -t2.medium Download the pem-key and transfer the pem-key to ansible node. scp -i <pemkey> <pemkey> ubuntu@3.15.192.170:/home/ubuntu/ cp /home/ubuntu/<pemkey> . chmod 400 <pemkey> vi /etc/hosts 172.31.27.136 cruise . org Configure HA proxy to build a Multi-master Kubernetes cluster: \u00b6 Download and install the haproxy apt-get update && apt-get install haproxy -y Update haproxy configuration with as below with the details of the Ansible and Master nodes IP vi /etc/haproxy/haproxy.cfg listen kubernetes - apiserver - https bind ansiblenodeIP : 8383 mode tcp option log - health - checks timeout client 3 h timeout server 3 h server master1 < IP1 >: 6443 check check - ssl verify none inter 10000 server master2 < IP2 >: 6443 check check - ssl verify none inter 10000 balance roundrobin systemctl restart haproxy netstat -atnlp Deploying Multi-master Kubernetes cluster with Kubespray: \u00b6 Install Ansible and PIP apt update && apt install software-properties-common -y add-apt-repository --yes --update ppa:ansible/ansible apt install ansible -y ansible --version apt install python3-pip -y Clone the Kubespray source code and install requirements through Pip. git clone https://github.com/kubernetes-sigs/kubespray.git cd kubespray/ pip3 install -r requirements.txt cp -rfp inventory/sample inventory/mycluster declare -a IPS =( <master01_IP> <master02_IP> <Worker01_IP> <Worker02_IP> ) CONFIG_FILE = inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${ IPS [@] } Update the HA-Proxy server details \u00b6 vi inventory/mycluster/group_vars/all/all.yml ## External LB example config apiserver_loadbalancer_domain_name : \"cruise.org\" loadbalancer_apiserver : address : 172.31.27.136 port : 8383 ## Internal loadbalancers for apiservers loadbalancer_apiserver_localhost : false Enable Kubernetes dashboard by updating the below files vi inventory/sample/group_vars/k8s_cluster/addons.yml vi inventory/mycluster/group_vars/k8s_cluster/addons.yml # Kubernetes dashboard # RBAC required. see docs/getting-started.md for access details. dashboard_enabled : true Deploy cluster with below Ansible command. ansible all -m ping -i inventory/mycluster/hosts.yaml --user ubuntu --private-key ../kmvelero.pem ansible-playbook -i inventory/mycluster/hosts.yaml --user ubuntu --private-key kmvelero.pem --become cluster.yml On Master Node: kubectl get nodes Output NAME STATUS ROLES AGE VERSION node1 Ready control - plane , master 5 h14m v1 .22.3 node2 Ready control - plane , master 5 h14m v1 .22.3 node3 Ready < none > 5 h13m v1 .22.3 node4 Ready < none > 5 h13m v1 .22.3 Configure Minio on the Ansible Node: \u00b6 wget https://dl.min.io/server/minio/release/linux-amd64/minio chmod +x minio ll /usr/bin/minio export MINIO_ROOT_USER = <Username> export MINIO_ROOT_PASSWORD = <Password> nohup minio server --console-address <AnsibleNodeIP>:<Port> /data > /dev/null 2 > & 1 & Manually create the bucket by accessing the Minio Console. Configure Velero: \u00b6 Switch to any master and download the velero source code and configure it. Kubernetes Master Node01 ssh -i kmvelero.pem ubuntu@172.31.16.30 wget https://github.com/vmware-tanzu/velero/releases/download/v1.7.0/velero-v1.7.0-linux-amd64.tar.gz tar -xvzf velero-v1.7.0-linux-amd64.tar.gz mv velero-v1.7.0-linux-amd64/velero /usr/local/bin/ wget https://github.com/digitalocean/velero-plugin/archive/refs/tags/v1.0.0.tar.gz nohup tar -xvzf v1.0.0.tar.gz source < ( velero completion bash ) Provide the username and password of the Minio console. cp velero-plugin-1.0.0/examples/cloud-credentials cloud-credentials vi cloud-credentials [default] aws_access_key_id = <MinioUsername> aws_secret_access_key = <MinioPassword> Initiate Velero to store the backup in the Minio buckets. velero install --provider aws --plugins velero/velero-plugin-for-aws:v1.0.0 --bucket <bucketname created in the minio console> -- backup-location-configregion = minio,s3ForcePathStyle = true,s3Url = http://ansible_host ( minio ) :9000 --secret-file <CredentialFile> Validate the access: velero get backup-location output NAME PROVIDER BUCKET / PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT default aws kmvbucket Available 2021-11-09 12 : 08 : 44 + 0000 UTC ReadWrite true Copy the kubeconfig file to the Ansible node. cp /etc/kubernetes/admin.conf /home/ubuntu/ chmod 755 /home/ubuntu/admin.conf scp -i kmvelero.pem ubuntu@172.31.16.30:/home/ubuntu/admin.conf kubeconfig Configure Velero on the Ansible Node to access it from out of the cluster: wget https://github.com/vmware-tanzu/velero/releases/download/v1.7.0/velero-v1.7.0-linux-amd64.tar.gz nohup tar -xvzf velero-v1.7.0-linux-amd64.tar.gz mv velero-v1.7.0-linux-amd64/velero /usr/local/bin/ wget https://github.com/digitalocean/velero-plugin/archive/refs/tags/v1.0.0.tar.gz nohup tar -xvzf v1.0.0.tar.gz cp velero-plugin-1.0.0/examples/cloud-credentials cloud-credentials vi cloud-credentials source < ( velero completion bash ) vi /etc/hosts Validate the access from the Ansible node: \u00b6 velero get backup-locations --kubeconfig kubeconfig Output NAME PROVIDER BUCKET / PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT default aws kmvbucket Available 2021-11-09 12 : 08 : 44 + 0000 UTC ReadWrite true Backup & Restore: \u00b6 Backup: \u00b6 Creating a namespace and deployment in it. \u00b6 On Master01: \u00b6 kubectl create ns testing kubectl -n testing apply -f nginx.yaml ### Deploying 4 nginx pods On Ansible node: \u00b6 velero backup create firstbackup --include-namespaces testing --kubeconfig kubeconfig velero get backup --kubeconfig kubeconfig output NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR firstbackup Completed 0 0 2021-11-09 12 : 11 : 30 + 0000 UTC 29 d default < none > Restore: \u00b6 Deleting the Namespace Testing to perform restore On Master01: \u00b6 kubectl delete namespace testing On Ansible node: \u00b6 velero restore create firstbackup-restore --from-backup firstbackup --kubeconfig kubeconfig velero get restores --kubeconfig kubeconfig Output NAME BACKUP STATUS STARTED COMPLETED ERRORS WARNINGS CREATED SELECTOR firstbackup - restore firstbackup Completed 2021-11-08 11 : 20 : 35 + 0000 UTC 2021-11-08 11 : 20 : 36 + 0000 UTC 0 0 2021-11-08 11 : 20 : 35 + 0000 UTC < none > On Master01: \u00b6 kubectl get deployments.apps -n testing Output NAME READY UP - TO - DATE AVAILABLE AGE nginx - deployment 4 / 4 4 4 155 m Scheduling backup: \u00b6 velero create schedule initbackup1 --schedule = \"13 12 * * *\" --kubeconfig kubeconfig --include-namespaces testing velero get schedules --kubeconfig kubeconfig Note Velero commands can also be executed from the Master node.","title":"Kubernetes Setup With Backup and Restore Solution"},{"location":"k8s_backup_restore/#kubernetes-setup-with-backup-and-restiore-solution","text":"Cluster Build Tool -Kubespray Object Storage -Minio Backup Solution -Velero For this activity, deploy 5 AWS instances with the Security groups All traffic is enabled between the instances All traffic enabled for the testing machine(Our Laptop) Node Name Instance Details Ansible Node -t2.micro Control Plane (2-Nodes) -t2.medium Worker (2-Nodes) -t2.medium Download the pem-key and transfer the pem-key to ansible node. scp -i <pemkey> <pemkey> ubuntu@3.15.192.170:/home/ubuntu/ cp /home/ubuntu/<pemkey> . chmod 400 <pemkey> vi /etc/hosts 172.31.27.136 cruise . org","title":"Kubernetes Setup With Backup and Restiore Solution"},{"location":"k8s_backup_restore/#configure-ha-proxy-to-build-a-multi-master-kubernetes-cluster","text":"Download and install the haproxy apt-get update && apt-get install haproxy -y Update haproxy configuration with as below with the details of the Ansible and Master nodes IP vi /etc/haproxy/haproxy.cfg listen kubernetes - apiserver - https bind ansiblenodeIP : 8383 mode tcp option log - health - checks timeout client 3 h timeout server 3 h server master1 < IP1 >: 6443 check check - ssl verify none inter 10000 server master2 < IP2 >: 6443 check check - ssl verify none inter 10000 balance roundrobin systemctl restart haproxy netstat -atnlp","title":"Configure HA proxy to build a Multi-master Kubernetes cluster:"},{"location":"k8s_backup_restore/#deploying-multi-master-kubernetes-cluster-with-kubespray","text":"Install Ansible and PIP apt update && apt install software-properties-common -y add-apt-repository --yes --update ppa:ansible/ansible apt install ansible -y ansible --version apt install python3-pip -y Clone the Kubespray source code and install requirements through Pip. git clone https://github.com/kubernetes-sigs/kubespray.git cd kubespray/ pip3 install -r requirements.txt cp -rfp inventory/sample inventory/mycluster declare -a IPS =( <master01_IP> <master02_IP> <Worker01_IP> <Worker02_IP> ) CONFIG_FILE = inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${ IPS [@] }","title":"Deploying Multi-master Kubernetes cluster with Kubespray:"},{"location":"k8s_backup_restore/#update-the-ha-proxy-server-details","text":"vi inventory/mycluster/group_vars/all/all.yml ## External LB example config apiserver_loadbalancer_domain_name : \"cruise.org\" loadbalancer_apiserver : address : 172.31.27.136 port : 8383 ## Internal loadbalancers for apiservers loadbalancer_apiserver_localhost : false Enable Kubernetes dashboard by updating the below files vi inventory/sample/group_vars/k8s_cluster/addons.yml vi inventory/mycluster/group_vars/k8s_cluster/addons.yml # Kubernetes dashboard # RBAC required. see docs/getting-started.md for access details. dashboard_enabled : true Deploy cluster with below Ansible command. ansible all -m ping -i inventory/mycluster/hosts.yaml --user ubuntu --private-key ../kmvelero.pem ansible-playbook -i inventory/mycluster/hosts.yaml --user ubuntu --private-key kmvelero.pem --become cluster.yml On Master Node: kubectl get nodes Output NAME STATUS ROLES AGE VERSION node1 Ready control - plane , master 5 h14m v1 .22.3 node2 Ready control - plane , master 5 h14m v1 .22.3 node3 Ready < none > 5 h13m v1 .22.3 node4 Ready < none > 5 h13m v1 .22.3","title":"Update the HA-Proxy server details"},{"location":"k8s_backup_restore/#configure-minio-on-the-ansible-node","text":"wget https://dl.min.io/server/minio/release/linux-amd64/minio chmod +x minio ll /usr/bin/minio export MINIO_ROOT_USER = <Username> export MINIO_ROOT_PASSWORD = <Password> nohup minio server --console-address <AnsibleNodeIP>:<Port> /data > /dev/null 2 > & 1 & Manually create the bucket by accessing the Minio Console.","title":"Configure Minio on the Ansible Node:"},{"location":"k8s_backup_restore/#configure-velero","text":"Switch to any master and download the velero source code and configure it. Kubernetes Master Node01 ssh -i kmvelero.pem ubuntu@172.31.16.30 wget https://github.com/vmware-tanzu/velero/releases/download/v1.7.0/velero-v1.7.0-linux-amd64.tar.gz tar -xvzf velero-v1.7.0-linux-amd64.tar.gz mv velero-v1.7.0-linux-amd64/velero /usr/local/bin/ wget https://github.com/digitalocean/velero-plugin/archive/refs/tags/v1.0.0.tar.gz nohup tar -xvzf v1.0.0.tar.gz source < ( velero completion bash ) Provide the username and password of the Minio console. cp velero-plugin-1.0.0/examples/cloud-credentials cloud-credentials vi cloud-credentials [default] aws_access_key_id = <MinioUsername> aws_secret_access_key = <MinioPassword> Initiate Velero to store the backup in the Minio buckets. velero install --provider aws --plugins velero/velero-plugin-for-aws:v1.0.0 --bucket <bucketname created in the minio console> -- backup-location-configregion = minio,s3ForcePathStyle = true,s3Url = http://ansible_host ( minio ) :9000 --secret-file <CredentialFile> Validate the access: velero get backup-location output NAME PROVIDER BUCKET / PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT default aws kmvbucket Available 2021-11-09 12 : 08 : 44 + 0000 UTC ReadWrite true Copy the kubeconfig file to the Ansible node. cp /etc/kubernetes/admin.conf /home/ubuntu/ chmod 755 /home/ubuntu/admin.conf scp -i kmvelero.pem ubuntu@172.31.16.30:/home/ubuntu/admin.conf kubeconfig Configure Velero on the Ansible Node to access it from out of the cluster: wget https://github.com/vmware-tanzu/velero/releases/download/v1.7.0/velero-v1.7.0-linux-amd64.tar.gz nohup tar -xvzf velero-v1.7.0-linux-amd64.tar.gz mv velero-v1.7.0-linux-amd64/velero /usr/local/bin/ wget https://github.com/digitalocean/velero-plugin/archive/refs/tags/v1.0.0.tar.gz nohup tar -xvzf v1.0.0.tar.gz cp velero-plugin-1.0.0/examples/cloud-credentials cloud-credentials vi cloud-credentials source < ( velero completion bash ) vi /etc/hosts","title":"Configure Velero:"},{"location":"k8s_backup_restore/#validate-the-access-from-the-ansible-node","text":"velero get backup-locations --kubeconfig kubeconfig Output NAME PROVIDER BUCKET / PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT default aws kmvbucket Available 2021-11-09 12 : 08 : 44 + 0000 UTC ReadWrite true","title":"Validate the access from the Ansible node:"},{"location":"k8s_backup_restore/#backup-restore","text":"","title":"Backup &amp; Restore: "},{"location":"k8s_backup_restore/#backup","text":"","title":"Backup:"},{"location":"k8s_backup_restore/#creating-a-namespace-and-deployment-in-it","text":"","title":"Creating a namespace and deployment in it."},{"location":"k8s_backup_restore/#on-master01","text":"kubectl create ns testing kubectl -n testing apply -f nginx.yaml ### Deploying 4 nginx pods","title":"On Master01:"},{"location":"k8s_backup_restore/#on-ansible-node","text":"velero backup create firstbackup --include-namespaces testing --kubeconfig kubeconfig velero get backup --kubeconfig kubeconfig output NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR firstbackup Completed 0 0 2021-11-09 12 : 11 : 30 + 0000 UTC 29 d default < none >","title":"On Ansible node:"},{"location":"k8s_backup_restore/#restore","text":"Deleting the Namespace Testing to perform restore","title":"Restore:"},{"location":"k8s_backup_restore/#on-master01_1","text":"kubectl delete namespace testing","title":"On Master01:"},{"location":"k8s_backup_restore/#on-ansible-node_1","text":"velero restore create firstbackup-restore --from-backup firstbackup --kubeconfig kubeconfig velero get restores --kubeconfig kubeconfig Output NAME BACKUP STATUS STARTED COMPLETED ERRORS WARNINGS CREATED SELECTOR firstbackup - restore firstbackup Completed 2021-11-08 11 : 20 : 35 + 0000 UTC 2021-11-08 11 : 20 : 36 + 0000 UTC 0 0 2021-11-08 11 : 20 : 35 + 0000 UTC < none >","title":"On Ansible node:"},{"location":"k8s_backup_restore/#on-master01_2","text":"kubectl get deployments.apps -n testing Output NAME READY UP - TO - DATE AVAILABLE AGE nginx - deployment 4 / 4 4 4 155 m","title":"On Master01:"},{"location":"k8s_backup_restore/#scheduling-backup","text":"velero create schedule initbackup1 --schedule = \"13 12 * * *\" --kubeconfig kubeconfig --include-namespaces testing velero get schedules --kubeconfig kubeconfig Note Velero commands can also be executed from the Master node.","title":"Scheduling backup:"},{"location":"setupv1/","text":"Setup \u00b6 Configure the host file with the hostname and IPs. \u00b6 cat /etc/hosts 192.168.0.100 dnsserver . cruise . com 192.168.135.21 test01 . cruise . com 192.168.0.100 dnsserver 127.0.0.1 localhost Download CoreDNS from the website, unpack the binary to /usr/local/bin and make it executable. \u00b6 wget https://github.com/coredns/coredns/releases/download/v1.8.6/coredns_1.8.6_linux_amd64.tgz tar -xvzf coredns_1.8.6_linux_amd64.tgz cp coredns /usr/local/bin/ sudo chmod +x /usr/local/bin/coredns ll /usr/local/bin/coredns Install resolvconf as a tool to manually manage /etc/resolv.conf . \u00b6 apt install resolvconf Set dns as default in /etc/NetworkManager/NetworkManager.conf . \u00b6 vi /etc/NetworkManager/NetworkManager.conf dns=default Add nameserver 127.0.0.1 to /etc/resolvconf/resolv.conf.d/head . \u00b6 vi /etc/resolvconf/resolv.conf.d/head nameserver 127.0.0.1 Create /etc/coredns/Corefile and paste the configuration shown below. \u00b6 vi /etc/coredns/Corefile groophy.org:53 { forward . tls://2606:4700:4700::1111 tls://1.1.1.1 hosts /etc/hosts log errors Cache } .:53 { forward . tls://2606:4700:4700::1111 tls://1.1.1.1 log errors Cache } We are using Cloudflare as a DNS provider(1.1.1.1). \u00b6 Create a new user for CoreDNS and set some permissions on the /opt/coredns directory. \u00b6 sudo useradd -d /var/lib/coredns -m coredns sudo chown coredns:coredns /opt/coredns Download the SystemD service unit file from coredns to /etc/systemd/system/coredns.service . \u00b6 wget https://github.com/coredns/deployment/blob/master/systemd/coredns.service mv coredns.service /etc/systemd/system/ Disable SystemD's default DNS server. \u00b6 sudo systemctl stop systemd-resolved && sudo systemctl disable systemd-resolved Note From that moment on, you will not be able to resolve any web pages anymore, unless you enable DNS again Enable and start CoreDNS \u00b6 sudo systemctl enable coredns && sudo systemctl start coredns Now we can able to resolve domain names, again. E.g. try to dig +short kit.edu . If an IP address is printed, everything works fine. \u00b6 dig +short kit.edu Output \u00b6 141.3.128.6 nslookup dnsserver.groophy.org Output \u00b6 Server : 192.168.0.100 Address : 192.168.0.100 # 53 Name : dnsserver . cruise . com Address : 192.168.0.100 nslookup google.com Output \u00b6 Server : 192.168.0.100 Address : 192.168.0.100 # 53 Non - authoritative answer : Name : google . com Address : 142.250.71.14 Name : google . com Address : 2404 : 6800 : 4007 : 824 :: 200 e Conclusion Able to resolve both internal servers and outside the world.","title":"Setupv1"},{"location":"setupv1/#setup","text":"","title":"Setup"},{"location":"setupv1/#configure-the-host-file-with-the-hostname-and-ips","text":"cat /etc/hosts 192.168.0.100 dnsserver . cruise . com 192.168.135.21 test01 . cruise . com 192.168.0.100 dnsserver 127.0.0.1 localhost","title":"Configure the host file with the hostname and IPs."},{"location":"setupv1/#download-coredns-from-the-website-unpack-the-binary-to-usrlocalbin-and-make-it-executable","text":"wget https://github.com/coredns/coredns/releases/download/v1.8.6/coredns_1.8.6_linux_amd64.tgz tar -xvzf coredns_1.8.6_linux_amd64.tgz cp coredns /usr/local/bin/ sudo chmod +x /usr/local/bin/coredns ll /usr/local/bin/coredns","title":"Download CoreDNS from the website, unpack the binary to /usr/local/bin and make it executable."},{"location":"setupv1/#install-resolvconf-as-a-tool-to-manually-manage-etcresolvconf","text":"apt install resolvconf","title":"Install resolvconf as a tool to manually manage /etc/resolv.conf."},{"location":"setupv1/#set-dns-as-default-in-etcnetworkmanagernetworkmanagerconf","text":"vi /etc/NetworkManager/NetworkManager.conf dns=default","title":"Set dns as default in /etc/NetworkManager/NetworkManager.conf."},{"location":"setupv1/#add-nameserver-127001-to-etcresolvconfresolvconfdhead","text":"vi /etc/resolvconf/resolv.conf.d/head nameserver 127.0.0.1","title":"Add nameserver 127.0.0.1 to /etc/resolvconf/resolv.conf.d/head."},{"location":"setupv1/#create-etccorednscorefile-and-paste-the-configuration-shown-below","text":"vi /etc/coredns/Corefile groophy.org:53 { forward . tls://2606:4700:4700::1111 tls://1.1.1.1 hosts /etc/hosts log errors Cache } .:53 { forward . tls://2606:4700:4700::1111 tls://1.1.1.1 log errors Cache }","title":"Create /etc/coredns/Corefile and paste the configuration shown below."},{"location":"setupv1/#we-are-using-cloudflare-as-a-dns-provider1111","text":"","title":"We are using Cloudflare as a DNS provider(1.1.1.1)."},{"location":"setupv1/#create-a-new-user-for-coredns-and-set-some-permissions-on-the-optcoredns-directory","text":"sudo useradd -d /var/lib/coredns -m coredns sudo chown coredns:coredns /opt/coredns","title":"Create a new user for CoreDNS and set some permissions on the /opt/coredns directory."},{"location":"setupv1/#download-the-systemd-service-unit-file-from-coredns-to-etcsystemdsystemcorednsservice","text":"wget https://github.com/coredns/deployment/blob/master/systemd/coredns.service mv coredns.service /etc/systemd/system/","title":"Download the SystemD service unit file from coredns to /etc/systemd/system/coredns.service."},{"location":"setupv1/#disable-systemds-default-dns-server","text":"sudo systemctl stop systemd-resolved && sudo systemctl disable systemd-resolved Note From that moment on, you will not be able to resolve any web pages anymore, unless you enable DNS again","title":"Disable SystemD's default DNS server."},{"location":"setupv1/#enable-and-start-coredns","text":"sudo systemctl enable coredns && sudo systemctl start coredns","title":"Enable and start CoreDNS"},{"location":"setupv1/#now-we-can-able-to-resolve-domain-names-again-eg-try-to-dig-short-kitedu-if-an-ip-address-is-printed-everything-works-fine","text":"dig +short kit.edu","title":"Now we can able to resolve domain names, again. E.g. try to dig +short kit.edu. If an IP address is printed, everything works fine."},{"location":"setupv1/#output","text":"141.3.128.6 nslookup dnsserver.groophy.org","title":"Output"},{"location":"setupv1/#output_1","text":"Server : 192.168.0.100 Address : 192.168.0.100 # 53 Name : dnsserver . cruise . com Address : 192.168.0.100 nslookup google.com","title":"Output"},{"location":"setupv1/#output_2","text":"Server : 192.168.0.100 Address : 192.168.0.100 # 53 Non - authoritative answer : Name : google . com Address : 142.250.71.14 Name : google . com Address : 2404 : 6800 : 4007 : 824 :: 200 e Conclusion Able to resolve both internal servers and outside the world.","title":"Output"},{"location":"setupv2/","text":"Setup \u00b6 Configure the host file with the hostname and IPs. cat /etc/hosts 192.168.0.100 dnsserver . cruise . com 192.168.135.21 test01 . cruise . com 192.168.0.100 dnsserver 127.0.0.1 localhost Download CoreDNS from the website, unpack the binary to /usr/local/bin and make it executable. wget https://github.com/coredns/coredns/releases/download/v1.8.6/coredns_1.8.6_linux_amd64.tgz tar -xvzf coredns_1.8.6_linux_amd64.tgz cp coredns /usr/local/bin/ sudo chmod +x /usr/local/bin/coredns ll /usr/local/bin/coredns Install resolvconf as a tool to manually manage /etc/resolv.conf . apt install resolvconf Set dns as default in /etc/NetworkManager/NetworkManager.conf . vi /etc/NetworkManager/NetworkManager.conf dns=default Add nameserver 127.0.0.1 to /etc/resolvconf/resolv.conf.d/head . vi /etc/resolvconf/resolv.conf.d/head nameserver 127.0.0.1 Create /etc/coredns/Corefile and paste the configuration shown below. vi /etc/coredns/Corefile groophy.org:53 { forward . tls://2606:4700:4700::1111 tls://1.1.1.1 hosts /etc/hosts log errors Cache } .:53 { forward . tls://2606:4700:4700::1111 tls://1.1.1.1 log errors Cache } We are using Cloudflare as a DNS provider(1.1.1.1). Create a new user for CoreDNS and set some permissions on the /opt/coredns directory. sudo useradd -d /var/lib/coredns -m coredns sudo chown coredns:coredns /opt/coredns Download the SystemD service unit file from coredns to /etc/systemd/system/coredns.service . wget https://github.com/coredns/deployment/blob/master/systemd/coredns.service mv coredns.service /etc/systemd/system/ Disable SystemD's default DNS server. sudo systemctl stop systemd-resolved && sudo systemctl disable systemd-resolved Note From that moment on, you will not be able to resolve any web pages anymore, unless you enable DNS again Enable and start CoreDNS sudo systemctl enable coredns && sudo systemctl start coredns Now we can able to resolve domain names, again. E.g. try to dig +short kit.edu . If an IP address is printed, everything works fine. dig +short kit.edu Output 141.3.128.6 Try the below command nslookup dnsserver.groophy.org Output Server : 192.168.0.100 Address : 192.168.0.100 # 53 Name : dnsserver . cruise . com Address : 192.168.0.100 Try the below command nslookup google.com Output Server : 192.168.0.100 Address : 192.168.0.100 # 53 Non - authoritative answer : Name : google . com Address : 142.250.71.14 Name : google . com Address : 2404 : 6800 : 4007 : 824 :: 200 e Conclusion Able to resolve both internal servers and outside the world.","title":"Setup"},{"location":"setupv2/#setup","text":"Configure the host file with the hostname and IPs. cat /etc/hosts 192.168.0.100 dnsserver . cruise . com 192.168.135.21 test01 . cruise . com 192.168.0.100 dnsserver 127.0.0.1 localhost Download CoreDNS from the website, unpack the binary to /usr/local/bin and make it executable. wget https://github.com/coredns/coredns/releases/download/v1.8.6/coredns_1.8.6_linux_amd64.tgz tar -xvzf coredns_1.8.6_linux_amd64.tgz cp coredns /usr/local/bin/ sudo chmod +x /usr/local/bin/coredns ll /usr/local/bin/coredns Install resolvconf as a tool to manually manage /etc/resolv.conf . apt install resolvconf Set dns as default in /etc/NetworkManager/NetworkManager.conf . vi /etc/NetworkManager/NetworkManager.conf dns=default Add nameserver 127.0.0.1 to /etc/resolvconf/resolv.conf.d/head . vi /etc/resolvconf/resolv.conf.d/head nameserver 127.0.0.1 Create /etc/coredns/Corefile and paste the configuration shown below. vi /etc/coredns/Corefile groophy.org:53 { forward . tls://2606:4700:4700::1111 tls://1.1.1.1 hosts /etc/hosts log errors Cache } .:53 { forward . tls://2606:4700:4700::1111 tls://1.1.1.1 log errors Cache } We are using Cloudflare as a DNS provider(1.1.1.1). Create a new user for CoreDNS and set some permissions on the /opt/coredns directory. sudo useradd -d /var/lib/coredns -m coredns sudo chown coredns:coredns /opt/coredns Download the SystemD service unit file from coredns to /etc/systemd/system/coredns.service . wget https://github.com/coredns/deployment/blob/master/systemd/coredns.service mv coredns.service /etc/systemd/system/ Disable SystemD's default DNS server. sudo systemctl stop systemd-resolved && sudo systemctl disable systemd-resolved Note From that moment on, you will not be able to resolve any web pages anymore, unless you enable DNS again Enable and start CoreDNS sudo systemctl enable coredns && sudo systemctl start coredns Now we can able to resolve domain names, again. E.g. try to dig +short kit.edu . If an IP address is printed, everything works fine. dig +short kit.edu Output 141.3.128.6 Try the below command nslookup dnsserver.groophy.org Output Server : 192.168.0.100 Address : 192.168.0.100 # 53 Name : dnsserver . cruise . com Address : 192.168.0.100 Try the below command nslookup google.com Output Server : 192.168.0.100 Address : 192.168.0.100 # 53 Non - authoritative answer : Name : google . com Address : 142.250.71.14 Name : google . com Address : 2404 : 6800 : 4007 : 824 :: 200 e Conclusion Able to resolve both internal servers and outside the world.","title":"Setup"}]}